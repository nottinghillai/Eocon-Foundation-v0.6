---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: 5, 2
---

# 5, 2

_Source page: 324_



0.2


0.6


0.8


0.4


These posteriors assign nonzero likelihood to the win probabilities be-


tween


and


. The density at


is


for both arms because they both received


at least one win. Similarly, the density at


for arm


is


because it received


at least one loss. The payoff probabilities


2/3


and


5/7


are shown


with vertical lines. We believe that the second arm has the best chance of


producing a payout.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


15.4. directed exploration strategies


303


One of the most common undirected exploration strategies is


-greedy explo-


ration


(algorithm 15.3). This strategy chooses a random arm with probability


Otherwise, we choose a greedy arm,


arg max


. This


is the posterior prob-


ability of a win with action


using the Bayesian model given in the previous


section. Alternatively, we can use the maximum likelihood estimate, but with


enough pulls, the difference between the two approaches is small. Larger values


of


lead to more exploration, thereby resulting in faster identification of the best


arm, but more pulls are wasted on suboptimal arms. Example 15.2 demonstrates


this exploration strategy and the evolution of our beliefs.


The


-greedy method maintains a constant amount of exploration, despite


there being far more uncertainty earlier in the interaction with the bandit than


later. One common adjustment is to decay


over time, such as with an exponential


decay schedule with the following update:


αϵ