---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ) =
---

# ) =

_Source page: 383_



, where


is a feature vector and


is a vector of weightings. In this section, we will focus


P. Abbeel and A. Y. Ng, “Appren-


on an approach known as


maximum margin inverse reinforcement learning


where


ticeship Learning via Inverse Re-


the features are assumed to be binary. Since optimal policies remain optimal with


inforcement Learning,” in


Interna-


positive scaling of the reward function, this method additionally constrains the


tional Conference on Machine Learn-


ing (ICML)


, 2004.


weight vector such that


. The expert data activates each binary feature


with different frequencies, perhaps pursuing some and avoiding others. This


approach attempts to learn this pattern of activation and trains an agent to mimic


these activation frequencies.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


362


chapter 18. imitation learning


Algorithm 18.3.


The SMILe al-


struct


SMILe


gorithm for training a stochastic


# problem with unknown reward


parameterized policy from expert


bc


# Behavioral cloning struct


demonstrations for an MDP


. It


k_max


# number of iterations


successively mixes in new com-


# number of rollouts per iteration


# rollout depth


ponent policies with smaller and


# initial state distribution


smaller weights, while simultane-


# mixing scalar (e.g., d^-3)


ously reducing the probability of


πE


# expert policy


acting according to the expert pol-


πθ


# parameterized policy


icy. The method returns the prob-


end


abilities


Ps


and parameterizations


θs


for the component policies.


function


optimize


::


SMILe


bc


k_max


bc


k_max


πE


πθ


πE


πθ


θs


[]


->


πE


for


in 1


k_max


# execute latest π to get new data set D


[]


for


in 1


rand


for


in 1


push!


, (


πE


)))


rand


))


end


end


# train new policy classifier


optimize


bc


push!


θs


# compute a new policy mixture


Pπ


Categorical


normalize


([(


-1


for


in 1


],


))


->


begin


if


rand


()


-1


return


πE


else


return


rand


Categorical


πθ


θs


rand


Pπ


)],


)))


end


end


end


Ps


normalize


([(


-1


for


in 1


k_max


],


return


Ps


θs


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


18.4. maximum margin inverse reinforcement learning


363


Example 18.4.


Using SMILe to


Consider using SMILe to train a policy on the mountain car problem where


learn a policy for the mountain


the reward is not observed. We use the same features that were used for


car problem. In contrast with DAg-


ger in example 18.3, SMILe mixes


DAgger in example 18.3. Both DAgger and SMILe receive a new expert-


the expert into the policy during


labeled data set with each iteration. Instead of accumulating a larger data


rollouts. This expert component,


set of expert-labeled data, SMILe trains a new policy component using only


whose influence wanes with each


iteration, causes the initial rollouts


the most recent data, mixing the new policy component with the previous


to better progress toward the goal.


policy components.


rollouts


10


0.8


0.6


0.4


speed


0.2


10


0.8


0.6


0.4


speed


0.2


10


0.8


0.6


0.4


speed


0.2


0.5


0.5


0.5


0.5


0.5


0.5


position


position


position


accel right


coast


accel left


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


364


chapter 18. imitation learning


An important part of this algorithm involves reasoning about the expected


return under a policy


for a weighting


and initial state distribution