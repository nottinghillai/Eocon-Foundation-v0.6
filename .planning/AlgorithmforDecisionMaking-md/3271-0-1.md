---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: 0.1/
---

# 0.1/

_Source page: 532_



. Shown here are


20


policy


updates. Although different simu-


0.4


0.4


rock


rock


lation traces will converge because


0.6


0.6


paper


paper


the step size goes to


, different


0.6


0.6


samples from the stochastic poli-


0.4


0.4


cies may result in convergence to


different policies.


0.8


0.8


0.2


0.2


0.2


0.6


0.8


0.2


0.6


0.8


0.4


0.4


scissors


scissors


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


24.11. exercises


511


Optimality is not as straightforward in the multiagent setting, with multiple


possible solution concepts for extracting policies from a reward specification.


A best response of an agent to a fixed set of policies of the other agents is one


where there is no incentive to deviate.


• A Nash equilibrium is a joint policy where all agents follow a best response.


A correlated equilibrium is the same as a Nash equilibrium, except that all the


agents follow a single joint action distribution that allows correlation between


agents.


Iterated best response can quickly optimize a joint policy by iteratively applying


best responses, but there are no general guarantees of convergence.


Hierarchical softmax attempts to model agents in terms of their depth of


rationality and precision, which can be learned from past joint actions.


Fictitious play is a learning algorithm that uses maximum-likelihood action


models for other agents to find best response policies, with the potential to


converge to a Nash equilibrium.


Gradient ascent, followed by projection onto the probability simplex, can be


used to learn policies.