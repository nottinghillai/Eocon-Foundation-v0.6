---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: TRPO
---

# TRPO

_Source page: 276_



).


It works by computing


M. Jordan, and P. Abbeel, “Trust


the next evaluation point


that would be taken by the natural policy gradient


Region Policy Optimization,” in


In-


and then conducting a


line search


along the line segment connecting


to


. A key


ternational Conference on Machine


Learning (ICML)


, 2015.


property of this line search phase is that evaluations of the approximate objective


and constraint do not require any additional rollout simulations.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


12.4. trust region update


255


Algorithm 12.4. The update func-


struct


NaturalPolicyUpdate


tion for the natural policy gradi-


# problem


ent, given policy


, for an


# initial state distribution


MDP


with initial state distribu-


# depth


tion


. The natural gradient with


# number of samples


respect to the parameter vector


is


∇logπ


# gradient of log likelihood


# policy


estimated from


rollouts to depth


# divergence bound


using the log policy gradients


end


∇logπ


. The


natural_update


helper


method conducts an update ac-


function


natural_update


∇f


τs


cording to equation (12.12), given


∇fθ


mean


∇f


for


in


τs


an objective gradient


∇f


and a


mean


for


in


τs


∇fθ


Fisher matrix


for a list of tra-


return


sqrt


dot


∇fθ


))


jectories.


end


function


update


::


NaturalPolicyUpdate


∇logπ


∇logπ


πθ


sum


-1


for


, (


))


in


enumerate


))


∇log


sum


∇logπ


for


in


∇U


∇log


∇log


∇log


τs


simulate


rand


),


πθ


for


in 1


return


natural_update


∇U


τs


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


256


chapter 12. policy gradient optimization


During the line search phase, we no longer use a first-order approximation. In-


stead, we use an approximation derived from an equality involving the advantage


A variation of this equality is


function


proven in lemma 6.1 of S. M.