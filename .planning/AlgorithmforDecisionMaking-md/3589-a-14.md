---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (A.14)
---

# (A.14)

_Source page: 589_



where


is called a


step factor


. The idea of this optimization approach is that


we take steps in the direction of the gradient until reaching a local maximum.


There is no guarantee that we will find a global maximum using this method.


Small values for


will generally require more iterations to come close to a local


maximum. Large values for


will often result in bouncing around the local


optimum without quite reaching it. If


is constant over iterations, it is sometimes


called a


learning rate


. Many applications involve a


decaying step factor


, where, in


addition to updating


at each iteration, we update


according to


γα