---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: '1.083'
---

# 1.083

_Source page: 332_



© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


15.8. exercises


311


Algorithm 15.9.


The simulation


function


simulate


::


MDP


model


loop for reinforcement learning


for


in 1


problems. The exploration policy


model


generates the next action based on


s′


TR


information in the model and the


update!


model


s′


current state


. The MDP problem


s′


is treated as the ground truth and


end


end


is used to sample the next state and


reward. The state transition and re-


ward are used to update the model.


The simulation is run to horizon


Directed exploration strategies, including softmax, quantile, UCB1, and poste-


rior sampling exploration, use information from past actions to better explore


promising actions.


Dynamic programming can be used to derive optimal exploration strategies


for finite horizons, but these strategies can be expensive to compute.