---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: '3.48'
---

# 3.48

_Source page: 178_



Exercise 7.8.


Prove that a Bellman residual of


guarantees that the value function obtained


by value iteration is within


δγ


of


at every state


Solution:


For a given


, suppose we know that


. Then we bound the


improvement in the next iteration:


) =


max


) +


max


) +


max


) +


max


) +


δγ


Similarly,


max


) +


max


) +


) +


δγ


The accumulated improvement after infinite iterations is thus bounded by


δγ


δγ


A Bellman residual of


thus guarantees that the optimal value function obtained by


value iteration is within


δγ


of


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


7.10. exercises


157


Exercise 7.9.


Suppose that we run policy evaluation on an expert policy to obtain a value


function. If acting greedily with respect to that value function is equivalent to the expert


policy, what can we deduce about the expert policy?


Solution:


We know from the Bellman optimality equation that greedy lookahead on an


optimal value function is stationary. If the greedy policy matches the expert policy, then


both policies are optimal.


Exercise 7.10.


Show how an LQR problem with a quadratic reward function


) =


can be reformulated so that the reward function includes linear terms in


and


Solution:


We can introduce an additional state dimension that is always equal to


, yielding


a new system with linear dynamics:


# "


The reward function of the augmented system can now have linear state reward terms:


augmented


scalar


linear


Similarly, we can include an additional action dimension that is always


in order to obtain


linear action reward terms.


Exercise 7.11.


Why does the optimal policy obtained in example 7.4 produce actions with


greater magnitude when the horizon is greater?


Solution:


The problem in example 7.4 has quadratic reward that penalizes deviations from


the origin. The longer the horizon, the greater the negative reward that can be accumulated,


making it more worthwhile to reach the origin sooner.


Exercise 7.12.


Prove that iterative policy evaluation converges to the solution of equa-


tion (7.6).


Solution:


Consider iterative policy evaluation applied to a policy


as given in equa-


tion (7.5):


) =