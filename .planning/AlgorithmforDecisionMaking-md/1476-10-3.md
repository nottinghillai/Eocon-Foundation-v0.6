---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (10.3)
---

# (10.3)

_Source page: 236_



weighted according to their likeli-


hood.


where


is the


th trajectory sample.


Algorithm 10.1. Monte Carlo pol-


struct


MonteCarloPolicyEvaluation


icy evaluation of a policy


. The


# problem


method runs


rollouts to depth


# initial state distribution


according to the dynamics speci-


# depth


fied by the problem


. Each rollout


# number of samples


is run from an initial state sampled


end


from state distribution


. The final


function


::


MonteCarloPolicyEvaluation


)(


line in this algorithm block evalu-


rollout


rand


),


ates a policy


parameterized by


return


mean


for


, which will be useful in the algo-


end


rithms in this chapter that attempt


to find a value of


that maximizes


::


MonteCarloPolicyEvaluation


)(


->


))


50


40


Monte Carlo policy evaluation is stochastic. Multiple evaluations of equa-


30


20


tion (10.1) with the same policy can give different estimates. Increasing the


10


10


10


number of rollouts decreases the variance of the evaluation, as demonstrated in


number of samples


figure 10.2.


We will use


. For convenience, we


to denote a policy parameterized by


Figure 10.2. The effect of the depth


will use


as shorthand for


in cases where it is not ambiguous. The


and sample count for Monte Carlo


policy evaluation of a uniform ran-


parameter


may be a vector or some other more complex representation. For


dom policy on the cart-pole prob-


example, we may want to represent our policy using a neural network with a


lem (appendix F.3). The variance


decreases as the number of sam-


particular structure. We would use


to represent the weights in the network.


ples increases. The blue regions in-


Many optimization algorithms assume that


is a vector with a fixed number of


dicate the


5 %


to