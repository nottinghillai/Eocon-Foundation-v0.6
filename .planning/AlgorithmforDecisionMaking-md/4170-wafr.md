---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (WAFR)
---

# (WAFR)

_Source page: 674_



, 2011 (cit. on p. 475).


14.


L. Baird, “Residual Algorithms: Reinforcement Learning with Function Approxima-


tion,” in


International Conference on Machine Learning (ICML)


, 1995 (cit. on p. 610).


15.


Y. Bar-Shalom, X. R. Li, and T. Kirubarajan,


Estimation with Applications to Tracking


and Navigation


. Wiley, 2001 (cit. on p. 383).


16.


D. Barber,


Bayesian Reasoning and Machine Learning


. Cambridge University Press,


2012 (cit. on p. 53).


17.


A. G. Barto, R. S. Sutton, and C. W. Anderson, “Neuronlike Adaptive Elements That


Can Solve Difficult Learning Control Problems,”


IEEE Transactions on Systems, Man,


and Cybernetics


, vol. SMC-13, no. 5, pp. 834–846, 1983 (cit. on p. 611).


18.


A. G. Barto, S. J. Bradtke, and S. P. Singh, “Learning to Act Using Real-Time Dynamic


Programming,”


Artificial Intelligence


, vol. 72, no. 1–2, pp. 81–138, 1995 (cit. on p. 197).


19.


K. Basu, “The Traveler’s Dilemma: Paradoxes of Rationality in Game Theory,”


American Economic Review


, vol. 84, no. 2, pp. 391–395, 1994 (cit. on p. 622).


20.


R. Bellman, “Minimization Problem,”


Bulletin of the American Mathematical Society


vol. 62, no. 3, p. 270, 1956 (cit. on p. 399).


21.


R. Bellman,


Eye of the Hurricane: An Autobiography


. World Scientific, 1984 (cit. on


p. 136).


22.


R. E. Bellman,


Dynamic Programming


. Princeton University Press, 1957 (cit. on


pp. 133, 138).


23.


A. Bemporad and M. Morari, “Robust Model Predictive Control: A Survey,” in


Robustness in Identification and Control


, A. Garulli, A. Tesi, and A. Vicino, eds.,


Springer, 1999, pp. 207–226 (cit. on p. 204).


24.


J. Bentham,


Theory of Legislation


. Trübner & Company, 1887 (cit. on p. 8).


25.


U. Berger, “Brown’s Original Fictitious Play,”


Journal of Economic Theory


, vol. 135,


no. 1, pp. 572–578, 2007 (cit. on p. 507).


26.


D. Bernoulli, “Exposition of a New Theory on the Measurement of Risk,”


Econo-


metrica


, vol. 22, no. 1, pp. 23–36, 1954 (cit. on p. 112).


27.


D. S. Bernstein, R. Givan, N. Immerman, and S. Zilberstein, “The Complexity of


Decentralized Control of Markov Decision Processes,”


Mathematics of Operation


Research


, vol. 27, no. 4, pp. 819–840, 2002 (cit. on pp. 545, 546).


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


references


653


28.


D. P. Bertsekas,


Dynamic Programming and Optimal Control


. Athena Scientific, 2007


(cit. on p. 148).


29.


D. P. Bertsekas,


Reinforcement Learning and Optimal Control


. Athena Scientific, 2019


(cit. on p. 335).


30.


D. P. Bertsekas and J. N. Tsitsiklis,


Introduction to Probability


. Athena Scientific, 2002


(cit. on p. 20).


31.


M. Besançon, T. Papamarkou, D. Anthoff, A. Arslan, S. Byrne, D. Lin, and J. Pear-


son, “Distributions.jl: Definition and Modeling of Probability Distributions in the


JuliaStats Ecosystem,” 2019. arXiv: 1907.08611v1 (cit. on p. 573).


32.


W. M. Bolstad and J. M. Curran,


Introduction to Bayesian Statistics


. Wiley, 2016 (cit.


on p. 11).


33.


B. Bonet and H. Geffner, “Labeled RTDP: Improving the Convergence of Real-Time


Dynamic Programming,” in


International Conference on Automated Planning and


Scheduling (ICAPS)


, 2003 (cit. on p. 197).


34.


F. Borrelli, A. Bemporad, and M. Morari,


Predictive Control for Linear and Hybrid


Systems


. Cambridge University Press, 2019 (cit. on p. 200).


35.


M. Bouton, A. Nakhaei, K. Fujimura, and M. J. Kochenderfer, “Safe Reinforce-


ment Learning with Scene Decomposition for Navigating Complex Urban Environ-


ments,” in


IEEE Intelligent Vehicles Symposium (IV)


, 2019 (cit. on p. 3).


36.


M. Bouton, J. Tumova, and M. J. Kochenderfer, “Point-Based Methods for Model


Checking in Partially Observable Markov Decision Processes,” in


AAAI Conference


on Artificial Intelligence (AAAI)


, 2020 (cit. on p. 293).


37.


M. Bowling, “Convergence and No-Regret in Multiagent Learning,” in


Advances in


Neural Information Processing Systems (NIPS)


, 2005 (cit. on p. 509).


38.


M. Bowling and M. Veloso, “An Analysis of Stochastic Game Theory for Multiagent


Reinforcement Learning,” Carnegie Mellon University, Tech. Rep. CMU-CS-00-165,


2000 (cit. on p. 521).


39.


S. Boyd and L. Vandenberghe,


Convex Optimization


. Cambridge University Press,


2004 (cit. on pp. 200, 565).


40.


R. I. Brafman and M. Tennenholtz, “R-MAX—A General Polynomial Time Algo-


rithm for Near-Optimal Reinforcement Learning,”


Journal of Machine Learning Re-


search


, vol. 3, pp. 213–231, 2002 (cit. on p. 323).


41.


D. Brockhoff, A. Auger, N. Hansen, D. Arnold, and T. Hohm, “Mirrored Sampling


and Sequential Selection for Evolution Strategies,” in


International Conference on


Parallel Problem Solving from Nature


, 2010 (cit. on p. 224).


42.


G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W.


Zaremba, “OpenAI Gym,” 2016. arXiv: 1606.01540v1 (cit. on p. 611).


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


654


references


43.


G. W. Brown, “Iterative Solution of Games by Fictitious Play,”


Activity Analysis of


Production and Allocation


, vol. 13, no. 1, pp. 374–376, 1951 (cit. on p. 505).


44.


C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen,


S. Tavener, D. Perez, S. Samothrakis, and S. Colton, “A Survey of Monte Carlo Tree


Search Methods,”


IEEE Transactions on Computational Intelligence and AI in Games


vol. 4, no. 1, pp. 1–43, 2012 (cit. on pp. 187, 276).


45.


J. A. Bucklew,


Introduction to Rare Event Simulation


. Springer, 2004 (cit. on p. 287).


46.


W. L. Buntine, “Theory Refinement on Bayesian Networks,” in


Conference on Uncer-


tainty in Artificial Intelligence (UAI)


, 1991 (cit. on p. 104).


47.


C. F. Camerer,


Behavioral Game Theory: Experiments in Strategic Interaction


. Princeton


University Press, 2003 (cit. on p. 504).


48.


A. R. Cassandra, M. L. Littman, and N. L. Zhang, “Incremental Pruning: A Sim-


ple, Fast, Exact Method for Partially Observable Markov Decision Processes,” in


Conference on Uncertainty in Artificial Intelligence (UAI)


, 1997 (cit. on p. 416).


49.


J. Chakravorty and A. Mahajan, “Multi-Armed Bandits, Gittins Index, and Its Cal-


culation,” in


Methods and Applications of Statistics in Clinical Trials


, N. Balakrishnan,


ed., vol. 2, Wiley, 2014, pp. 416–435 (cit. on p. 309).


50.


D. M. Chickering, “Learning Bayesian Networks is NP-Complete,” in


Learning from


Data: Artificial Intelligence and Statistics V


, D. Fisher and H.


J. Lenz, eds., Springer,


1996, pp. 121–130 (cit. on p. 97).


51.


D. M. Chickering, “Learning Equivalence Classes of Bayesian-Network Structures,”


Journal of Machine Learning Research


, vol. 2, pp. 445–498, 2002 (cit. on p. 106).


52.


D. M. Chickering, D. Heckerman, and C. Meek, “Large-Sample Learning of Bayesian


Networks is NP-Hard,”


Journal of Machine Learning Research


, vol. 5, pp. 1287–1330,


2004 (cit. on p. 97).


53.


K. Cho, B. van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,


and Y. Bengio, “Learning Phrase Representations Using RNN Encoder-Decoder


for Statistical Machine Translation,” in


Conference on Empirical Methods in Natural


Language Processing (EMNLP)


, 2014 (cit. on p. 592).


54.


E. K. P. Chong, R. L. Givan, and H. S. Chang, “A Framework for Simulation-Based


Network Control via Hindsight Optimization,” in


IEEE Conference on Decision and


Control (CDC)


, 2000 (cit. on p. 207).


55.


B. Christian,


The Alignment Problem


. Norton & Company, 2020 (cit. on p. 13).


56.


G. F. Cooper, “The Computational Complexity of Probabilistic Inference Using


Bayesian Belief Networks,”


Artificial Intelligence


, vol. 42, no. 2–3, pp. 393–405, 1990


(cit. on p. 53).


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


references


655


57.


G. F. Cooper and E. Herskovits, “A Bayesian Method for the Induction of Proba-


bilistic Networks from Data,”


Machine Learning


, vol. 4, no. 9, pp. 309–347, 1992 (cit.


on pp. 97, 100).


58.


T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein,


Introduction to Algorithms


3rd ed. MIT Press, 2009 (cit. on p. 136).


59.


A. Corso, R. J. Moss, M. Koren, R. Lee, and M. J. Kochenderfer, “A Survey of Al-


gorithms for Black-Box Safety Validation,”


Journal of Artificial Intelligence Research


vol. 72, pp. 377–428, 2021 (cit. on p. 281).


60.


A. Couëtoux, J.


B. Hoock, N. Sokolovska, O. Teytaud, and N. Bonnard, “Continuous


Upper Confidence Trees,” in


Learning and Intelligent Optimization (LION)


, 2011 (cit.


on p. 197).


61.


F. Cuzzolin,


The Geometry of Uncertainty


. Springer, 2021 (cit. on p. 19).


62.


G. B. Dantzig, “Linear Programming,”


Operations Research


, vol. 50, no. 1, pp. 42–47,


2002 (cit. on p. 8).


63.


C. Daskalakis, P. W. Goldberg, and C. H. Papadimitriou, “The Complexity of Com-


puting a Nash Equilibrium,”


Communications of the ACM


, vol. 52, no. 2, pp. 89–97,


2009 (cit. on p. 498).


64.


A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum Likelihood from Incom-


plete Data via the EM Algorithm,”


Journal of the Royal Statistical Society, Series B


(Methodological)


, vol. 39, no. 1, pp. 1–38, 1977 (cit. on p. 87).


65.


S. L. Dittmer and F. V. Jensen, “Myopic Value of Information in Influence Diagrams,”


in


Conference on Uncertainty in Artificial Intelligence (UAI)


, 1997 (cit. on p. 119).


66.


J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra, “Efficient Projections onto


the


-Ball for Learning in High Dimensions,” in


International Conference on Machine


Learning (ICML)


, 2008 (cit. on p. 485).


67.


M. J. Dupré and F. J. Tipler, “New Axioms for Rigorous Bayesian Probability,”


Bayesian Analysis


, vol. 4, no. 3, pp. 599–606, 2009 (cit. on p. 20).


68.


M. Egorov, Z. N. Sunberg, E. Balaban, T. A. Wheeler, J. K. Gupta, and M. J. Kochen-


derfer, “POMDPs.jl: A Framework for Sequential Decision Making Under Uncer-


tainty,”


Journal of Machine Learning Research


, vol. 18, no. 26, pp. 1–5, 2017 (cit. on


p. 381).


69.


C. Elkan, “The Foundations of Cost-Sensitive Learning,” in


International Joint Con-


ference on Artificial Intelligence (IJCAI)


, 2001 (cit. on p. 373).


70.


P. H. Farquhar, “Utility Assessment Methods,”


Management Science


, vol. 30, no. 11,


pp. 1283–1300, 1984 (cit. on p. 114).


71.


J. A. Filar, T. A. Schultz, F. Thuijsman, and O. Vrieze, “Nonlinear Programming


and Stationary Equilibria in Stochastic Games,”


Mathematical Programming


, vol. 50,


no. 1–3, pp. 227–237, 1991 (cit. on p. 521).


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


656


references


72.


A. M. Fink, “Equilibrium in a Stochastic


-Person Game,”


Journal of Science of the


Hiroshima University, Series A-I


, vol. 28, no. 1, pp. 89–93, 1964 (cit. on p. 520).


73.


C. Finn, S. Levine, and P. Abbeel, “Guided Cost Learning: Deep Inverse Optimal


Control via Policy Optimization,” in


International Conference on Machine Learning