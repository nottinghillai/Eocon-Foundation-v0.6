---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: 0, 1, 0,
---

# 0, 1, 0,

_Source page: 297_



for different values of


. Each


iteration was run with five rollouts to depth


10


with


0.9


0.2


0.4


policy performance


10


10


value function loss


10


20


25


30


35


50


10


15


40


45


iteration


0.5


2.0


0.1


1.0


For this simple problem, the policy quickly converges to optimality almost


regardless of


. However, if


is either too small or too large, the value


function takes longer to improve. In the case of very small values of


, our


policy conducts insufficient exploration from which to effectively learn the


value function. For larger values of


, we explore more, but we also tend to


make poor move choices more frequently.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


276


chapter 13. actor-critic methods


This general approach was intro-


and we use the results from Monte Carlo tree search to refine our parameterized


duced by D. Silver, J. Schrittwieser,


K. Simonyan, I. Antonoglou, A.


policy and value function. As with the other actor critic methods, we apply


Huang, A. Guez, T. Hubert, L.


gradient-based optimization of


and


Baker, M. Lai, A. Bolton, et al.,


As we perform Monte Carlo tree search, we want to direct our exploration to


“Mastering the Game of Go With-


out Human Knowledge,”


Nature


some extent by our parameterized policy


. One approach is to use an


vol. 550, pp. 354–359, 2017. The


action that maximizes the


probabilistic upper confidence bound


discussion here loosely follows


their


AlphaGo Zero


algorithm, but


instead of trying to solve the game


arg max