---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (D.4)
---

# (D.4)

_Source page: 607_



exp


Gradients for neural networks are typically computed using


reverse accumu-


This process is commonly called


lation


The method begins with a forward step, in which the neural network is


backpropagation


, which specifically


evaluated using all input parameters. In the backward step, the gradient of each


refers to reverse accumulation ap-


term of interest is computed working from the output back to the input. Reverse


plied to a scalar loss function. D. E.


Rumelhart, G. E. Hinton, and R. J.


accumulation uses the chain rule for derivatives:


Williams, “Learning Representa-


tions by Back-Propagating Errors,”