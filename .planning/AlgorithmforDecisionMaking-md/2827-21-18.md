---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (21.18)
---

# (21.18)

_Source page: 467_



Algorithm 21.15 extracts this utility function and policy from the pairs in


Algorithm 21.16 applies a variation of approximate value iteration (introduced


in algorithm 8.1) to our triangulated policy representation. We simply iteratively


apply backups over our beliefs in


using one-step lookahead with our value


15


See lemma 4 of W. S. Lovejoy,


function interpolation. If


is initialized with an upper bound, value iteration will


“Computationally Feasible Bounds


result in an upper bound even after a finite number of iterations. This property


for Partially Observed Markov De-


holds because value functions are convex and the linear interpolation between ver-


cision Processes,”


Operations Re-


15


search


, vol. 39, no. 1, pp. 162–175,


tices on the value function must lie on or above the underlying convex function.


1991.


Figure 21.10 shows an example of a policy and utility function.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


446


chapter 21. offline belief state planning


Algorithm 21.15.


A policy rep-


struct


TriangulatedPolicy


resentation


using


Freudenthal


# POMDP problem


triangulation with granularity


# dictionary mapping beliefs to utilities


As with the sawtooth method,


# beliefs


we maintain a dictionary that


# Freudenthal triangulation


maps belief vectors to utilities.


end


This implementation initializes


function


TriangulatedPolicy


::