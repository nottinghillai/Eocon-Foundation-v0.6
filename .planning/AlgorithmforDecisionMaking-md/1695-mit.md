---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: . MIT
---

# . MIT

_Source page: 272_



Press, 2019.


Algorithm 12.1. The gradient as-


struct


PolicyGradientUpdate


cent method for policy optimiza-


∇U


# policy gradient estimate


tion. It takes a step from a point


# step factor


in the direction of the gradient


end


∇U


with step factor


. We can use


one of the methods in the previous


function


update


::


PolicyGradientUpdate


return


∇U


chapter to compute


∇U


end


Very large gradients tend to overshoot the optimum and may occur due to a


variety of reasons. Rewards for some problems, such as for the 2048 problem


(appendix F.2), can vary by orders of magnitude. One approach for keeping the


gradients manageable is to use


gradient scaling


, which limits the magnitude of a


gradient estimate before using it to update the policy parameterization. Gradients


are commonly limited to having an


-norm of


. Another approach is


gradient


clipping


, which conducts elementwise clamping of the gradient before using it to


update the policy. Clipping commonly limits the entries to lie between


. Both


techniques are implemented in algorithm 12.2.


Algorithm 12.2. Methods for gra-


scale_gradient


L2_max


min


L2_max


norm


),


dient scaling and clipping. Gradi-


clip_gradient


clamp.


ent scaling limits the magnitude


of the provided gradient vector


to


L2_max


. Gradient clipping pro-


Scaling and clipping differ in how they affect the final gradient direction, as


vides elementwise clamping of the


provided gradient vector


to be-


demonstrated in figure 12.1. Scaling will leave the direction unaffected, whereas


tween


and


clipping affects each component individually. Whether this difference is advanta-


geous depends on the problem. For example, if a single component dominates


the gradient vector, scaling will zero out the other components.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


12.2. restricted gradient update


251


no modification


scale gradient to


scale gradient to


scale gradient to


1/2


0.5


clip gradient to


500


clip gradient to


clip gradient to


1/2


expected reward


Figure 12.1.


The effect of gradi-


ent scaling and clipping applied to


the simple regulator problem. Each


0.5


0.5


gradient evaluation ran


10


rollouts


iteration


to depth


10


. Step updates were ap-


plied with a step size of


0.2


. The


optimal policy parameterization is


shown in black.