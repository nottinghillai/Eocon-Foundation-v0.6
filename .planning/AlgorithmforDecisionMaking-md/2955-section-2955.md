---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ) =
---

# ) =

_Source page: 479_



is the total visit count for history


and


is an explo-


ration parameter. Importantly,


augments the value of actions that are unexplored


and underexplored, thus representing the relative trade-off between exploration


and exploitation.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


458


chapter 22. online belief state planning


Algorithm 22.1. Monte Carlo tree


struct


HistoryMonteCarloTreeSearch


search for POMDPs from belief


# problem


. The initial history


is optional.


# visit counts


This implementation is similar to


# action value estimates


the one in algorithm 9.5.


# depth


# number of simulations


# exploration constant


# value function estimate


end


function


explore


::


HistoryMonteCarloTreeSearch


Nh


sum


get


, (


),


for


in


return


argmax


->


[(


)]


bonus


[(


)],


Nh


),


end


function


simulate


::


HistoryMonteCarloTreeSearch


if


return


end


TRO


TRO


if


haskey


, (


first


)))


for


in


[(


)]


[(


)]


0.0


end


return


end


explore


s′


TRO


simulate


s′


vcat


, (


)),


-1


[(


)]


+=


[(


)]


+=


[(


)])


[(


)]


return


end


function


::


HistoryMonteCarloTreeSearch


)(


[])


for


in 1


rand


SetCategorical


))


simulate


end


return


argmax


->


[(


)],


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


22.6. determinized sparse tree search


459


Figure 22.2.


A search tree con-


taining all the histories covered


67


when running a Monte Carlo tree


search with


100


samples on the ma-


chine replacement problem. Visi-


tations are given beneath each ac-


tion node, and color indicates node


values with high values in blue


and low values in red. Expanded


nodes with zero visitations are not


shown. This search used an explo-


ration constant


0.5


, a max-


imum depth


, and a uni-


form random rollout policy. The


initial belief is certainty in a fully


working system. Monte Carlo tree


search is able to avoid certain ac-


tions and instead focus samples on


As with the MDP version, the Monte Carlo tree search algorithm is an anytime


more promising paths.


algorithm. The loop in algorithm 22.1 can be terminated at any time, and the


best solution found up to that point will be returned. With a sufficient number of


iterations, the algorithm converges to the optimal action.


Prior knowledge can be incorporated into Monte Carlo tree search in how we


initialize


and


. Our implementation uses zero, but other choices are possible,


including having the initialization of the action values be a function of history.


The value estimates can again be obtained from simulations of a rollout policy.


The algorithm does not need to be reinitialized with each decision. The


history


tree


and associated counts and value estimates can be maintained between calls.


The observation node associated with the selected action and actual observation


Ye, Somani, Hsu, and Lee present


becomes the root node at the next time step.


a determinized sparse tree search


algorithm for POMDPs called


De-


terminized Sparse Partially Observ-