---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: 15,000
---

# 15,000

_Source page: 122_



number of samples


Fortunately, search is a general problem, and a wide variety of generic search


algorithms have been studied over the years.


The name comes from the fact


One of the most common search strategies is called


K2


The search (algo-


that it is an evolution of a sys-


rithm 5.2) runs in polynomial time but does not guarantee finding a globally


tem called Kutató. The algorithm


optimal network structure. It can use any scoring function, but it is often used with


was introduced by G. F. Cooper


and E. Herskovits, “A Bayesian


the Bayesian score because of its ability to balance the complexity of the model


Method for the Induction of Prob-


with the amount of data available. K2 begins with a graph with no directed edges


abilistic Networks from Data,”


Ma-


chine Learning


, vol. 4, no. 9, pp. 309–


and then iterates over the variables according to a provided ordering, greedily


347, 1992.


adding parents to the nodes in a way that maximally increases the score. It is


common for K2 to impose an upper bound on the number of parents for any one


node to reduce the required computation. The original K2 algorithm assumed a


unit uniform Dirichlet prior with


for all


, and


, but any prior can be


ijk


used in principle.


A general search strategy is


local search


, which is sometimes called


hill climbing


Algorithm 5.3 provides an implementation of this concept. We start with an initial


graph and then move to the highest-scoring neighbor. The neighborhood of a


graph consists of the graphs that are only one basic graph operation away, where


the basic graph operations include introducing an edge, removing an edge, and


reversing an edge. Of course, not all operations are possible from a particular


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


5.2. directed graph search


101


Algorithm 5.2. K2 search of the


struct


K2Search


space of directed acyclic graphs us-


ordering


::


Vector


Int


# variable ordering


ing a specified variable ordering.


end


This variable ordering imposes a


topological ordering in the result-


function


fit


method


::


K2Search


vars


ing graph. The


fit


function takes


SimpleDiGraph


length


vars


))


for


in


enumerate


method


ordering


end


])


an ordered list variables


vars


and


bayesian_score


vars


a data set


. The method starts


while true


with an empty graph and itera-


y_best


j_best


= -


Inf


tively adds the next parent that


for


in


method


ordering


maximally improves the Bayesian


if


has_edge


score.


add_edge!


y′


bayesian_score


vars


if


y′


y_best


y_best


j_best


y′


end


rem_edge!


end


end


if


y_best


y_best


add_edge!


j_best


else


break


end


end


end


return


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


102


chapter 5. structure learning


graph, and operations that introduce cycles into the graph are invalid. The search


continues until the current graph scores no lower than any of its neighbors.


An


opportunistic


version of local search is implemented in algorithm 5.3. Rather


than generating all graph neighbors at every iteration, this method generates a


single random neighbor and accepts it if its Bayesian score is greater than that of


the current graph.


Algorithm 5.3.


Local directed


struct


LocalDirectedGraphSearch


graph search, which starts with


# initial graph


an initial directed graph


and


k_max


# number of iterations


opportunistically moves to a ran-


end


dom graph neighbor whenever its


Bayesian score is greater. It repeats


function


rand_graph_neighbor


nv


this process for


k_max


iterations.


rand


Random graph neighbors are gen-


mod1


rand


-1


erated by either adding or remov-


G′


copy


ing a single edge. This algorithm


has_edge


rem_edge!


G′


add_edge!


G′


can be extended to include revers-


return


G′


ing the direction of an edge. Edge


end


addition can result in a graph with


cycles, in which case we assign a


function


fit


method


::


LocalDirectedGraphSearch


vars


score of


method


bayesian_score


vars


for


in 1


method


k_max


G′


rand_graph_neighbor


y′


is_cyclic


G′


? -


Inf


bayesian_score


vars


G′


if


y′


y′


G′


end


end


return


end


Local search can get stuck in


local optima


, preventing it from finding the globally


optimal network structure. Various strategies have been proposed for addressing


The field of optimization is quite


local optima, including the following:


vast, and many methods have been


developed for addressing local op-


Randomized restart


. Once a local optima has been found, simply restart the


tima. This textbook provides an


search at a random point in the search space.


overview: M. J. Kochenderfer and


T. A. Wheeler,


Algorithms for Opti-


Simulated annealing


. Instead of always moving to the neighbor with greatest


mization


. MIT Press, 2019.


fitness, the search can visit neighbors with lower fitness according to some


randomized exploration strategy. As the search progresses, the randomness in


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


5.3. markov equivalence classes


103


the exploration decreases according to a particular schedule. This approach


is called simulated annealing because of its inspiration from annealing in


metallurgy.


Genetic algorithms


. The procedure begins with an initial random population of


points in the search space represented as binary strings. Each bit in a string


indicates the presence or absence of an arrow between two nodes. String manip-


ulation thus allows for searching the space of directed graphs. The individuals


in the population reproduce at a rate proportional to their score. Individuals


selected for reproduction have their strings recombined randomly through


genetic crossover, which involves selecting a crossover point on two randomly


selected individuals and then swapping the strings after that point. Mutations


are also introduced randomly into the population by randomly flipping bits in


the strings. The process of evolution continues until a satisfactory point in the


search space is found.


Memetic algorithms


. This approach, sometimes called


genetic local search


, is


simply a combination of genetic algorithms and local search. After genetic


recombination, local search is applied to the individuals.


Tabu search


. Previous methods can be augmented to maintain a


tabu list


con-


taining recently visited points in the search space. The search algorithm avoids


neighbors in the tabu list.


Some search strategies may work better than others on certain data sets, but in


general, finding the global optima remains NP-hard. Many applications, however,


do not require the globally optimal network structure. A locally optimal structure


is often acceptable.