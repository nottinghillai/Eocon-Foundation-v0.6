---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: 1, 0
---

# 1, 0

_Source page: 275_



. A sim-


ilar figure is presented in J. Pe-


ters and S. Schaal, “Reinforcement


Learning of Motor Skills with Pol-


icy Gradients,”


Neural Networks


vol. 21, no. 4, pp. 682–697, 2008.


The natural policy gradient method uses the same first-order approximation


of the objective as in the previous section. The constraint, however, is different.


The intuition is that we want to restrict changes in


that result in large changes


in the distribution over trajectories. A way to measure how much a distribution


changes is to use the


Kullback-Leibler divergence


, or KL divergence (appendix A.10).


We could impose the constraint