---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ∈S
---

# ∈S

_Source page: 539_



The likelihood of transitioning from a state


to a state


under a joint action


is given by the transition distribution


. Each agent


receives a reward


according to its own reward function


, which now also depends on the


state. Example 25.1 sketches out how traffic routing can be framed as an MG.


Algorithm 25.1. Data structure for


struct


MG


an MG.


# discount factor


# agents


# state space


# joint action space


# transition function


# joint reward function


end


518


chapter 25. sequential problems


Example 25.1. Traffic routing as an


Consider commuters headed to work by car. Each car has a starting position


MG. The problem cannot be mod-


and a destination. Each car can take any of several available roads to get to


eled using a single agent model like


an MDP because we do not know


their destination, but these roads vary in the time it takes to drive them. The


the behavior of other agents, only


more cars that drive on a given road, the slower they all move.


their rewards. We can try to find


This problem is an MG. The agents are the commuters in their cars, the


equilibria or learn policies through


interaction, similar to what we did


states are the locations of all the cars on the roads, and the actions corre-


for simple games.


spond to decisions of which road to take next. The state transition moves


all car agents forward following their joint action. The negative reward is


proportional to the time spent driving on a road.


The joint policy


in an MG specifies a probability distribution over joint


actions, given the current state. As with MDPs, we will focus on policies that


depend on the current state rather than the past history because future states and


rewards are conditionally independent of the history, given the current state. In


addition, we will focus on stationary policies, which do not depend on time. The


probability that agent


selects action


at state


is given by


. We will


often use


to represent a distribution over joint actions.


The utility of a joint policy


from the perspective of agent


can be computed


using a variation of policy evaluation introduced in section 7.2 for MDPs. The


reward to agent


from state


when following joint policy


is