---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (UAI)
---

# (UAI)

_Source page: 689_



, 2005 (cit. on p. 550).


254.


W. R. Thompson, “On the Likelihood That One Unknown Probability Exceeds


Another in View of the Evidence of Two Samples,”


Biometrika


, vol. 25, no. 3/4,


pp. 285–294, 1933 (cit. on p. 306).


255.


S. Thrun, “Probabilistic Robotics,”


Communications of the ACM


, vol. 45, no. 3, pp. 52–


57, 2002 (cit. on p. 10).


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


668


references


256.


S. Thrun, W. Burgard, and D. Fox,


Probabilistic Robotics


. MIT Press, 2006 (cit. on


pp. 379, 385, 394).


257.


K. S. Trivedi and A. Bobbio,


Reliability and Availability Engineering


. Cambridge


University Press, 2017 (cit. on p. 92).


258.


A. M. Turing, “Intelligent Machinery,” National Physical Laboratory, Report, 1948


(cit. on p. 9).


259.


A. Tversky and D. Kahneman, “The Framing of Decisions and the Psychology of


Choice,”


Science


, vol. 211, no. 4481, pp. 453–458, 1981 (cit. on pp. 123, 124).


260.


W. Uther and M. Veloso, “Adversarial Reinforcement Learning,” Carnegie Mellon


University, Tech. Rep. CMU-CS-03-107, 1997 (cit. on p. 521).


261.


R. Vanderbei,


Linear Programming, Foundations and Extensions


, 4th ed. Springer, 2014


(cit. on p. 147).


262.


H. van Hasselt, “Double Q-Learning,” in


Advances in Neural Information Processing


Systems (NIPS)


, 2010 (cit. on p. 338).


263.


S. Vasileiadou, D. Kalligeropoulos, and N. Karcanias, “Systems, Modelling and


Control in Ancient Greece: Part 1: Mythical Automata,”


Measurement and Control


vol. 36, no. 3, pp. 76–80, 2003 (cit. on p. 7).


264.


J. von Neumann and O. Morgenstern,


Theory of Games and Economic Behavior


. Prince-


ton University Press, 1944 (cit. on p. 112).


265.


A. Wächter and L. T. Biegler, “On the Implementation of an Interior-Point Filter


Line-Search Algorithm for Large-Scale Nonlinear Programming,”


Mathematical


Programming


, vol. 106, no. 1, pp. 25–57, 2005 (cit. on p. 205).


266.


C. J. C. H. Watkins, “Learning from Delayed Rewards,” Ph.D. dissertation, Univer-


sity of Cambridge, 1989 (cit. on pp. 336, 341).


267.


D. J. White, “A Survey of Applications of Markov Decision Processes,”


Journal of


the Operational Research Society


, vol. 44, no. 11, pp. 1073–1096, 1993 (cit. on p. 134).


268.


M. Wiering and M. van Otterlo, eds.,


Reinforcement Learning: State of the Art


. Springer,


2012 (cit. on p. 299).


269.


D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Peters, and J. Schmidhuber, “Natu-


ral Evolution Strategies,”


Journal of Machine Learning Research


, vol. 15, pp. 949–980,


2014 (cit. on pp. 219, 222).


270.


R. J. Williams, “Simple Statistical Gradient-Following Algorithms for Connectionist


Reinforcement Learning,”


Machine Learning


, vol. 8, pp. 229–256, 1992 (cit. on p. 245).


271.


B. Wong, “Points of View: Color Blindness,”


Nature Methods


, vol. 8, no. 6, pp. 441–


442, 2011 (cit. on p. xxii).


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


references


669


272.


J. R. Wright and K. Leyton-Brown, “Beyond Equilibrium: Predicting Human Behav-


ior in Normal Form Games,” in


AAAI Conference on Artificial Intelligence (AAAI)


2010 (cit. on p. 505).


273.


J. R. Wright and K. Leyton-Brown, “Behavioral Game Theoretic Models: A Bayesian


Framework for Parameter Analysis,” in


International Conference on Autonomous


Agents and Multiagent Systems (AAMAS)


, 2012 (cit. on p. 505).


274.


N. Ye, A. Somani, D. Hsu, and W. S. Lee, “DESPOT: Online POMDP Planning with


Regularization,”


Journal of Artificial Intelligence Research


, vol. 58, pp. 231–266, 2017


(cit. on p. 459).


275.


B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey, “Maximum Entropy Inverse


Reinforcement Learning,” in


AAAI Conference on Artificial Intelligence (AAAI)