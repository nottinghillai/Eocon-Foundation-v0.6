---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (7.6)
---

# (7.6)

_Source page: 158_



© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


7.2. policy evaluation


137


Algorithm 7.2. Functions for com-


function


lookahead


::


MDP


puting the lookahead state-action


value from a state


given an action


return


sum


s′


s′


for


s′


in


using an estimate of the value


end


function


for the MDP


. The sec-


function


lookahead


::


MDP


::


Vector


ond version handles the case when


is a vector.


return


sum


s′


for


s′


in


enumerate


))


end


Algorithm 7.3.


Iterative policy


function


iterative_policy_evaluation


::


MDP


k_max


evaluation, which iteratively com-


putes the value function for a pol-


0.0 for


in


icy


for MDP


with discrete state


for


in 1


k_max


and action spaces using


k_max


iter-


lookahead


))


for


in


ations.


end


return


end


Figure 7.3. Iterative policy eval-


iteration 1


iteration 2


uation used to evaluate an east-


moving policy on the hex world


problem (see appendix F.1). The


arrows indicate the direction rec-


ommended by the policy (i.e., al-


ways move east), and the colors in-


iteration 3


iteration 4


dicate the values associated with


the states. The values change with


each iteration.


10


10


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


138


chapter 7. exact solution methods


This


This equality is called the


Bellman expectation equation


equation


is


named


for


Richard E. Bellman, one of the


Policy evaluation can be done without iteration by solving the system of equa-


pioneers of dynamic program-


tions in the Bellman expectation equation directly. Equation (7.6) defines a set of


ming.


R. E.


Bellman,


Dynamic


Programming


. Princeton University


linear equations with


unknowns corresponding to the values at each state.


Press, 1957.


One way to solve this system of equations is to first convert it into matrix form: