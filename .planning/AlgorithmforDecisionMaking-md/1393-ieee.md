---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: IEEE
---

# IEEE

_Source page: 209_



Monte Carlo tree search


(algorithm 9.5) avoids the exponential complexity in the


Transactions on Computational Intel-


horizon by running


simulations from the current state.


During these simula-


ligence and AI in Games


, vol. 4, no. 1,


pp. 1–43, 2012.


tions, the algorithm updates estimates of the action value function


and


UCB stands for upper confidence


a record of the number of times a particular state-action pair has been selected,


bound. This is one of many strate-


. After running these


simulations from our current state


, we simply


gies discussed by P. Auer, N. Cesa-


choose the action that maximizes our estimate of


Bianchi, and P. Fischer, “Finite-


Time Analysis of the Multiarmed


A simulation (algorithm 9.6) begins by traversing the explored state space,


Bandit Problem,”


Machine Learning


consisting of the states for which we have estimates of


and


. We follow


vol. 47, no. 2–3, pp. 235–256, 2002.


an exploration strategy to choose actions from the various states. A common


The equation is derived from the


Chernoff-Hoeffding bound.


approach is to select the action that maximizes the


UCB1 exploration heuristic


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


188


chapter 9. online planning


Algorithm 9.4.


The sparse sam-


struct


SparseSampling


pling algorithm for finding an ap-


# problem


proximately optimal action online


# depth


for a discrete problem


from a


# number of samples


current state


to depth


with


# value function at depth d


samples per action. The returned


end


named tuple consists of the best


function


sparse_sampling


action


and its finite-horizon ex-


if


pected value


return


nothing


))


end


best


nothing


=-


Inf


for


in


0.0


for


in 1


s′


randstep


a′


u′


sparse_sampling


s′


-1


+=


u′


end


if


best


best


end


end


return


best


end


::


SparseSampling


)(


sparse_sampling


).


Figure 9.3. Sparse sampling with


Depth


Depth


10


applied to the hex world


problem. Visited tiles are colored


according to their estimated value.


The bordered tile is the initial state.


Compare to forward search in fig-


ure 9.2.


Depth


Depth


10


10


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


9.6. monte carlo tree search


189


Algorithm 9.5. The Monte Carlo


struct


MonteCarloTreeSearch


tree search policy for finding an ap-


# problem


proximately optimal action from a


# visit counts


current state


# action value estimates


# depth


# number of simulations


# exploration constant


# value function estimate


end


function


::


MonteCarloTreeSearch


)(


for


in 1


simulate!


end


return


argmax


->


[(


)],


end


log