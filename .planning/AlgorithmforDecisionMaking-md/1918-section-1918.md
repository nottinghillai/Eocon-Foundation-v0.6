---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ) =
---

# ) =

_Source page: 295_



cos


sin


As with the other actor-critic methods, we perform gradient descent on


and gradient ascent on


. For this approach to work in practice, a few addi-


tional techniques are needed. One is to generate experiences from a stochastic


policy to allow better exploration. It is often adequate to simply add zero-mean


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


274


chapter 13. actor-critic methods


We will discuss experience re-


play in section 17.7 in the context


Gaussian noise to actions generated by our deterministic policy


, as done


of reinforcement learning. Other


in algorithm 13.3. To encourage stability when learning


and


, we can use


techniques for stabilizing learn-


ing include using


target parameter-


experience replay.


izations


, described in the context


An example of this method and the effect of


on performance is given in


of neural representations by T. P.


Lillicrap, J. J. Hunt, A. Pritzel, N.


example 13.2.


Heess, T. Erez, Y. Tassa, D. Sil-


ver, and D. Wierstra, “Continu-


struct


DeterministicPolicyGradient


ous Control with Deep Reinforce-


# problem


ment Learning,” in


International


# initial state distribution


Conference on Learning Representa-


# depth


tions (ICLR)


, 2016. arXiv: 1509.029


# number of samples


71v6.


∇π


# gradient of deterministic policy π(θ, s)


# parameterized value function Q(ϕ,s,a)


∇Qϕ


# gradient of value function with respect to ϕ


∇Qa


# gradient of value function with respect to a


Algorithm 13.3. The deterministic


# policy noise


policy gradient method for com-


end


puting a policy gradient


∇Uθ


for a


deterministic policy


and a value


function


gradient


::


DeterministicPolicyGradient


function gradient


∇ℓϕ


for a continu-


∇π


∇π


ous action MDP


with initial state


∇Qϕ


∇Qa


∇Qϕ


∇Qa


distribution


. The policy is param-


π_rand


randn


()


eterized by


and has a gradient


∇Uθ


sum


∇π


∇Qa


))


-1


for


,(


))


in


enumerate


))


∇π


that produces a matrix where


∇ℓϕ


begin


each column is the gradient with


respect to that continuous action


s′


][


component. The value function


is


a′


s′


parameterized by


and has a gradi-


s′


a′


ent


∇Qϕ


with respect to the param-


return


∇Qϕ


s′


a′


∇Qϕ


))


eterization and gradient


∇Qa


with


end


respect to the action. This method


∇ℓϕ


sum


∇ℓϕ


for


in 1


length


-1


runs


rollouts to depth


, and per-


trajs


simulate


rand


),


π_rand


for


in 1


forms exploration using


-mean


return


mean


∇Uθ


for


in


trajs


),


mean


∇ℓϕ


for


in


trajs


Gaussian noise with standard de-


end


viation