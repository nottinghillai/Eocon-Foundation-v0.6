---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (14.15)
---

# (14.15)

_Source page: 313_



where


is our policy,


is our original reward function,


is our original transition


The log probability of a trajectory


model, and


is a parameter that controls the importance of maximizing the


is equal to the sum of the log of the


resulting likelihood of the trajectory. Since an adversary attempts to maximize the


individual state transition proba-


sum of adversarial reward, it is maximizing our expected negative return plus


bilities.


times the log probability of the resulting trajectory.


The adversarial transition


Â© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


292


chapter 14. policy validation


Example 14.5. An analysis of the


In our aircraft collision avoidance problem, we must balance safety in terms


trade-off between safety and op-


of collision probability with other metrics, such as the expected number of


erational efficiency when varying


parameters of different collision


advisory changes. Both of these can be implemented using trajectory metrics


avoidance systems.


that are additively decomposed by steps as done in equation (14.3), allowing


us to compute them using exact policy evaluation.


The plot here shows three curves associated with different parameterized


versions of the simple and optimal policies. The first curve shows the per-


formance of the simple policy on the two metrics as the


thresh


parameter


(defined in example 14.1) is varied. The second curve shows the performance


of the simple policy as


thresh


is varied. The third curve shows the optimal


policy as the parameter


is varied, where the cost of collision is


and the


cost of changing advisories is