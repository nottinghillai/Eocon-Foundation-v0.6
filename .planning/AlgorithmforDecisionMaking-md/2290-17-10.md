---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (17.10)
---

# (17.10)

_Source page: 360_



tempt to correct for this bias and


can lead to better performance. H.


van Hasselt, “Double Q-Learning,”


Our choice of actions affects which states we end up in, and therefore our


in


Advances in Neural Information


ability to estimate


accurately. To guarantee convergence of our action


Processing Systems (NIPS)


, 2010.


value function, we need to adopt some form of exploration policy, such as


greedy or softmax, just as we did for our model-based methods in the previous


chapter. Example 17.2 shows how to run a simulation with the


-learning update


rule and an exploration policy. Figure 17.1 illustrates this process on the hex world


problem.


Algorithm 17.2.


The


-learning


mutable struct


QLearning


update for model-free reinforce-


# state space (assumes 1:nstates)


ment learning, which can be ap-


# action space (assumes 1:nactions)


plied to problems with unknown


# discount


transition and reward functions.


# action value function


The update modifies


, which is a


# learning rate


end


matrix of state-action values. This


update function can be used to-


lookahead


model


::


QLearning


model


gether with an exploration strategy,


such as


-greedy, in the simulate


function


update!


model


::


QLearning


s′


function of algorithm 15.9. That


model


model


model


simulate function calls the update


+=


maximum


s′


])


])


function with


s′


, though this


return


model


learning implementation does not


end


use it.