---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: = (
---

# = (

_Source page: 157_



1:


1:


. Because the future states and rewards depend only on the current state


and action (as made apparent in the conditional independence assumptions in


figure 7.1), we can restrict our attention to policies that depend only on the current


state. In addition, we will primarily focus on


deterministic policies


because there is


guaranteed to exist in MDPs an optimal policy that is deterministic. Later chapters


discuss


stochastic policies


, where


denotes the probability that the policy


assigns to taking action


in state


at time


In infinite horizon problems with stationary transitions and rewards, we can


further restrict our attention to


stationary policies


, which do not depend on time. We


will write the action associated with stationary policy


in state


as


, without


the temporal subscript. In finite horizon problems, however, it may be beneficial


to select a different action depending on how many time steps are remaining. For


example, when playing basketball, it is generally not a good strategy to attempt a


half-court shot unless there are only a couple of seconds remaining in the game.


We can make stationary policies account for time by incorporating time as a state


variable.


Â© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


136


chapter 7. exact solution methods


The expected utility of executing


from state


is denoted as


. In the


context of MDPs,


is often referred to as the


value function


. An


optimal policy


Doing so is consistent with the


is a policy that maximizes expected utility:


maximum expected utility princi-


ple introduced in section 6.4.