---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (7.17)
---

# (7.17)

_Source page: 164_



Further applications of the Bellman backup once this equality holds do not change


the value function. An optimal policy can be extracted from


using equa-


tion (7.11). Value iteration is implemented in algorithm 7.8 and is applied to the


hex world problem in figure 7.6.


The implementation in algorithm 7.8 stops after a fixed number of iterations,


but it is also common to terminate the iterations early based on the maximum


change in value


, called the


Bellman residual


. If the Bellman residual


drops below a threshold


, then the iterations terminate. A Bellman residual


of


guarantees that the optimal value function obtained by value iteration is


12


12


See exercise 7.8.


within


δγ


of


Discount factors closer to


significantly inflate


this error, leading to slower convergence. If we heavily discount future reward (


closer to


), then we do not need to iterate as much into the future. This effect is


demonstrated in example 7.2.


13


S. P. Singh and R. C. Yee, “An


Knowing the maximum deviation of the estimated value function from the


Upper Bound on the Loss from


optimal value function,


, allows us to bound the maximum


Approximate Optimal-Value Func-


deviation of reward obtained under the extracted policy


from an optimal policy


tions,”


Machine Learning


, vol. 16,


13


no. 3, pp. 227–233, 1994.


. This


policy loss


is bounded by


ϵγ


Algorithm 7.8.


Value iteration,


struct


ValueIteration


which iteratively improves a value


k_max


# maximum number of iterations


function


to obtain an optimal pol-


end


icy for an MDP


with discrete state


and action spaces. The method ter-


function


solve


::


ValueIteration


::


MDP


minates after


k_max


iterations.


0.0 for


in


for


k_max


backup


for


in


end


return


ValueFunctionPolicy


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


7.5. value iteration


143


Figure 7.6. Value iteration in the


iteration 0


iteration 1


hex world problem to obtain an op-


timal policy. Each hex is colored ac-


cording to the value function, and


arrows indicate the policy that is


greedy with respect to that value


function.


iteration 2


iteration 3


iteration 4


iteration 5


iteration 6


iteration 7


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


144


chapter 7. exact solution methods


Example 7.2.


The effect of the


Consider a simple variation of the hex world problem, consisting of a straight


discount factor on convergence of


line of tiles with a single consuming tile at the end producing a reward of


value iteration. In each case, value


iteration was run until the Bellman


10


. The discount factor directly affects the rate at which reward from the


residual was less than


consuming tile propagates down the line to the other tiles, and thus how


quickly value iteration converges.


0.9


0.5


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


7.6. asynchronous value iteration


145