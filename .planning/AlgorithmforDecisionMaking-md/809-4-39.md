---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (4.39)
---

# (4.39)

_Source page: 107_



mis


obs


obs


or, alternatively, we can take a maximum likelihood approach.


Solving equation (4.38) may still be computationally challenging. One sim-


ple approach for discrete data sets is to replace missing entries with the most


6.5


0.9


4.2


commonly observed value, called the


marginal mode


. For example, in table 4.1, we


4.4


9.2


might replace all the missing values for


with its marginal mode of


7.8


Continuous data often lacks duplicates. However, we can fit a distribution


to continuous values and then use the mode of the resulting distribution. For


Table 4.2. Example of data with


example, we might fit a Gaussian distribution to the data in table 4.2, and then


continuous values.


fill in the missing entries with the mean of the observed values associated with


each variable. The top-left plot in figure 4.6 illustrates the effect of this approach


on two-dimensional data. The red lines show how values with missing first or


second components are paired with their imputed counterparts. We can then use


the observed and imputed data to arrive at a maximum likelihood estimate of


the parameters of a joint Gaussian distribution. As we can see, this method of


imputation does not always produce sensible predictions and the learned model


is quite poor.


We can often do better if we account for the probabilistic relationships between


the observed and unobserved variables. In figure 4.6, there is clearly correlation


between the two variables; hence, knowing the value of one variable can help


predict the value of the other variable. A common approach to imputation, called


nearest-neighbor imputation


, is to use the values associated with the instance that


is nearest with respect to a distance measure defined on the observed variables.


The top-right plot in figure 4.6 uses the Euclidean distance for imputation. This


approach tends to lead to better imputations and learned distributions.


An alternative approach is to fit a distribution to the fully observed data and


then use that distribution to infer the missing values. We can use the inference


algorithms from the previous chapter to perform this inference. For example, if


our data is discrete and we can assume a Bayesian network structure, we can use


variable elimination or Gibbs sampling to produce a distribution over the missing


variables for an instance from the observed variables. From this distribution, we


might use the mean or mode to impute the missing values. Alternatively, we can


Â© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


86


chapter 4. parameter learning


marginal mode


nearest


posterior sample


posterior mode


Marker is data that is:


Density ellipse estimated from:


Figure 4.6. A demonstration of im-


observed


all data (ground truth)


putation techniques. Shown here


missing


observed and imputed


are ellipses where the density of


imputed


observed only


the maximum likelihood estimate


of the joint distribution equals