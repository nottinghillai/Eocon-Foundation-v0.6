---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (9.2)
---

# (9.2)

_Source page: 219_



© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


198


chapter 9. online planning


Algorithm 9.8.


Heuristic search


struct


HeuristicSearch


runs


simulations starting from


# problem


an initial state


to a depth


. The


Uhi


# upper bound on value function


search is guided by a heuristic ini-


# depth


tial value function


Uhi


, which leads


# number of simulations


to optimality in the limit of simula-


end


tions if it is an upper bound on the


function


simulate!


::


HeuristicSearch


optimal value function.


for


in 1


greedy


rand


))


end


end


function


::


HeuristicSearch


)(


Uhi


for


in


for


in 1


simulate!


end


return


greedy


).


end


Figure 9.5. Heuristic search runs


simulations


10


simulations


simulations with Bellman updates


to improve a value function on the


hex world problem to obtain a pol-


icy from an initial state, shown here


with an additional black border.


These simulations are run to depth


20


simulations


50


simulations


with heuristic


) =


10


. Each


hex is colored according to the util-


ity function value in that iteration.


We see that the algorithm eventu-


ally finds an optimal policy.


10


10


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


9.8. labeled heuristic search


199


We run simulations with value updates until the current state is solved. In contrast


with the heuristic search in the previous section, which runs a fixed number of


iterations, this labeling process focuses computational effort on the most important


areas of the state space.


Algorithm 9.9.


Labeled heuris-


struct


LabeledHeuristicSearch


tic search, which runs simulations


# problem


starting from the current state to


Uhi


# upper bound on value function


depth


until the current state is


# depth


solved. The search is guided by a


# gap threshold


heuristic upper bound on the value


end


function


Uhi


and maintains a grow-


function


::


LabeledHeuristicSearch


)(


ing set of solved states. States are


solved


Uhi


for


in


],


Set


()


considered solved when their util-


while


solved


ity residuals fall below


. A value


simulate!


solved


function policy is returned.


end


return


greedy


).


end


Simulations in labeled heuristic search (algorithm 9.10) begin by running to


a maximum depth of


by following a policy that is greedy with respect to our


estimated value function


, similar to the heuristic search in the previous section.


We may stop a simulation before a depth of


if we reach a state that has been


labeled as solved in a prior simulation.


Algorithm 9.10. Simulations are


function


simulate!


::


LabeledHeuristicSearch


solved


run from the current state to a max-


visited


[]


imum depth


. We stop a simula-


for


in 1


tion at depth


or if we encounter a


if


solved


state that is in the set


solved


. After


break


a simulation, we call


label!


on the


end


states we visited in reverse order.


push!


visited


greedy


rand


))


end


while


isempty


visited


if


label!


solved


pop!


visited


))


break


end


end


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


200


chapter 9. online planning


After each simulation, we iterate over the states we visited during that simula-


tion in reverse order, performing a labeling routine on each state and stopping if a


state is found that is not solved. The labeling routine (algorithm 9.11) searches the


states in the


greedy envelope


of


, which is defined to be the states reachable from


under a greedy policy with respect to


. The state


is considered not solved if


there is a state in the greedy envelope of


whose utility residual is greater than


threshold


. If no such state is found, then


is marked as solved—as well as all


states in the greedy envelope of


because they must have converged as well. If


a state with a sufficiently large utility residual is found, then the utilities of all


states traversed during the search of the greedy enveloped are updated.


Figure 9.6 shows several different greedy envelopes. Figure 9.7 shows the states


traversed in a single iteration of labeled heuristic search. Figure 9.8 shows the


progression of heuristic search on the hex world problem.