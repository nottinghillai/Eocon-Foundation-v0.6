---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (13.6)
---

# (13.6)

_Source page: 290_



to-go


where


is the reward-to-go at step


in a particular trajectory


to-go


Algorithm 13.1 shows how to estimate


and


from rollouts. With


each iteration, we step


in the direction of


to maximize utility, and we


step


in the opposite direction of


to minimize our loss. This approach


can become unstable due to the dependency between the estimation of


and


, but this approach has worked well for a variety of problems. It is a common


practice to update the policy more frequently than the value function to improve


stability. The implementations in this chapter can easily be adapted to update the


value function only for a subset of the iterations that the policy is updated.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


13.2. generalized advantage estimation


269


Algorithm 13.1. A basic actor-critic


struct


ActorCritic


method for computing both a pol-


# problem


icy gradient and a value function


# initial state distribution


gradient for an MDP


with initial


# depth


state distribution


. The policy


# number of samples


is parameterized by


and has a


∇logπ


# gradient of log likelihood ∇logπ(θ,a,s)


# parameterized value function U(ϕ, s)


log-gradient


∇logπ


. The value func-


∇U


# gradient of value function ∇U(ϕ,s)


tion


is parameterized by


and the


end


gradient of its objective function is


∇U


. This method runs


rollouts to


function


gradient


::


ActorCritic


depth


. The results are used to up-


∇logπ


∇logπ


date


and


. The policy parameter-


∇U


∇U


ization is updated in the direction


πθ


of


∇Uθ


to maximize the expected


sum


-1


for


,(


))


in


enumerate


end


]))


value, whereas the value function


][


][


])


][


])


parameterization is updated in the


∇Uθ


sum


∇logπ


-1


for


, (


))


negative direction of


∇ℓϕ


to mini-


in


enumerate


end-1


]))


mize the value loss.


∇ℓϕ


sum


((


))


∇U


for


, (


))


in


enumerate


))


trajs


simulate


rand


),


πθ


for


in 1


return


mean


∇Uθ


for


in


trajs


),


mean


∇ℓϕ


for


in


trajs


end