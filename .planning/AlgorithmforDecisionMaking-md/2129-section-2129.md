---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ) =
---

# ) =

_Source page: 331_



and


apply equation (15.6). This process is repeated until we reach our initial state.


Such an optimal policy is computed in example 15.4.


Although this dynamic programming solution is optimal, the number of belief


states is


. We can formulate an infinite horizon, discounted version of the


J. C. Gittins, “Bandit Processes


problem that can be solved efficiently using the


Gittins allocation index


which


and Dynamic Allocation Indices,”


can be stored as a lookup table that specifies a scalar allocation index value, given


Journal of the Royal Statistical Society.


10


the number of pulls and the number of wins associated with an arm.


The arm


Series B (Methodological)


, vol. 41,


no. 2, pp. 148–177, 1979. J. Git-


that has the highest allocation index is the one that should be pulled next.


tins, K. Glazebrook, and R. Weber,


Multi-Armed Bandit Allocation In-


dices


, 2nd ed. Wiley, 2011.