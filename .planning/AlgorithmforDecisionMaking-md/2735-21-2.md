---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (21.2)
---

# (21.2)

_Source page: 451_



The assumption of full observability after the first step can cause QMDP to


poorly approximate the value of


information-gathering


actions, which are actions


that significantly reduce the uncertainty in the state. For example, looking over


oneâ€™s shoulder before changing lanes when driving is an information-gathering


action. QMDP can perform well in problems where the optimal policy does not


include costly information gathering.


We can generalize the QMDP approach to problems that may not have a small,


discrete state space. In such problems, the iteration in equation (21.1) may not be


feasible, but we may use one of the many methods discussed in earlier chapters for


obtaining an approximate action value function


. This value function might


be defined over a high-dimensional, continuous state space using, for example, a


neural network representation. The value function evaluated at a belief point is,


then,