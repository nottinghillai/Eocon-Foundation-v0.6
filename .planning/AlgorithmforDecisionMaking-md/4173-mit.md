---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: . MIT
---

# . MIT

_Source page: 682_



Press, 2015 (cit. on pp. 3, 511, 615).


139.


M. J. Kochenderfer and T. A. Wheeler,


Algorithms for Optimization


. MIT Press, 2019


(cit. on pp. 102, 172, 213, 250).


140.


M. J. Kochenderfer and J. P. Chryssanthacopoulos, “Robust Airborne Collision


Avoidance Through Dynamic Programming,” Massachusetts Institute of Technol-


ogy, Lincoln Laboratory, Project Report ATC-371, 2011 (cit. on p. 614).


141.


M. J. Kochenderfer, J. P. Chryssanthacopoulos, and P. Radecki, “Robustness of Opti-


mized Collision Avoidance Logic to Modeling Errors,” in


Digital Avionics Systems


Conference (DASC)


, 2010 (cit. on p. 289).


142.


D. Koller and N. Friedman,


Probabilistic Graphical Models: Principles and Techniques


MIT Press, 2009 (cit. on pp. 32, 36, 97).


143.


A. Kolmogorov,


Foundations of the Theory of Probability


, 2nd ed. Chelsea, 1956 (cit.


on p. 562).


144.


H. Koontz, “The Management Theory Jungle,”


Academy of Management Journal


vol. 4, no. 3, pp. 174–188, 1961 (cit. on p. 12).


145.


B. O. Koopman,


Search and Screening: General Principles with Historical Applications


Pergamon Press, 1980 (cit. on p. 11).


146.


F. Kschischang, B. Frey, and H.


A. Loeliger, “Factor Graphs and the Sum-Product


Algorithm,”


IEEE Transactions on Information Theory


, vol. 47, no. 2, pp. 498–519,


2001 (cit. on p. 53).


147.


A. Kuefler, J. Morton, T. A. Wheeler, and M. J. Kochenderfer, “Imitating Driver


Behavior with Generative Adversarial Networks,” in


IEEE Intelligent Vehicles Sym-


posium (IV)


, 2017 (cit. on p. 375).


148.


H. Kuhn, “Extensive Games and the Problem of Information,” in


Contributions to


the Theory of Games II


, H. Kuhn and A. Tucker, eds., Princeton University Press, 1953,


pp. 193–216 (cit. on p. 533).


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


references


661


149.


S. Kullback and R. A. Leibler, “On Information and Sufficiency,”


Annals of Mathe-


matical Statistics


, vol. 22, no. 1, pp. 79–86, 1951 (cit. on p. 567).


150.


S. Kullback,


Information Theory and Statistics


. Wiley, 1959 (cit. on p. 567).


151.


H. Kurniawati, D. Hsu, and W. S. Lee, “SARSOP: Efficient Point-Based POMDP


Planning by Approximating Optimally Reachable Belief Spaces,” in


Robotics: Science


and Systems


, 2008 (cit. on pp. 440, 442).


152.


Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-Based Learning Applied


to Document Recognition,”


Proceedings of the IEEE


, vol. 86, no. 11, pp. 2278–2324,


1998 (cit. on pp. 587, 588).


153.


R. Lee, M. J. Kochenderfer, O. J. Mengshoel, G. P. Brat, and M. P. Owen, “Adaptive


Stress Testing of Airborne Collision Avoidance Systems,” in


Digital Avionics Systems


Conference (DASC)


, 2015 (cit. on p. 294).


154.


J. Lehrer,


How We Decide


. Houghton Mifflin, 2009 (cit. on p. 122).


155.


T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D.


Wierstra, “Continuous Control with Deep Reinforcement Learning,” in


International


Conference on Learning Representations (ICLR)


, 2016. arXiv: 1509.02971v6 (cit. on


p. 274).


156.


L.


J. Lin, “Reinforcement Learning for Robots Using Neural Networks,” Ph.D.


dissertation, Carnegie Mellon University, 1993 (cit. on p. 345).


157.


R. J. A. Little and D. B. Rubin,


Statistical Analysis with Missing Data


, 3rd ed. Wiley,


2020 (cit. on p. 84).


158.


M. L. Littman, “Markov Games as a Framework for Multi-Agent Reinforcement


Learning,” in


International Conference on Machine Learning (ICML)


, 1994 (cit. on


p. 517).


159.


M. L. Littman, A. R. Cassandra, and L. P. Kaelbling, “Learning Policies for Par-


tially Observable Environments: Scaling Up,” in


International Conference on Machine


Learning (ICML)


, 1995 (cit. on p. 427).


160.


W. S. Lovejoy, “Computationally Feasible Bounds for Partially Observed Markov


Decision Processes,”


Operations Research


, vol. 39, no. 1, pp. 162–175, 1991 (cit. on


p. 445).


161.


O. Madani, S. Hanks, and A. Condon, “On the Undecidability of Probabilistic


Planning and Related Stochastic Optimization Problems,”


Artificial Intelligence


vol. 147, no. 1–2, pp. 5–34, 2003 (cit. on p. 427).


162.


S. Mannor, R. Y. Rubinstein, and Y. Gat, “The Cross Entropy Method for Fast Policy


Search,” in


International Conference on Machine Learning (ICML)


, 2003 (cit. on p. 218).


163.


H. Markowitz, “The Utility of Wealth,”


Journal of Political Economy


, vol. 60, no. 2,


pp. 151–158, 1952 (cit. on p. 114).


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


662


references


164.


Mausam and A. Kolobov,


Planning with Markov Decision Processes: An AI Perspective


Morgan & Claypool, 2012 (cit. on p. 197).


165.


S. B. McGrayne,


The Theory That Would Not Die


. Yale University Press, 2011 (cit. on


p. 30).


166.


R. C. Merton, “Optimum Consumption and Portfolio Rules in a Continuous-Time


Model,”


Journal of Economic Theory


, vol. 3, no. 4, pp. 373–413, 1971 (cit. on p. 4).


167.


N. Meuleau, K.


E. Kim, L. P. Kaelbling, and A. R. Cassandra, “Solving POMDPs


by Searching the Space of Finite Policies,” in


Conference on Uncertainty in Artificial


Intelligence (UAI)


, 1999 (cit. on p. 481).


168.


D. A. Mindell,


Between Human and Machine: Feedback, Control, and Computing Before


Cybernetics


. JHU Press, 2002 (cit. on p. 11).


169.


V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and


M. Riedmiller, “Playing Atari with Deep Reinforcement Learning,” 2013. arXiv:


1312.5602v1 (cit. on p. 345).


170.


N. Moehle, E. Busseti, S. Boyd, and M. Wytock, “Dynamic Energy Management,”


in


Large Scale Optimization in Supply Chains and Smart Manufacturing


, Springer, 2019,


pp. 69–126 (cit. on p. 208).


171.


G. Molenberghs, G. Fitzmaurice, M. G. Kenward, A. Tsiatis, and G. Verbeke, eds.,


Handbook of Missing Data Methodology


. CRC Press, 2014 (cit. on p. 82).


172.


A. Moore, “Efficient Memory-Based Learning for Robot Control,” Ph.D. dissertation,


University of Cambridge, 1990 (cit. on p. 612).


173.


A. W. Moore, “Simplicial Mesh Generation with Applications,” Ph.D. dissertation,


Cornell University, 1992 (cit. on pp. 168, 170).


174.


A. W. Moore and C. G. Atkeson, “Prioritized Sweeping: Reinforcement Learning


with Less Data and Less Time,”


Machine Learning


, vol. 13, no. 1, pp. 103–130, 1993


(cit. on p. 321).


175.


G. E. Moore, “Cramming More Components onto Integrated Circuits,”


Electronics


vol. 38, no. 8, pp. 114–117, 1965 (cit. on p. 11).


176.


O. Morgenstern and J. von Neumann,


Theory of Games and Economic Behavior


. Prince-


ton University Press, 1953 (cit. on p. 8).


177.


R. Motwani and P. Raghavan,


Randomized Algorithms


. Cambridge University Press,


1995 (cit. on p. 54).


178.


B. Müller, J. Reinhardt, and M. T. Strickland,


Neural Networks


. Springer, 1995 (cit.


on p. 581).


179.


K. P. Murphy,


Probabilistic Machine Learning: An Introduction


. MIT Press, 2022 (cit.


on p. 71).


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


references


663


180.


R. B. Myerson,


Game Theory: Analysis of Conflict


. Harvard University Press, 1997 (cit.


on p. 493).


181.


R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella, “Taming Decentral-


ized POMDPs: Towards Efficient Policy Computation for Multiagent Settings,” in


International Joint Conference on Artificial Intelligence (IJCAI)


, 2003 (cit. on p. 550).


182.


J. Nash, “Non-Cooperative Games,”


Annals of Mathematics


, pp. 286–295, 1951 (cit.


on p. 498).


183.


R. E. Neapolitan,


Learning Bayesian Networks


. Prentice Hall, 2003 (cit. on p. 97).


184.


A. Y. Ng, D. Harada, and S. Russell, “Policy Invariance Under Reward Transforma-


tions: Theory and Application to Reward Shaping,” in


International Conference on


Machine Learning (ICML)


, 1999 (cit. on p. 343).


185.


A. Y. Ng and M. Jordan, “A Policy Search Method for Large MDPs and POMDPs,”


in


Conference on Uncertainty in Artificial Intelligence (UAI)


, 2000 (cit. on p. 232).


186.


N. J. Nilsson,


The Quest for Artificial Intelligence


. Cambridge University Press, 2009


(cit. on pp. 7, 9).


187.


N. Nisan, T. Roughgarden, É. Tardos, and V. V. Vazirani, eds.,


Algorithmic Game


Theory


. Cambridge University Press, 2007 (cit. on p. 503).


188.


F. A. Oliehoek and C. Amato,


A Concise Introduction to Decentralized POMDPs


Springer, 2016 (cit. on p. 545).


189.


C. Papadimitriou and J. Tsitsiklis, “The Complexity of Markov Decision Processes,”


Mathematics of Operation Research


, vol. 12, no. 3, pp. 441–450, 1987 (cit. on pp. 427,


549).


190.


J. Pearl,


Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference


Morgan Kaufmann, 1988 (cit. on p. 36).


191.


J. Pearl,


Causality: Models, Reasoning, and Inference


, 2nd ed. Cambridge University


Press, 2009 (cit. on p. 33).


192.


J. Peng and R. J. Williams, “Incremental Multi-Step Q-Learning,”


Machine Learning


vol. 22, no. 1–3, pp. 283–290, 1996 (cit. on p. 341).


193.


J. Peters and S. Schaal, “Reinforcement Learning of Motor Skills with Policy Gra-


dients,”


Neural Networks


, vol. 21, no. 4, pp. 682–697, 2008 (cit. on pp. 234, 243,


253).


194.


M. Peterson,


An Introduction to Decision Theory


. Cambridge University Press, 2009


(cit. on p. 111).


195.


A. Pinkus, “Approximation Theory of the MLP Model in Neural Networks,”


Acta


Numerica


, vol. 8, pp. 143–195, 1999 (cit. on p. 582).


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


664


references


196.


R. Platt Jr., R. Tedrake, L. P. Kaelbling, and T. Lozano-Pérez, “Belief Space Planning


Assuming Maximum Likelihood Observations,” in


Robotics: Science and Systems


2010 (cit. on p. 454).


197.


D. A. Pomerleau, “Efficient Training of Artificial Neural Networks for Autonomous


Navigation,”


Neural Computation


, vol. 3, no. 1, pp. 88–97, 1991 (cit. on p. 355).


198.


W. Poundstone,


Prisoner’s Dilemma


. Doubleday, 1992 (cit. on p. 621).


199.


P. Poupart and C. Boutilier, “Bounded Finite State Controllers,” in


Advances in


Neural Information Processing Systems (NIPS)


, 2003 (cit. on p. 475).


200.


W. B. Powell,


Reinforcement Learning and Stochastic Optimization


. Wiley, 2022 (cit. on


p. 161).


201.


W. B. Powell,


Approximate Dynamic Programming: Solving the Curses of Dimensionality


2nd ed. Wiley, 2011 (cit. on p. 161).


202.


M. L. Puterman,


Markov Decision Processes: Discrete Stochastic Dynamic Programming


Wiley, 2005 (cit. on p. 133).


203.


M. L. Puterman and M. C. Shin, “Modified Policy Iteration Algorithms for Dis-


counted Markov Decision Problems,”


Management Science


, vol. 24, no. 11, pp. 1127–


1137, 1978 (cit. on p. 141).


204.


J. Robinson, “An Iterative Method of Solving a Game,”


Annals of Mathematics


pp. 296–301, 1951 (cit. on p. 505).


205.


R. W. Robinson, “Counting Labeled Acyclic Digraphs,” in


Ann Arbor Conference on


Graph Theory


, 1973 (cit. on p. 99).


206.


S. Ross and J. A. Bagnell, “Efficient Reductions for Imitation Learning,” in


Inter-


national Conference on Artificial Intelligence and Statistics (AISTATS)


, 2010 (cit. on


p. 358).


207.


S. Ross and B. Chaib-draa, “AEMS: An Anytime Online Search Algorithm for


Approximate Policy Refinement in Large POMDPs,” in


International Joint Conference


on Artificial Intelligence (IJCAI)


, 2007 (cit. on p. 464).


208.


S. Ross, G. J. Gordon, and J. A. Bagnell, “A Reduction of Imitation Learning and


Structured Prediction to No-Regret Online Learning,” in


International Conference on


Artificial Intelligence and Statistics (AISTATS)


, vol. 15, 2011 (cit. on p. 358).


209.


S. Ross, J. Pineau, S. Paquet, and B. Chaib-draa, “Online Planning Algorithms for


POMDPs,”


Journal of Artificial Intelligence Research


, vol. 32, pp. 663–704, 2008 (cit.


on p. 453).


210.


D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning Representations by


Back-Propagating Errors,”


Nature


, vol. 323, pp. 533–536, 1986 (cit. on p. 585).


211.


G. A. Rummery and M. Niranjan, “On-Line Q-Learning Using Connectionist Sys-


tems,” Cambridge University, Tech. Rep. CUED/F-INFENG/TR 166, 1994 (cit. on


p. 338).


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


references


665


212.


S. Russell and P. Norvig,


Artificial Intelligence: A Modern Approach


, 4th ed. Pearson,


2021 (cit. on pp. 2, 116).


213.


D. Russo, B. V. Roy, A. Kazerouni, I. Osband, and Z. Wen, “A Tutorial on Thompson


Sampling,”


Foundations and Trends in Machine Learning


, vol. 11, no. 1, pp. 1–96, 2018


(cit. on p. 306).


214.


A. Ruszczyński, “Risk-Averse Dynamic Programming for Markov Decision Pro-


cesses,”


Mathematical Programming


, vol. 125, no. 2, pp. 235–261, 2010 (cit. on p. 282).


215.


T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever, “Evolution Strategies as a


Scalable Alternative to Reinforcement Learning,” 2017. arXiv: 1703.03864v2 (cit.


on p. 224).


216.


T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized Experience Replay,” in


International Conference on Learning Representations (ICLR)


, 2016 (cit. on p. 345).


217.


P. J. H. Schoemaker, “The Expected Utility Model: Its Variants, Purposes, Evidence


and Limitations,”


Journal of Economic Literature


, vol. 20, no. 2, pp. 529–563, 1982 (cit.


on p. 111).


218.


J. Schulman, S. Levine, P. Moritz, M. Jordan, and P. Abbeel, “Trust Region Policy


Optimization,” in


International Conference on Machine Learning (ICML)


, 2015 (cit. on


p. 254).


219.


J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-Dimensional


Continuous Control Using Generalized Advantage Estimation,” in


International


Conference on Learning Representations (ICLR)


, 2016. arXiv: 1506.02438v6 (cit. on


p. 269).


220.


J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy


Optimization Algorithms,” 2017. arXiv: 1707.06347v2 (cit. on p. 257).


221.


S. Seuken and S. Zilberstein, “Memory-Bounded Dynamic Programming for Dec-


POMDPs,” in


International Joint Conference on Artificial Intelligence (IJCAI)


, 2007 (cit.


on p. 550).


222.


S. Seuken and S. Zilberstein, “Formal Models and Algorithms for Decentralized


Decision Making Under Uncertainty,”


Autonomous Agents and Multi-Agent Systems


vol. 17, no. 2, pp. 190–250, 2008 (cit. on p. 549).


223.


R. D. Shachter, “Evaluating Influence Diagrams,”


Operations Research


, vol. 34, no. 6,


pp. 871–882, 1986 (cit. on p. 119).


224.


R. D. Shachter, “Probabilistic Inference and Influence Diagrams,”


Operations Re-


search


, vol. 36, no. 4, pp. 589–604, 1988 (cit. on p. 119).


225.


R. D. Shachter, “Efficient Value of Information Computation,” in


Conference on


Uncertainty in Artificial Intelligence (UAI)


, 1999 (cit. on p. 119).


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


666


references


226.


A. Shaiju and I. R. Petersen, “Formulas for Discrete Time LQR, LQG, LEQG and


Minimax LQG Optimal Control Problems,”


IFAC Proceedings Volumes


, vol. 41, no. 2,


pp. 8773–8778, 2008 (cit. on pp. 148, 149).


227.


G. Shani, J. Pineau, and R. Kaplow, “A Survey of Point-Based POMDP Solvers,”


Autonomous Agents and Multi-Agent Systems


, vol. 27, pp. 1–51, 2012 (cit. on p. 432).


228.


C. E. Shannon, “A Mathematical Theory of Communication,”


Bell System Technical


Journal


, vol. 27, no. 4, pp. 623–656, 1948 (cit. on p. 565).


229.


L. S. Shapley, “Stochastic Games,”


Proceedings of the National Academy of Sciences


vol. 39, no. 10, pp. 1095–1100, 1953 (cit. on p. 517).


230.


Z. R. Shi, C. Wang, and F. Fang, “Artificial Intelligence for Social Good: A Survey,”


2020. arXiv: 2001.01818v1 (cit. on p. 12).


231.


Y. Shoham and K. Leyton-Brown,


Multiagent Systems: Algorithmic, Game Theoretic,


and Logical Foundations


. Cambridge University Press, 2009 (cit. on pp. 493, 515).


232.


D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller, “Determin-


istic Policy Gradient Algorithms,” in


International Conference on Machine Learning