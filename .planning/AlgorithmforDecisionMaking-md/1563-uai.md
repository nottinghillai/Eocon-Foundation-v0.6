---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (UAI)
---

# (UAI)

_Source page: 254_



, 2000.


ple 11.1 demonstrates the sensitivity of the policy gradient to the policy parame-


terization. Finite differences for policy optimization can perform poorly when the


parameters differ in scale.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


11.1. finite difference


233


Algorithm 11.2. A method for es-


struct


FiniteDifferenceGradient


timating a policy gradient using fi-


# problem


nite differences for a problem


, a


# initial state distribution


parameterized policy


, and


# depth


a policy parameterization vector


# number of samples


Utility estimates are made from


# step size


end


rollouts to depth


. The step size is


given by


function


gradient


::


FiniteDifferenceGradient


length


Δθ


==


0.0 for


in 1


sum


-1


for


, (


))


in


enumerate


))


mean


simulate


rand


),


->


),


))


for


in 1


ΔU


Δθ


))


for


in 1


return


ΔU


end


Example 11.1. An example of how


Consider a single-state, single-step MDP with a one-dimensional continuous


policy parameterization has a sig-


action space and a reward function