---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (16.4)
---

# (16.4)

_Source page: 345_



otherwise


Hence, underexplored states have value


, providing an incentive


max


to explore them. This exploration incentive relieves us of needing a separate


exploration mechanism. We simply choose our actions greedily with respect to


the value function derived from our transition and reward estimates. Example 16.2


demonstrates


-greedy and R-MAX exploration.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


324


chapter 16. model-based methods


Algorithm 16.7.


The R-MAX


mutable struct


RmaxMDP


exploration strategy modifies the


# state space (assumes 1:nstates)


transition and reward model from


# action space (assumes 1:nactions)


maximum likelihood estimation.


# transition count N(s,a,s′)


It assigns the maximum reward


# reward sum ρ(s, a)


rmax


to


any


underexplored


# discount


# value function


state-action pair, defined as being


planner


those that have been tried fewer


# count threshold


than


times. In addition, all


rmax


# maximum reward


underexplored state-action pairs


end


are


modeled


as


transitioning


to the same state. This


RmaxMDP


function


lookahead


model


::


RmaxMDP


can be used as a replacement


model


model


model


for


the


MaximumLikelihoodMDP


sum


model


])


introduced in algorithm 16.1.


if


model


return


model


rmax


end


model


s′


model


s′


return


sum


s′


s′


for


s′


in


end


function


backup


model


::


RmaxMDP


return


maximum


lookahead


model


for


in


model


end


function


update!


model


::


RmaxMDP


s′


model


s′


+=


model


+=


update!


model


planner


model


s′


return


model


end


function


MDP


model


::


RmaxMDP


model


model


model


model


model


rmax


similar


),


similar


),


model


model


rmax


for


in


for


in


sum


])


if


] .


0.0


1.0


rmax


else


end


end


end


return


MDP


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


16.3. exploration


325


Example 16.2. Demonstration of


We can apply


-greedy exploration to maximum likelihood model estimates


-greedy and R-MAX exploration.


constructed while interacting with the environment. The code that follows


initializes the counts, rewards, and utilities to zero. It uses full updates to


the value function with each step. For exploration, we choose a random


action with probability


0.1


. The last line runs a simulation (algorithm 15.9)


of problem


for


100


steps starting in a random initial state:


zeros


length


),


length


),


length


))


zeros


length


),


length


))


zeros


length


))


planner


FullUpdate


()


model


MaximumLikelihoodMDP


planner


EpsilonGreedyExploration


0.1


simulate


model


100


rand


))


Alternatively, we can use R-MAX with an exploration threshold of


We can act greedily with respect to the R-MAX model:


rmax


maximum


for


in


in


model


RmaxMDP


planner


rmax


EpsilonGreedyExploration


simulate


model


100


rand


))


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


326


chapter 16. model-based methods