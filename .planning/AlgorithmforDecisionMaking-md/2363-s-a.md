---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: '|S||A|'
---

# |S||A|

_Source page: 374_



. Thus, the computational


complexity per experience tuple is greater for Sarsa(


). However, Sarsa(


) often converges


using fewer experience tuples.


Exercise 17.6.


What is the behavior of


in the limit as


? What is the behavior of


in the limit as


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


17.9. exercises


353


Solution:


For


, we perform the following update rules:


max


) +


) +


αδ


for all


γλ


for all


In the limit as


, for our first iteration, we compute the temporal difference error


and we increment the visit count


. In the action value function update, the only


nonzero


is at


, so we perform


) +


αδ


. Finally,


we reset all the visit counts to zero. From this, we can see that in the limit as


, we


have no eligibility traces and we are performing a straightforward


-learning update.


In the limit as


, our visit counts will accumulate and we have full eligibility traces,


which will spread the reward over all previously visited state-action pairs.


Exercise 17.7.


Compute


using Sarsa(


) after following the trajectory