---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (15.1)
---

# (15.1)

_Source page: 323_



Algorithm 15.2 provides an implementation of this. Example 15.1 illustrates how


to compute these posterior distributions from counts of wins and losses.


Algorithm 15.2. The Bayesian up-


struct


BanditModel


date function for bandit models.


# vector of beta distributions


After observing reward


after tak-


end


ing action


, we update the beta dis-


tribution associated with that ac-


function


update!


model


::


BanditModel


tion by incrementing the appropri-


StatsBase


params


model


])


model


Beta


))


ate parameter.


return


model


end


greedy action


is one that maximizes our expected immediate rewardâ€”or, in


other words, the posterior probability of winning in the context of our binary


bandit problem. There may be multiple greedy actions. We do not always want to


select a greedy action because we may miss out on discovering another action that


may actually provide higher reward in expectation. We can use the information


from the beta distributions associated with the different actions to drive our


exploration of nongreedy actions.