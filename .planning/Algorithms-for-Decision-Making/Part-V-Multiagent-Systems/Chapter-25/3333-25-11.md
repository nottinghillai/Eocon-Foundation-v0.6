---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (25.11)
---

# (25.11)

_Source page: 545_



In the implementation here, we use the property that each state of an MG policy


is a simple game policy whose reward is the corresponding


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


524


chapter 25. sequential problems


Algorithm 25.7. Fictitious play for


mutable struct


MGFictitiousPlay


agent


in an MG


that main-


# Markov game


tains counts


Ni


of other agent ac-


# agent index


tion selections over time for each


Qi


# state-action value estimates


state and averages them, assuming


Ni


# state-action counts


that this is their stochastic policy.


end


It then computes a best response


function


MGFictitiousPlay


::


MG


to this policy and performs the cor-


responding utility-maximizing ac-


Qi


Dict


((


=>


)[


for


in


for


in


joint


))


tion.


Ni


Dict


((


aj


=>


1.0 for


in


for


in


for


aj


in


])


return


MGFictitiousPlay


Qi


Ni


end


function


πi


::


MGFictitiousPlay


)(


Qi


πi


πi


πi


Qi


πi′


SimpleGamePolicy


ai


=>


πi


Ni


ai


for


ai


in


])


πi′


MGPolicy


=>


πi′


for


in


πi′


for


in


sum


πi


Qi


probability


for


in


joint


))


reward


sum


transition


s′


s′


for


s′


in


ai


joint


SimpleGamePolicy


ai


),


))


ai


argmax


πi


])


return


SimpleGamePolicy


ai


end


function


update!


πi


::


MGFictitiousPlay


s′


Qi


πi


πi


πi


Qi


for


aj


in


enumerate


πi


Ni


aj


+=


end


πi′


SimpleGamePolicy


ai


=>


πi


Ni


ai


for


ai


in


])


πi′


MGPolicy


=>


πi′


for


in


πi′


for


in


sum


πi


Qi


probability


for


in


joint


))


)[


sum


s′


s′


for


s′


in


for


in


joint


πi


Qi


end


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


25.4. fictitious play


525


Example 25.2. Fictitious play on


The predator-prey hex world MG (appendix F.13) has one predator (red)


the predator-prey hex world prob-


and one prey (blue). If the predator catches the prey, it receives a reward of


lem. Stochasticity was introduced


when initializing the policies to bet-


10


and the prey receives a reward of


100


. Otherwise, both agents receive a


ter show learning trends.


reward. The agents move simultaneously. We apply fictitious play with


resets to the initial state every


10


steps.


We observe that the predator learns to chase the prey and the prey learns


to flee. Interestingly, the predator also learns that the prey runs to the east


corner and waits. The prey learns that if it waits at this corner, it can flee


from the predator immediately as it jumps toward the prey. Here, the prey


evades the predator by moving west when the predator moves north east.


Here is a plot of the learned opponent model of the highlighted state (both


predator and prey hex locations) for both the predator and the prey:


opponent model


policy


east


north east


north west


0.5


west


action


south west


north east


predator


south east


0.5


west


action


prey


200


300


200


300


100


100


iteration


iteration


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


526


chapter 25. sequential problems