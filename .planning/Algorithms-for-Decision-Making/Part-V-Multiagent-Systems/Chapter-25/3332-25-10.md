---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (25.10)
---

# (25.10)

_Source page: 545_



for each agent


As the distributions of the other agentsâ€™ actions change, we must update the


utilities. The utilities in MGs are significantly more difficult to compute than


simple games because of the state dependency. As described in section 25.2.1, any


assignment of fixed policies of others


induces an MDP. In fictitious play,


is determined by equation (25.9). Instead of solving an MDP at each update, it is


common to apply the update periodically, a strategy adopted from asynchronous


value iteration. An example of fictitious play is given in example 25.2.


Our policy


for a state


is derived from a given opponent model


and


computed utility


. We then select a best response:


arg max