---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (24.11)
---

# (24.11)

_Source page: 531_



ferred to as infinitesimal because


∂π


as


. We use this learn-


ing rate in our implementation.


17


This


with learning rate


may need to be projected back to a valid proba-


S. Singh, M. Kearns, and Y. Man-


sour, “Nash Convergence of Gra-


bility distribution, just as in section 23.4 for POMDP policies.


dient Dynamics in General-Sum


In practice, however, an agent


knows only its own policy


, not the policies


Games,” in


Conference on Uncer-


of the others, making the computation of the gradient difficult. But agents do


tainty in Artificial Intelligence (UAI)


2000.


observe the joint actions


that are performed. Although we could try to estimate


18


This approach is used in


their policies as done in fictitious play, one simple approach is to assume the


gener-


alized infinitesimal gradient ascent


18


policy of the other agents is to replay their most recent action.


The gradient