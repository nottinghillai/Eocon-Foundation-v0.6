---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: '0.15'
---

# 0.15

_Source page: 631_



. If we bump against the outer border of the grid, then


Figure F.1. Actions in the hex world


problem have probabilistic effects.


we do not move at all, at a cost of


1.0


610


appendix f. problems


Taking any action in certain cells gives us a specified reward and then transports


us to a terminal state. No further reward is received in the terminal state. The total


The straight-line formulation is


number of states in the hex world problem is thus the number of tiles plus


, for


similar to the


hall problem


, a com-


the terminal state. Figure F.2 shows an optimal policy for two hex world problem


mon benchmark MDP. See, for ex-


configurations used throughout this book. We refer to the larger instance as ‘‘hex


ample, L. Baird, “Residual Algo-


rithms: Reinforcement Learning


world’’ and to the smaller, simpler instance as ‘‘straight-line hex world.’’


The


with Function Approximation,” in


straight-line hex world formulation is used to illustrate how reward is propagated


International Conference on Machine


Learning (ICML)


, 1995.


from its single reward-bearing state on the rightmost cell.


Figure F.2. The standard hex world


standard hex world


straight-line hex world


and straight-line hex world prob-


lems. The top row shows the base


problem setup and colors hexes


that have terminal rewards. The


-10


10


10


bottom row shows an optimal pol-


icy for each problem, colored ac-


cording to the expected value, with


arrows indicating the action to take


in each state.


10


10