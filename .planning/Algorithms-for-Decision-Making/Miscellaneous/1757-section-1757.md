---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ) =
---

# ) =

_Source page: 281_



Kullback-Leibler divergence. After


computing the natural policy gra-


dient ascent direction, a line search


is conducted to ensure that the up-


dated policy improves the policy


reward and adheres to the diver-


gence constraint. The line search


starts from the estimated maxi-


mum step size and reduces the step


size along the ascent direction until


a satisfactory point is found.


Figure 12.5. Trust region policy op-


timization applied to the simple


regulator problem with rollouts to


depth


10


with


and


. The


optimal policy parameterization is


20


0.5


shown in black.


expected reward


40


0.5


iteration


Â© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


260


chapter 12. policy gradient optimization


Example 12.1. An example of one


Consider applying TRPO to the Gaussian policy


from example 11.3


iteration of trust region policy op-


to the single-state MDP from example 11.1 with


. Recall that the gradient


timization.


of the log policy likelihood is


log