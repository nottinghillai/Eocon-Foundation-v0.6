---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: UCB1
---

# UCB1

_Source page: 334_



horizon =


10


0.7


horizon =


100


horizon =


200


0.6


mean reward


0.5


10


The softmax strategy performs best for large values of


, which prioritize pulling arms


with higher expected reward according to the current belief. Quantile exploration performs


better with longer horizons, independent of its parameterization. The size of the confidence


bound


does not significantly affect performance except for values very close to


or


The UCB1 strategy performs best with small positive values of the exploration scalar


. The


expected reward decays as


increases. All three policies can be tuned to produce similar


maximal expected rewards.


Exercise 15.2.


Give an example of a practical application of a multiarmed bandit problem.


Solution:


There are many multiarmed bandit problems. Consider, for example, a news


company that would like to maximize interaction (clicks) on articles on its website. The


Â© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


15.8. exercises


313


company may have several articles to display, but it must select one article to display at any


given time. This problem is a multiarmed bandit problem because a user will either click


article


with probability


. Exploration would consist


or not click with probability


of displaying articles on the website and observing the number of clicks, and exploitation


would consist of displaying the article likely to lead to the highest number of clicks. This


problem is related to


A/B testing


, where companies test different versions of a website to


determine which version yields the most interactions.


Exercise 15.3.


Given a one-armed bandit with a prior of


Beta