---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: '0.00'
---

# 0.00

_Source page: 314_



0.5


1.5


Advisory changes


We can see that the optimal policy dominates the curves generated by the


parameterized simple policies. When


is close to


, then we are very safe, but


we have to tolerate more advisory changes. As


goes to


, we are less safe


but do not produce advisories. Given a particular threshold level of safety,


we are able to create an optimized policy that has fewer advisory changes in


expectation than either of the simple parametric policies.


Â© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


14.5. adversarial analysis


293


model is deterministic; the state transitions to exactly what the adversary specifies


as its action.


Algorithm 14.1 implements this conversion to an adversarial problem. It as-


sumes a discrete state and action space, which can then be solved using one of


the dynamic programming algorithms in chapter 7. The solution is an adversarial


policy that maps states to states. Given an initial state, we can generate a trajectory


that minimizes our reward given some level of probability. Since the problem


is deterministic, it is actually a search problem, and any of the algorithms in


appendix E can be used. If our problem is high-dimensional or continuous, we


may use one of the approximate solution techniques discussed in chapters 8 and 9.


Algorithm 14.1. Conversion to an


function


adversarial


::


MDP


adversarial problem, given a pol-


icy


. An adversarial agent tries to


ğ’®â€²


ğ’œâ€²


change the outcomes of our policy


Râ€²


zeros


length


ğ’®â€²


),


length


ğ’œâ€²


))


actions so as to balance minimiz-


Tâ€²


zeros


length


ğ’®â€²


),


length


ğ’œâ€²


),


length


ğ’®â€²


))


ing our original utility and max-


for


in


ğ’®â€²


imizing the likelihood of the tra-


for


in


ğ’œâ€²


Râ€²


= -


))


log


),


))


jectory. The parameter


controls


Tâ€²


how important it is to maximize


end


the likelihood of the resulting tra-


end


jectory. It returns an MDP whose


return


MDP


Tâ€²


Râ€²


transition and reward models are


end


represented as matrices.


Sometimes we are interested in finding the


most likely failure


associated with


a policy for a particular definition of failure. In some problems, failure can be


defined as entering a particular state. For example, a collision may be considered


a failure in our collision avoidance problem. Other problems may require a more


10


M. Bouton, J. Tumova, and M. J.


complicated definition of failure that goes beyond just entering a subset of the


Kochenderfer, â€œPoint-Based Meth-


ods for Model Checking in Partially


state space. For example, we may want to specify failure using a


temporal logic


Observable Markov Decision Pro-


which is a way to represent and reason about propositions qualified in terms of


cesses,â€ in


AAAI Conference on Arti-


ficial Intelligence (AAAI)


, 2020.


time. In many cases, however, we can use these failure specifications to create an


10


augmented state space that we can then solve.


Â© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


294


chapter 14. policy validation


With the failure states defined, we can solve for the most likely failure trajectory


by changing the reward function in equation (14.15) to


if


is terminal and not a failure