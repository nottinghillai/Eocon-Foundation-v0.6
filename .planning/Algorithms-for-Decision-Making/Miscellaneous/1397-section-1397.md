---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ) =
---

# ) =

_Source page: 211_



, the bonus is defined to be infinity. With


in the denominator, the exploration bonus is higher for actions that have


not been tried as frequently. Algorithm 9.7 implements this exploration strategy.


We will discuss many other exploration strategies later in chapter 15.


As we take actions specified by algorithm 9.7, we step into new states sampled


from the generative model


, similar to the sparse sampling method. We


increment the visit count


and update


to maintain the mean value.


At some point, we will either reach the maximum depth or a state that we


have not yet explored. If we reach an unexplored state


, we initialize


and


to zero for each action


. We may modify algorithm 9.6 to initialize these


counts and value estimates to some other values based on prior expert knowledge


of the problem. After initializing


and


, we then return a value estimate at the


state


. It is common to estimate this value through a rollout of some policy using


the process outlined in section 9.2.


Examples 9.3 to 9.7 work through an illustration of Monte Carlo tree search


applied to the 2048 problem. Figure 9.4 shows a search tree generated by running


Monte Carlo tree search on 2048. Example 9.8 discusses the impact of using


different strategies for estimating values.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


190


chapter 9. online planning


Algorithm 9.6. A method for run-


function


simulate!


::


MonteCarloTreeSearch


ning a Monte Carlo tree search


if


simulation starting from state


to


return


depth


end


TR


TR


if


haskey


, (


first


)))


for


in


[(


)]


[(


)]


0.0


end


return


end


explore


s′


TR


simulate!


s′


-1


[(


)]


+=


[(


)]


+=


[(


)])


[(


)]


return


end


Algorithm 9.7. An exploration pol-


bonus


Nsa


Ns


Nsa


==


Inf


sqrt


log


Ns


Nsa


icy used in Monte Carlo tree search


when determining which nodes to


function


explore


::


MonteCarloTreeSearch


traverse through the search tree.


The policy is determined by a dic-


Ns


sum


[(


)]


for


in


tionary of state-action visitation


return


argmax


->


[(


)]


bonus


[(


)],


Ns


),


counts


and values


, as well as


end


an exploration parameter


. When


[(


)]


, the policy returns


infinity.


Example 9.3. An example of solv-


Consider using Monte Carlo tree search to play 2048 (appendix F.2) with


ing 2048 with Monte Carlo tree


a maximum depth


10


, an exploration parameter


100


, and a


10


-step


search.


random rollout to estimate


. Our first simulation expands the starting


state. The count and value are initialized for each action from the initial state:


left


down


right


up


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


9.6. monte carlo tree search


191


Example 9.4. A (continued) ex-


The second simulation begins by selecting the best action from the initial


ample of solving 2048 with Monte


state according to our exploration strategy in equation (9.1). Because all


Carlo tree search.


states have the same value, we arbitrarily choose the first action,


left


. We


then sample a new successor state and expand it, initializing the associated


counts and value estimates. A rollout is run from the successor state and its


value is used to update the value of


left


left


down


right


up


72


) =


72