---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ) =
---

# ) =

_Source page: 279_



min


max


did in previous algorithms, its role


Clamping the probability ratio alone does not produce a lower bound; we must


is similar. A typical value is


0.2


also take the minimum of the clamped and original objectives. The lower bound


is shown in figure 12.6, together with the original and clamped objectives. The


end result of the lower bound is that the change in probability ratio is ignored


when it would cause the objective to improve significantly. Using the lower bound


thus prevents large, often detrimental, updates in these situations and removes


the need for the trust region surrogate constraint equation (12.19). Without the


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


258


chapter 12. policy gradient optimization


Algorithm 12.5. The update pro-


struct


TrustRegionUpdate


cedure for trust region policy opti-


# problem


mization, which augments the nat-


# initial state distribution


ural gradient with a line search. It


# depth


generates


trajectories using pol-


# number of samples


icy


in problem


with initial state


# policy π(s)


# policy likelihood p(θ, a, s)


distribution


and depth


. To ob-


∇logπ


# log likelihood gradient


tain the starting point of the line


KL


# KL divergence KL(θ, θ′, s)


search, we need the gradient of the


# divergence bound


log-probability of the policy gener-


# line search reduction factor (e.g., 0.5)


ating a particular action from the


end


current state, which we denote as


∇logπ


. For the surrogate objective,


function


surrogate_objective


::


TrustRegionUpdate


θ′


τs


we need the probability function


, which gives the probability that


sum


-1


for


,(


))


in


zip


end


]))


our policy generates a particular


θ′


action from the current state. For


mean


for


,(


))


in


enumerate


))


the surrogate constraint, we need


return


mean


for


in


τs


the divergence between the action


end


distributions generated by


and


. At each step of the line search,


function


surrogate_constraint


::


TrustRegionUpdate


θ′


τs


we shrink the distance between the


considered point


θ′


and


while


KL


mean


KL


θ′


-1


for


,(


))


in


enumerate


))


maintaining the search direction.


return


mean


KL


for


in


τs


end


function


linesearch


::


TrustRegionUpdate


θ′


fθ


while


θ′


||


θ′


fθ


θ′


θ′


end


return


θ′


end


function


update


::


TrustRegionUpdate


∇logπ


∇logπ


πθ


sum


-1


for


, (


))


in


enumerate


))


∇log


sum


∇logπ


for


in


∇U


∇log


∇log


∇log


τs


simulate


rand


),


πθ


for


in 1


θ′


natural_update


∇U


τs


θ′


surrogate_objective


θ′


τs


θ′


surrogate_constraint


θ′


τs


return


linesearch


θ′


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


12.5. clamped surrogate objective


259


Figure 12.4. Trust region policy op-