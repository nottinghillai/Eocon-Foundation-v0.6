---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ()) +
---

# ()) +

_Source page: 579_



()


()


This is the Dec-MDP utility function derived from equation (26.1), completing the proof.


Exercise 27.3.


How can we use an MMDP or MPOMDP as a heuristic in Dec-POMDP


heuristic search?


Solution:


We can assume free communication for planning. At each time step


, all agents


know


, allowing us to maintain a multiagent belief


, resulting in an MPOMDP.


and


This MPOMDP solution can be used as a heuristic to guide the search of policy trees.


Alternatively, we create a heuristic where we assume that the true state and joint actions


are known. This results in an MMDP, and it can also be used as a heuristic. These assump-


tions are used only for planning. Execution is still a Dec-POMDP wherein agents receive


individual observations without free communication. Either heuristic results in a joint


policy


for heuristic exploration.


Exercise 27.4.


How can we compute a best response controller? Describe how this could


be used in an iterated best response.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


558


chapter 27. collaborative agents


Solution:


For an agent


, the best response controller


, and


can be computed by


solving a nonlinear program. The program is similar to what is given in section 27.6, except


that


, and


are now given and are no longer variables:


maximize


subject to


) =


) +


for all


for all


) =


for all


for all


) =


for all


Adapting algorithm 27.3 for controller policies, this program replaces the inner best


response operation.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com