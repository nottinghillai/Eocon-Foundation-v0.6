---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: '0.12'
---

# 0.12

_Source page: 176_



Exercise 7.5.


Suppose that we have a three-tile, straight-line hex world (appendix F.1)


where the rightmost tile is an absorbing state. When we take any action in the rightmost


state, we get a reward of


10


and we are transported to a fourth terminal state where we no


longer receive any reward. Use a discount factor of


0.9


, and perform a single step of


policy iteration where the initial policy


has us move east in the first tile, northeast in the


second tile, and southwest in the third tile. For the policy evaluation step, write out the


transition matrix


and the reward vector


, and then solve the infinite horizon value


function


directly using matrix inversion. For the policy improvement step, compute


the updated policy


by maximizing the lookahead equation.


Solution:


For the policy evaluation step, we use equation (7.10), repeated here:


= (


Â© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


7.10. exercises


155


Forming the transition matrix


and reward vector


with an additional state for the


26


26


The hex world problem defines


terminal state, we can solve for the infinite horizon value function


, so in order to produce


entries for


, we must compute


0.3


0.7


0.3