---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ), []
---

# ), []

_Source page: 501_



policy


, using the utilities


com-


# prune dominated from previous nodes


puted by policy evaluation and


dominated


x′


all


x′


for


in


the previous node list,


prevX


. Its


for


x′


in


product


prevX


newX


first step replaces any point-wise


if


x′


removeX


&&


dominated


x′


for


in


dominated previous nodes by their


x′


improved nodes, marking the re-


end


dundant node as now dominated.


for


in


The second step marks any newly


x′


added nodes that are identical to


for


x′′


in


product


previous nodes. The third step


x′′


x′


x′′


marks any point-wise dominated


end


new nodes. Finally, all marked


end


nodes are pruned.


push!


removeX


x′


end


end


# prune identical from new nodes


identical_action


x′


all


x′


for


in


identical_successor


x′


all


x′′


x′


x′′


for


in


in


x′′


in


identical


x′


identical_action


x′


&&


identical_successor


x′


for


x′


in


product


prevX


newX


if


x′


removeX


&&


identical


x′


push!


removeX


x′


end


end


# prune dominated from new nodes


for


x′


in


product


newX


if


x′


removeX


&&


dominated


x′


&&


x′


push!


removeX


x′


end


end


# update controller


setdiff


removeX


Dict


=>


for


in


if


removeX


Dict


=>


for


in


if


removeX


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


480


chapter 23. controller abstractions


Example 23.4. Policy iteration, il-


Recall example 23.3. Here, we show the first iteration of policy iteration using


lustrating the evaluation, improve-


the same initial controller. It consists of the two main steps: policy evaluation


ment, and pruning steps on the cry-


ing baby domain with a controller


(left) and policy improvement (center), as well as the optional pruning step


policy representation.


(right).


policy evaluation


policy improvement


pruning


10


10


10


20


20


20


30


30


30


40


40


40


50


50


50


0.2


0.6


0.8


0.2


0.6


0.8


0.2


0.6


0.8


0.4


0.4


0.4


hungry


hungry


hungry


The second iteration of policy iteration follows the same pattern:


policy evaluation


policy improvement


pruning


10


10


10


20


20


20


30


30


30


40


40


40


50


50


50


0.2


0.6


0.8


0.2


0.6


0.8


0.2


0.6


0.8


0.4


0.4


0.4


hungry


hungry


hungry


The utility has greatly improved after the second iteration, to near-optimal


values. We see that the prune step removes dominated and duplicate nodes


from previous iterations, as well as the current iteration’s new nodes.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


23.4. gradient ascent


481


maximize


subject to