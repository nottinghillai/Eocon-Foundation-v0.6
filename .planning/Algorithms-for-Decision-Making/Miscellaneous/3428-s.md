---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ∈S
---

# ∈S

_Source page: 567_



is shared by all


agents. A single reward is generated by


based on state


and joint action


The goal of all agents is to maximize the shared expected reward over time under


local partial observability. Example 27.1 describes a Dec-POMDP version of the


predator-prey problem.


Example 27.1. The collaborative


Consider a predator-prey hex world problem in which a team of predators


predator-prey problem as a Dec-


strives to capture a single fleeing prey. The predators move independently.


POMDP. Additional detail is pro-


vided in appendix F.15.


The prey moves randomly to a neighboring cell not occupied by a predator.


The predators must work together to capture the prey.


546


chapter 27. collaborative agents


Many of the same challenges of POMGs persist in Dec-POMDPs, such as


the general inability of agents to maintain a belief state. We focus on policies


represented as conditional plans or controllers. The same algorithms introduced


in the previous chapter can be used to evaluate policies. All that is required is


to create a POMG with


for each agent


equal to the


from the


Dec-POMDP.


Algorithm 27.1.


Data structure


struct


DecPOMDP


for a Dec-POMDP. The


joint


func-


# discount factor


tion from algorithm 24.2 allows the


# agents


creation of all combinations of a


# state space


set provided, such as


or


. The


# joint action space


# joint observation space


tensorform


function converts the


# transition function


Dec-POMDP


to a tensor represen-


# joint observation function


tation.


# reward function


end