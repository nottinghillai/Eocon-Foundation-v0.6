---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ) =
---

# ) =

_Source page: 358_



and


The first condition ensures that the steps are sufficiently large, and the second


For a discussion of convergence


condition ensures that the steps are sufficiently small.


and its application to some of the


If the learning rate is constant, which is common in reinforcement learning


other algorithms discussed in this


applications, then the weights of older samples decay exponentially at the rate


chapter, see T. Jaakkola, M. I. Jor-


dan, and S. P. Singh, “On the Con-


. With a constant learning rate, we can update our estimate after observing


vergence of Stochastic Iterative Dy-


using the following rule:


namic Programming Algorithms,”


Neural Computation


, vol. 6, no. 6,