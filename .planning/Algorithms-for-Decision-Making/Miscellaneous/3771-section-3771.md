---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ) +
---

# ) +

_Source page: 613_



relu


relu


principles apply.


The gradient with respect to a loss function depends on the gradient of


We can get vanishing gradients in the parameters of the first layer,


and


, if the gradient contributions in successive layers are less than


. For


example, if any of the layers has a negative input to its


relu


function, the


gradient of its inputs will be zero, so the gradient vanishes entirely. In a less


extreme case, suppose that the weights are all


0.5


, the offsets are all


, and the input


is positive. In this case, the gradient with respect to


is


. . .


The deeper the network, the smaller the gradient will be.


We can get exploding gradients in the parameters of the first layer if the


gradient contributions in successive layers are greater than


. If we merely


increase our weights to


, the very same gradient is suddenly doubling


every layer.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


592


appendix d. neural representations


While exploding gradients can often be handled with gradient clipping, regu-


larization, and initializing parameters to small values, these solutions merely shift


the problem toward that of vanishing gradients. Recurrent neural networks often


use layers specifically constructed to mitigate the vanishing gradients problem.


S. Hochreiter and J. Schmidhuber,


They function by selectively choosing whether to retain memory, and these gates


“Long Short-Term Memory,”


Neural


help regulate the memory and the gradient. Two common recurrent layers are


Computation


, vol. 9, no. 8, pp. 1735–


10


long short-term memory