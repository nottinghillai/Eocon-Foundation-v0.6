---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ', . . . ,'
---

# , . . . ,

_Source page: 328_



vol. 11, no. 1, pp. 1–96, 2018.


represent our belief about payoffs, and


thus represent a


belief state


. These


numbers can describe


continuous proba-


bility distributions over possible payoff probabilities.


We can construct an MDP whose states are vectors of length


that represent


the agent’s belief over the


-armed bandit problem. Dynamic programming can


be used to solve this MDP to obtain an optimal policy


that specifies which


arm to pull given the counts.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


15.5. optimal exploration strategies


307


Example 15.3. Exploration strate-


Consider using exploration strategies given the information obtained in the


gies used with the two-armed ban-


two-armed bandit problem of example 15.1, where the posterior distribution


dit problem from example 15.1.


for


Beta