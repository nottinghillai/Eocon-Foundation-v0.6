---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: )] =
---

# )] =

_Source page: 248_



Var


Var


Var


)]


Var


where


is the utility from Monte Carlo policy evaluation and


is the trajectory


reward for a sampled trajectory


. The sample variance, therefore, decreases with


1/


Exercise 10.2.


What effect does varying the number of samples


and the number of elite


samples


elite


have on cross entropy policy search?


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


10.8. exercises


227


Solution:


The computational cost per iteration scales linearly with the number of samples.


More samples will better cover the search space, resulting in a better chance of identifying


better elite samples to improve the policy. The number of elite samples also has an effect.


Making all samples elite provides no feedback to the improvement process. Having too


few elite samples can lead to premature convergence to a suboptimal solution.


Exercise 10.3.


Consider using evolution strategies with a univariate Gaussian distribution,


∼N


. What is the search gradient with respect to the variance


? What issue arises


as the variance becomes small?


Solution:


The search gradient is the gradient of the log-likelihood:


log


) =


log


exp


∂ν


∂ν


πν


log


log


∂ν


+ (


0.4


0,


0.2


log


∂ν


We find that the gradient goes to infinity as the variance approaches zero. This is a


problem because the variance should be small when the search distribution converges.


Very large gradients can cause simple ascent methods to overshoot optima.


Exercise 10.4.


Equation (10.14) defines the objective in terms of a search distribution


∼N


. What advantage does this objective have over directly optimizing


using


the expected utility objective in equation (10.1)?


Solution:


The added Gaussian noise around the policy parameters can smooth discontinu-


ities in the original objective, which can make optimization more reliable.


Exercise 10.5.


Which of the methods in this chapter are best suited to the fact that multiple


types of policies could perform well in a given problem?


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


228


chapter 10. policy search


Solution:


The Hooke-Jeeves method improves a single policy parameterization, so it cannot


retain multiple policies. Both the cross entropy method and evolution strategies use search


distributions. In order to successfully represent multiple types of policies, a multimodal


distribution would have to be used. One common multimodal distribution is a mixture of


Gaussians. A mixture of Gaussians cannot be fit analytically, but they can be reliably fit


using expectation maximization (EM), as demonstrated in example 4.4. Genetic algorithms


can retain multiple policies if the population size is sufficiently large.


Exercise 10.6.


Suppose we have a parameterized policy


that we would like to optimize


using the Hooke-Jeeves method. If we initialize our parameter


and the utility function


is


) =


, what is the largest step size


that would still guarantee policy


improvement in the first iteration of the Hooke-Jeeves method?


Solution:


The Hooke-Jeeves method evaluates the objective function at the center point


along each coordinate direction. In order to guarantee improvement in the first iteration of


Hooke-Jeeves search, at least one of the objective function values at the new points must


improve the objective function value. For our policy optimization problem, this means


that we are searching for the largest step size


such that either


or


is


greater than


Since the underlying utility function is parabolic and concave, the largest step size that


would still lead to improvement is slightly less than the width of the parabola at the current


point. Thus, we compute the point on the parabola opposite the current point,


at which


) =


) =


) +


) =


)(


0,


The point on the parabola opposite the current point is thus


. The distance be-


tween


and


is


. Thus, the maximal step size we can take and still guarantee


improvement in the first iteration is just under


Exercise 10.7.


Suppose we have a policy parameterized by a single parameter


. We


take an evolution strategies approach with a search distribution that follows a Bernoulli


distribution


) =


. Compute the log-likelihood gradient


log


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


10.8. exercises


229


Solution:


The log-likelihood gradient can be computed as follows:


) =


log


) =


log


log


) =


log


+ (


log


) =


log


+ (


log


)]


log


) =


log


Exercise 10.8.


Compute the sample weights for search gradient estimation with rank


shaping given


samples.


Solution:


We first compute the numerator of the first term from equation (10.13), for all


max


0, log


log 1


log


max


0, log


log 2


log


max


0, log


log 3


Now, we compute the weights:


log