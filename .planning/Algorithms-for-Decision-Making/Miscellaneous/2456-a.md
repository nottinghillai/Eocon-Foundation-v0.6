---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: '|A| −'
---

# |A| −

_Source page: 394_



|S|


independent parameters that


must be learned, which is often prohibitively large. Expert demonstrations typically cover


only a small portion of the state space. Even if


can be reliably trained for the states


covered in the provided data set, the resulting policy would be untrained in other states.


Using a feature function allows generalization to unseen states.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


18.8. exercises


373


Exercise 18.2.


Section 18.1 suggested using a maximum likelihood approach for training a


policy from expert data. This approach attempts to find the parameters of the policy that


maximizes the likelihood assigned to the training examples. In some problems, however,


we know that assigning high probability to one incorrect action is not as bad as assigning


high probability to another incorrect action. For example, predicting an acceleration of


in the mountain car problem when the expert dictates an acceleration of


is worse


than predicting an acceleration of


. How might behavioral cloning be modified to allow


different penalties to be given to different misclassifications?


Solution:


We can instead supply a cost function


that defines the cost of


true


pred


predicting action


when the expert’s action is


. For example, with the


true


pred


for state


mountain car problem, we might use


) =


−|


true


true


pred


pred


which penalizes greater deviations more than smaller deviations. The cost associated with


the expert’s action is typically zero.


If we have a stochastic policy


, we then seek to minimize the cost over our data


set:


minimize


true


pred


pred


pred


∈D


true


14


14


C. Elkan, “The Foundations of


This technique is called


cost-sensitive classification


One benefit of cost-sensitive classi-


Cost-Sensitive Learning,” in


Inter-


fication is that we can use a wide variety of off-the-shelf classification models, such as


national Joint Conference on Artificial


-nearest neighbors, support vector machines, or decision trees, to train a policy.


Intelligence (IJCAI)


, 2001.


Exercise 18.3.


Provide an example of where maximum margin inverse reinforcement


learning does not uniquely define an optimal policy.


Solution:


Maximum margin inverse reinforcement learning extracts binary features from


the expert data and seeks a reward function whose optimal policy produces trajectories


with the same frequencies of these binary features. There is no guarantee that multiple


policies do not produce the same feature expectations. For example, an autonomous car


that makes only left lane changes could have the same lane change frequencies as an


autonomous car that makes only right lane changes.


Exercise 18.4.


Maximum margin inverse reinforcement learning measures how similar


a policy is to expert demonstrations using feature expectations. How is this similarity


measure affected if nonbinary features are used?


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


374


chapter 18. imitation learning


Solution:


If we use nonbinary features, then it is possible that some features can get larger


than others, incentivizing the agent to match those features rather than those that tend


to be smaller. Scale is not the only issue. Even if all features are constrained to lie within