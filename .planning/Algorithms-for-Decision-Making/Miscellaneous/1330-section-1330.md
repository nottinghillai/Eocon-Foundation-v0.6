---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ']...)'
---

# ]...)

_Source page: 197_



Uθ


pinv


return


Uθ


end


As discussed in appendix D, we can optimize the network weights to achieve


a particular objective. In the context of approximate dynamic programming, we


would want to minimize the error of our predictions, just as we did in the previous


section. However, minimizing the squared error cannot be done through simple


matrix operations. Instead, we generally have to rely on optimization techniques


such as gradient descent. Fortunately, computing the gradient of neural networks


can be done exactly through straightforward application of the derivative chain


rule.