---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: 10,000
---

# 10,000

_Source page: 618_



images for the MNIST data


encoding space:


set. Each encoding is again colored according to the corresponding digit:


The variational autoencoder also produces clusters in the embedding


space for each digit, but this time they are roughly distributed according to


a zero-mean, unit variance Gaussian distribution. We again see how some


encodings are similar, such as the significant overlap for


and


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


d.7. adversarial networks


597


One common approach to penalize off-nominal outputs or behavior is to use


14


adversarial learning


by including a discriminator, as shown in figure D.13.


primary network


discriminator


is a neural network that acts as a binary classifier that takes in


neural network outputs and learns to distinguish between real outputs from


true


the training set and the outputs from the primary neural network. The primary


neural network, also called a


generator


, is then trained to deceive the discriminator,


discriminator


thereby naturally producing outputs that are more difficult to distinguish from


the data set. The primary advantage of this technique is that we do not need


true


to design special features to identify or quantify how the output fails to match


Figure D.13. A generative adversar-


the training data, but we can allow the discriminator to naturally learn such


ial network causes a primary net-


work’s output to be more realistic


differences.


by using a discriminator to force


Learning is adversarial in the sense that we have two neural networks: the


the primary network to produce


more realistic output.


primary neural network that we would like to produce realistic outputs and the


14


These techniques were intro-


discriminator network that distinguishes between primary network outputs and


duced by I. Goodfellow, J. Pouget-


real examples. They are each training to outperform the other. Training is an


Abadie, M. Mirza, B. Xu, D. Warde-


iterative process in which each network is improved in turn. It can sometimes be


Farley, S. Ozair, A. Courville, and


Y. Bengio, “Generative Adversarial


challenging to balance their relative performance; if one network becomes too


Nets,” in


Advances in Neural Infor-


good, the other can become stuck.


mation Processing Systems (NIPS)


2014.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com