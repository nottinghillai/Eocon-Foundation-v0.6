---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ∼N
---

# ∼N

_Source page: 615_



as an additional input to the neural network


“Auto-Encoding


Variational


and obtain our sample according to


Bayes,” in


International Conference


The components are kept close to unit Gaussian by also minimizing the KL


on Learning Representations (ICLR)


13


2013.


divergence (appendix A.10).


This objective encourages smooth latent space


13


The KL divergence for two unit


representations. The network is penalized for spreading out the latent representa-


Gaussians is


tions (large values for


) and for focusing each representation into a very small


+ (


log


encoding space (small values for


), ensuring better coverage of the latent


space. As a result, smooth variations into the decoder can result in smoothly


varying outputs. This property allows decoders to be used as


generative models


where samples from a unit multivariate Gaussian can be input to the decoder to


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


594


appendix d. neural representations


produce realistic samples in the original space. The combined loss function is


minimize