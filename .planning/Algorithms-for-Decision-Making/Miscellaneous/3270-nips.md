---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (NIPS)
---

# (NIPS)

_Source page: 531_



, 2005.


â€¢ In simple games, multiple agents compete to maximize expected reward.


Â© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


510


chapter 24. multiagent reasoning


Algorithm 24.12. An implementa-


mutable struct


GradientAscent


tion of gradient ascent for an agent


# simple game


of a simple game


. The algorithm


# agent index


updates its distribution over ac-


# time step


tions incrementally following gra-


Ï€i


# current policy


dient ascent to improve the ex-


end


pected utility. The projection func-


function


GradientAscent


::


SimpleGame


tion from algorithm 23.6 is used


uniform


()


SimpleGamePolicy


ai


=>


1.0 for


ai


in


])


to ensure that the resulting policy


return


GradientAscent


uniform


())


remains a valid probability distri-


end


bution.


Ï€i


::


GradientAscent


)()


Ï€i


Ï€i


()


Ï€i


::


GradientAscent


)(


ai


Ï€i


Ï€i


ai


function


update!


Ï€i


::


GradientAscent


ğ’œi


Ï€i


Ï€i


Ï€i


Ï€i


],


Ï€i


Ï€i


jointÏ€


ai


SimpleGamePolicy


==


ai


])


for


in


utility


jointÏ€


ai


),


for


ai


in


ğ’œi


Ï€â€²


Ï€i


Ï€i


ai


for


ai


in


ğ’œi


project_to_simplex


Ï€â€²


sqrt


))


Ï€i


Ï€i


Ï€i


SimpleGamePolicy


ai


=>


for


ai


in


zip


ğ’œi


))


end


Figure 24.4. Two gradient ascent


agents with randomly initialized


policies in a rock-paper-scissors


game. We use a variation of algo-


0.2


0.2


rithm 24.12 with a learning rate of


0.8


0.8