---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: '0.05'
---

# 0.05

_Source page: 378_



0.5


0.5


0.5


0.5


position


position


The state space is not fully covered by expert demonstrations, which is


typical of imitation learning problems. The resulting policy may perform well


when used in regions with coverage, but it assigns a uniform distribution


to actions in regions without coverage. Even if we start in a region with


coverage, we may transition to regions without coverage due to stochasticity


in the environment.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


18.1. behavioral cloning


357


the parameters (chapter 4) from the data


. Given the current state, we can then


infer the distribution over actions using one of the inference algorithms discussed


earlier (chapter 3).


Figure 18.1. Bayesian networks can


be used to represent a joint distribu-


tion over the state and action vari-


ables. We can apply an inference al-


gorithm to generate a distribution


over actions, given the current val-


ues of the state variables.


We can use many other representations for


. For example, we might want


to use a neural network, where the input corresponds to the values of the state


variables and the output corresponds to parameters of a distribution over the


action space. If our representation is differentiable, which is the case with neural


networks, we can attempt to optimize equation (18.1) using gradient ascent. This


approach is implemented in algorithm 18.1.


Algorithm 18.1.


A method for


struct


BehavioralCloning


learning a parameterized stochas-


# step size


tic policy from expert demonstra-


k_max


# number of iterations


tions in the form of a set of state-


∇logπ


# log likelihood gradient


action tuples


. The policy param-


end


eterization vector


is iteratively


function


optimize


::


BehavioralCloning


improved by maximizing the log


k_max


∇logπ


k_max


∇logπ


likelihood of the actions given the


for


in 1


k_max


states. Behavioral cloning requires


mean


∇logπ


for


in


a step size


, an iteration count


+=


k_max


, and a log likelihood gradi-


end


ent


∇logπ


return


end


The closer the expert demonstrations are to optimal, the better the resulting


U. Syed and R. E. Schapire, “A


behavioral cloning policy will perform.


However, behavioral cloning suffers


Reduction from Apprenticeship


from


cascading errors


. As discussed in example 18.2, small inaccuracies compound


Learning to Classification,” in


Ad-


during a rollout and eventually lead to states that are poorly represented in the


vances in Neural Information Process-


ing Systems (NIPS)


, 2010.


training data, thereby leading to worse decisions, and ultimately to invalid or


unseen situations. Although behavioral cloning is attractive due to its simplic-


ity, cascading errors cause the method to perform poorly on many problems,


especially when policies must be used for long time horizons.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


358


chapter 18. imitation learning


Example 18.2. A brief example of


Consider applying behavioral cloning to train a policy for driving an au-


the generalization issue inherent to


tonomous race car. A human race car driver provides expert demonstrations.


behavioral cloning approaches.


Being an expert, the driver never drifts onto the grass or too close to a railing.


A model trained with behavioral cloning would have no information to use


when near a railing or when drifting onto the grass, and thus it would not


know how to recover.