---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: × −
---

# × −

_Source page: 139_



0.1


0.8


Hence, we will want to bring our umbrella.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


118


chapter 6. simple decisions


There are three kinds of directed edges:


conditional edge


ends in a chance node and indicates that the uncertainty in


that chance node is conditioned on the values of all its parents.


An


informational edge


ends in an action node and indicates that the decision


associated with that node is made with knowledge of the values of its parents.


(These edges are often drawn with dashed lines and are sometimes omitted


from diagrams for simplicity.)


functional edge


ends in a utility node and indicates that the utility node is


determined by the outcomes of its parents.


Like Bayesian networks, decision networks cannot have cycles. The utility asso-


ciated with an action is equal to the sum of the values at all the utility nodes.


Example 6.4 illustrates how a decision network can model the problem of whether


to treat a disease, given the results of diagnostic tests.


Example 6.4.


An example of a


We have a set of results from diagnostic tests that may indicate the presence of


decision network used to model


a particular disease. Given what is known about the tests, we need to decide


whether to treat a disease, given


information from diagnostic tests.


whether to apply a treatment. The utility is a function of whether a treatment


is applied and whether the disease is actually present. Conditional edges


connect


to


, and


. Informational edges are not explicitly shown


in the illustration, but they would connect the observations to


. Functional


edges connect


and


to


Treat?


Disease?


10


Results from diagnostic tests


Solving a simple problem (algorithm 6.1) requires iterating over all possible


decision instantiations to find a decision that maximizes expected utility. For each


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


6.6. value of information


119


instantiation, we evaluate the associated expected utility. We begin by instantiating


the action nodes and observed chance nodes. We can then apply any inference


algorithm to compute the posterior over the inputs to the utility function. The


expected utility is the sum of the values at the utility nodes. Example 6.5 shows


how this process can be applied to our running example.


Algorithm 6.1. A simple problem


struct


SimpleProblem


as a decision network. A decision


bn


::


BayesianNetwork


network is a Bayesian network with


chance_vars


::


Vector


Variable


chance, decision, and utility vari-


decision_vars


::


Vector


Variable


ables. Utility variables are treated


utility_vars


::


Vector


Variable


as deterministic. Because variables


utilities


::


Dict


Symbol


Vector


Float64


}}


end


in our Bayesian network take val-


ues from


1 :


, the utility variables


function


solve


::


SimpleProblem


evidence


are mapped to real values by the


query


var


name


for


var


in


utility_vars


utilities


field. For example, if we


sum


utilities


uname


][


uname


]]


for


uname


in


query


have a utility variable


u1


, the


th


best


nothing


=-


Inf


utility associated with that variable


for


assignment


in


assignments


decision_vars


is


utilities


u1


][


. The


solve


evidence


merge


evidence


assignment


function takes as input the prob-


infer


bn


query


evidence


lem, evidence, and an inference


sum


for


in


table


method. It returns the best assign-


if


best


ment to the decision variables and


best


assignment


its associated expected utility.


end


end


return


best


end


11


R. D. Shachter, “Evaluating In-


fluence Diagrams,”


Operations Re-


search


, vol. 34, no. 6, pp. 871–882,


A variety of methods have been developed to make evaluating decision net-


1986. R. D. Shachter, “Probabilis-


11


tic Inference and Influence Dia-


works more efficient.


One method involves removing action and chance nodes


grams,”


Operations Research


, vol. 36,


from decision networks if they have no children, as defined by conditional, infor-


no. 4, pp. 589–604, 1988.


mational, or functional edges. In example 6.5, we can remove


and


because


12


R. A.


Howard,


“Information


they have no children. We cannot remove


because we treated it as observed,


Value Theory,”


IEEE Transactions


on Systems Science and Cybernetics


indicating that there is an informational edge from


(although it is not


to


vol. 2, no. 1, pp. 22–26, 1966.


drawn explicitly).


Applications to decision networks


can be found in: S. L. Dittmer and


F. V. Jensen, “Myopic Value of In-