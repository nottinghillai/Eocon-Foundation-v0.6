---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: '0.00'
---

# 0.00

_Source page: 528_



50


50


100


100


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


24.8. fictitious play


507


Algorithm 24.10. A simulation of


function


simulate


::


SimpleGame


k_max


a joint policy in simple game


for


for


k_max


k_max


iterations. The joint policy


πi


()


for


πi


in


is a vector of policies that can be


for


πi


in


individually updated through calls


update!


πi


to


update!


πi


end


end


return


end


At each iteration, we have each agent act according to a best response, assuming


these stochastic count-based policies for the other agents. We then update the


15


A concise background is pro-


action counts for the actions taken. Algorithm 24.11 implements this simple


vided by U. Berger, “Brown’s Orig-


adaptive procedure. Figures 24.2 and 24.3 show how the policies evolve over


inal Fictitious Play,”


Journal of


time using fictitious play. Fictitious play is not guaranteed to converge to a Nash


Economic Theory


, vol. 135, no. 1,


15


pp. 572–578, 2007.


equilibrium.


Algorithm 24.11. Fictitious play is


mutable struct


FictitiousPlay


a simple learning algorithm for an


# simple game


agent


of a simple game


that


# agent index


maintains


counts


of other agent ac-


# array of action count dictionaries


tion selections over time and aver-


πi


# current policy


ages them, assuming that this is


end


their stochastic policy. It then com-


function


FictitiousPlay


::


SimpleGame


putes a best response to this pol-


Dict


aj


=>


1 for


aj


in


])


for


in


icy and performs the correspond-


πi


SimpleGamePolicy


ai


=>


1.0 for


ai


in


])


ing utility-maximizing action.


return


FictitiousPlay


πi


end


πi


::


FictitiousPlay


)()


πi


πi


()


πi


::


FictitiousPlay


)(


ai


πi


πi


ai


function


update!


πi


::


FictitiousPlay


πi


πi


πi


πi


for


aj


in


enumerate


][


aj


+=


end


SimpleGamePolicy


aj


=>


sum


values


]))


for


aj


in


])


for


in


πi


πi


best_response


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


508


chapter 24. multiagent reasoning


Figure 24.2.


Two fictitious play


opponent model


policy


agents learning and adapting to


one another in a prisoner’s dilem-


cooperate


ma game. The first row illustrates


defect


agent


’s learned model of


(left)


and agent


’s policy (right) over


0.5


action


iteration. The second row follows


agent


the same pattern, but for agent


. To illustrate variation in learn-


ing behavior, the initial counts for


each agent’s model over the other


agent’s action were assigned to a


random number between


and


10


0.5


action


agent


20


60


20


60


40


40


iteration


iteration


Figure 24.3.


A visualization of


opponent model


policy


two fictitious play agents learning


and adapting to one another in a


rock


rock-paper-scissors game. The first


paper


row illustrates agent


’s learned


scissors


model of


(left) and agent


’s


0.5


action


policy (right) over time. The sec-


agent


ond row follows the same pat-


tern, but for agent


. To illus-


trate variation in learning behavior,


the initial counts for each agent’s


model over the other agent’s ac-


tion were assigned to a random


number between


and


10


. In


this zero-sum game, fictitious play


0.5


agents approach convergence to


action


agent


their stochastic policy Nash equi-


librium.


20


60


20


60


40


40


iteration


iteration


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


24.9. gradient ascent


509


There are many variants of fictitious play. One variant, called


smooth fictitious


16


16


D. Fudenberg and D. Levine,


play


selects a best response using expected utility plus a smoothing function,


“Consistency and Cautious Ficti-


such as the entropy of the policy. Another variant is called


rational learning


or


tious Play,”


Journal of Economic Dy-


Bayesian learning


. Rational learning expands the model of fictitious play to be any


namics and Control


, vol. 19, no. 5–7,


pp. 1065–1089, 1995.


belief over other agents’ actions, formulated as a Bayesian prior. Bayes’ rule is then


used to update the beliefs, given the history of joint actions. Traditional fictitious


play can be seen as rational learning with a Dirichlet prior (section 4.2.2).