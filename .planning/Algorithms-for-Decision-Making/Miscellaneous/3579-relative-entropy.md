---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: Relative Entropy
---

# Relative Entropy

_Source page: 589_



Relative entropy


, also called the


Kullback-Leibler (KL) divergence


, is a measure of


Named for the two American


how one probability distribution is different from a reference distribution.


If


mathematicians who introduced


and


are mass functions, then the KL divergence from


to


is the


this measure, Solomon Kullback


expectation of the logarithmic differences, with the expectation using