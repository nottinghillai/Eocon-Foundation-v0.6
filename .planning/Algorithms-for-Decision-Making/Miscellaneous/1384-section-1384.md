---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ) =
---

# ) =

_Source page: 205_



. If our problem


requires planning beyond the depth that we can afford to compute online, we can


use an estimate of the value function obtained offline using, for example, one of


the value function approximations described in the previous chapter. Combining


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


184


chapter 9. online planning


Algorithm 9.1.


A function that


struct


RolloutLookahead


runs a rollout of policy


in prob-


# problem


lem


from state


to depth


. It re-


# rollout policy


turns the total discounted reward.


# depth


This function can be used with


end


the


greedy


function (introduced in


randstep


::


MDP


TR


algorithm 7.5) to generate an ac-


tion that is likely to be an improve-


function


rollout


ment over the original rollout pol-


ret


0.0


icy. We will use this algorithm later


for


in 1


for problems other than MDPs, re-


quiring us to only have to modify


randstep


randstep


appropriately.


ret


+=


-1


end


return


ret


end


function


::


RolloutLookahead


)(


rollout


return


greedy


).


end


depth


depth


Figure 9.1. A forward search tree


for a problem with three states and


two actions.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


9.4. branch and bound


185


online and offline approaches in this way is sometimes referred to as


hybrid


planning


Algorithm 9.2. The forward search


struct


ForwardSearch


algorithm for finding an approxi-


# problem


mately optimal action online for a


# depth


problem


from a current state


# value function at depth d


The search is performed to depth


end


at which point the terminal value


function


forward_search


is estimated with an approximate


if


value function


. The returned


return


nothing


))


named tuple consists of the best


end


action


and its finite-horizon ex-


best


nothing


=-


Inf


pected value


. The problem type is


U′


forward_search


-1


).


not constrained to be an MDP; sec-


for


in


tion 22.2 uses this same algorithm


lookahead


U′


in the context of partially observ-


if


best


able problems with a different im-


best


plementation for


lookahead


end


end


return


best


end


::


ForwardSearch


)(


forward_search


).