---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (D.8)
---

# (D.8)

_Source page: 615_



Training to minimize the reconstruction loss forces the autoencoder to find the


most efficient low-dimensional encoding that is sufficient to accurately reconstruct


the original input. Furthermore, training is


unsupervised


, in that we do not need


encoder


to guide the training to a particular feature set.


After training, the upper portion of the autoencoder above the bottleneck can


encoding distribution


be used as an


encoder


that transforms an input into the feature representation.


The lower portion of the autoencoder can be used as a


decoder


that transforms the


feature representation into the input representation. Decoding is useful when


training neural networks to generate images or other high-dimensional outputs.


Example D.5 shows an embedding learned for handwritten digits.


decoder


variational autoencoder


, shown in figure D.12, extends the autoencoder frame-


12


work to learn a probabilistic encoder.


Rather than outputting a deterministic


sample, the encoder produces a distribution over the encoding, which allows the


Figure D.12. A variational autoen-


model to assign confidence to its encoding. Multivariate Gaussian distributions


coder passes a high-dimensional


input through a low-dimensional


with diagonal covariance matrices are often used for their mathematical conve-


bottleneck that produces a prob-


nience. In such a case, the encoder outputs both an encoding mean and diagonal


ability distribution over the en-


coding. The decoder reconstructs


covariance matrix.


samples from this encoding to re-


Variational autoencoders are trained to both minimize the expected reconstruc-


construct the original input. Varia-


tion loss while keeping the encoding components close to unit Gaussian. The


tional autoencoders can therefore


assign confidence to each encoded


former is achieved by taking a single sample from the encoding distribution with


feature. The decoder can thereafter


each passthrough,