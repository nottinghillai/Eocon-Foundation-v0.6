---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ) = [
---

# ) = [

_Source page: 382_



, 1


xv


tion.


where


and


are the position and speed of the car.


rollouts


10


0.8


0.6


0.4


speed


0.2


10


0.8


0.6


0.4


speed


0.2


10


0.8


0.6


0.4


speed


0.2


0.5


0.5


0.5


0.5


0.5


0.5


position


position


position


accel right


coast


accel left


Trajectories are colored according to the action. In the first iteration, the


agent behaves randomly, unable to make progress toward the goal (


0.6


). With additional iterations, the agent learns to mimic the expert policy


of accelerating in the direction of travel. This behavior is apparent in the


new trajectories, which spiral outward, and the policy, which assigns high


likelihood to


when


and


when


Â© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


18.4. maximum margin inverse reinforcement learning


361


latest policy


to generate a new data set, querying the expert to provide the


correct actions. Behavioral cloning is applied only to this new data set to train


a new


component policy


. This component policy is mixed with component


policies from the previous iterations to produce a new policy


The mixing of component policies to generate


is governed by a mixing


scalar