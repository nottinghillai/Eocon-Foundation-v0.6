---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: 0, 1
---

# 0, 1

_Source page: 383_



. The probability of acting according to the expert policy is


and the probability of acting according to


is


. This scheme assigns


more weight to older policies under the hypothesis that older policy components


In SMILe, we are acting accord-


were trained on the states most likely to be encountered.


With each iteration, the


ing to our latest learned policy. We


probability of acting according to the original expert policy decays to zero. The


expect that this learned policy will


mixing scalar is typically small, such that the agent does not abandon the expertâ€™s


match the expert fairly well and pri-


marily mispredict when we deviate


policy too quickly. Example 18.4 demonstrates this approach with the mountain


from the expert policy. The learned


car problem.


component policies generally only


need to make smaller and smaller


contributions with each iteration to