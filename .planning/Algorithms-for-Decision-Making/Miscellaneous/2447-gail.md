---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: GAIL
---

# GAIL

_Source page: 391_



),


we optimize a differentiable


Adversarial Imitation Learning,” in


parameterized policy


, often represented by a neural network. Rather than


Advances in Neural Information Pro-


provide a reward function, we use


adversarial learning


(appendix D.7). We also


cessing Systems (NIPS)


, 2016.


train a


discriminator


, typically also a neural network, to return the proba-


bility that it assigns to the state-action pair coming from the learned policy. The


process involves alternating between training this discriminator to become better


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


370


chapter 18. imitation learning


Algorithm 18.6.


Maximum en-


struct


MaximumEntropyIRL


tropy inverse reinforcement learn-


# problem


ing, which finds a stochastic pol-


# initial state distribution


icy that maximizes the likelihood


# depth


of the expert demonstrations un-


# parameterized policy π(θ,s)


der a maximum-entropy trajectory


Pπ


# parameterized policy likelihood π(θ, a, s)


∇R


# reward function gradient


distribution. This implementation


RL


# reinforcement learning method


computes the expected visitations


# step size


using dynamic programming over


k_max


# number of iterations


all states, which requires that the


end


problem be discrete.


function


discounted_state_visitations


::


MaximumEntropyIRL


Pπ


Pπ


b_sk


zeros


length


),


b_sk


pdf


for


in


for


in 2


for


si′


s′


in


enumerate


b_sk


si′


sum


sum


b_sk


si


-1


Pπ


s′


for


si


in


enumerate


))


for


in


end


end


return


normalize!


vec


mean


b_sk


dims


)),


end


function


optimize


::


MaximumEntropyIRL


Pπ


∇R


RL


k_max


Pπ


∇R


RL


k_max


nD


length


for


in 1


k_max


copyto!


RL


# update parameters


optimize


RL


discounted_state_visitations


∇Rτ


->


sum


-1


∇R


for


,(


))


in


enumerate


))


∇f


sum


∇Rτ


for


in


nD


sum


si


sum


Pπ


∇R


for


ai


in


enumerate


))


for


si


in


enumerate


))


+=


∇f


end


return


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


18.7. summary


371


at distinguishing between simulated and expert state-action pairs, and training


the policy to look indistinguishable from the expert demonstrations. The process


is sketched in figure 18.3.


Figure 18.3. Instead of inferring a


state


reward function, generative adver-


sarial imitation learning optimizes


a discriminator to distinguish be-


policy


tween simulated and expert state-


action pairs, and it optimizes a pol-


icy to appear indistinguishable to


the discriminator. The aim is to


eventually produce a policy that re-


sembles the expert.


expert state-action pairs


simulated state-action pairs


discriminator


simulated


The discriminator and policy have opposing objectives. GAIL seeks to find a


saddle point


of the negative log loss of the discriminator’s binary classifi-


13


13


The original paper also includes


cation problem:


the following entropy term:


max


min


log