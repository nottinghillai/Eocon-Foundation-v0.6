---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ), []
---

# ), []

_Source page: 223_



old


. The function


expand


com-


while


isempty


toexpand


putes the greedy envelope of


and


pop!


toexpand


determines whether any of those


push!


envelope


greedy


states have utility residuals above


if


abs


the threshold. If a state has a resid-


found


true


ual that exceeds the threshold, then


else


we update the utilities of the states


for


s′


in


in the envelope. Otherwise, we add


if


s′


&&


s′


solved


envelope


that envelope to the set of solved


push!


toexpand


s′


states.


end


end


end


end


return


found


envelope


end


function


label!


::


LabeledHeuristicSearch


solved


if


solved


return false


end


found


envelope


expand


solved


if


found


for


reverse


envelope


greedy


).


end


else


union!


solved


envelope


end


return


found


end


Figure 9.6. The greedy envelope


for


for several states visu-


alized for a value function on the


hex world problem. The value func-


tion was obtained by running ba-


sic heuristic search for


10


iterations


from an initial state, shown with


a white hex center, to a maximum


depth of


. We find that the size


of the greedy envelope, outlined in


gray, can vary widely depending


on the state.


10


10


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


202


chapter 9. online planning


Figure 9.7. A single iteration of


labeled heuristic search conducts


an exploratory run (arrows), fol-


lowed by labeling (hexagonal bor-


der). Only two states are labeled in


this iteration: the hidden terminal


state and the state with a hexag-


onal border. Both the exploratory


run and the labeling step update


the value function.


Figure 9.8. A progression of heuris-


simulation


simulations


tic search on the hex world prob-


lem using


and a heuris-


tic


) =


10


. The solved states


in each iteration are covered in a


gray wash. The set of solved states


grows from the terminal reward


simulations


simulations


state back toward the initial state


with the dark border.


simulations (solved)


10


10


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


9.9. open-loop planning


203


Open-loop planning can often allow us to devise effective decision strategies


in high-dimensional spaces where closed-loop planning is computationally in-


feasible. This type of planning gains this efficiency by not accounting for future


information. Example 9.9 provides a simple instance of where open-loop planning


can result in poor decisions, even when we account for stochasticity.


Example 9.9.


Suboptimality of


Consider a problem with nine states, as shown in the margin, with two


open-loop planning.


decision steps starting from the initial state


. In our decisions, we must


30


decide between going up (blue arrows) and going down (green arrows).


The effects of these actions are deterministic, except that if we go up from


, then we end up in state


half the time and in state


half the time. We


receive a reward of


30


in states


20


in states


and


and a reward of


and


, as indicated in the illustration.