---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: '0.01'
---

# 0.01

_Source page: 305_



when


prev


to discourage advisory changes.


We can use dynamic programming with linear interpolation (section 8.4)


to derive an optimal policy. Alternatively, we can define a simple heuristic


policy parameterized by thresholds on


that works as follows. If


col


and


, then an advisory is generated. This advisory is


thresh


and


col


thresh


to climb if


and to descend otherwise. By default, we use


50 m


thresh


and


30 s


. The following are plots of both the optimal and simple


thresh


policies for two slices through the state space:


0.0 m/s


5.0 m/s


200


no advisory


descend


100


climb


Optimal


100


200


200


100


Simple


100


200


20


30


20


30


10


40


10


40


col


col


Â© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


284


chapter 14. policy validation


Example 14.2.


Probability of a


Here is the result of applying policy evaluation to both an optimal policy


collision when following the opti-


and the simple policy introduced in example 14.1. Each point in the plot


mal and simple collision avoidance


policies.


corresponds to the value of the metric, conditioned on starting from the


associated state. We define