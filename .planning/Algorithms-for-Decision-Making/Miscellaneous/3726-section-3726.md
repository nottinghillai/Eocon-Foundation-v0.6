---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: ) +
---

# ) +

_Source page: 605_



These nonlinearities are necessary to allow neural networks to adapt to fit arbitrary


Figure D.2. A more compact de-


target functions. To illustrate, figure D.3 shows the output of a neural network


piction of figure D.1. Neural net-


work layers are often represented


trained to approximate a nonlinear function.


as blocks or slices for simplicity.


Figure D.3.


A deep neural net-


true function


work fit to samples from a nonlin-


training samples


ear function so as to minimize the


learned model


squared error. This neural network


has four affine layers, with


10


neu-


rons in each intermediate represen-


tation.


10


There are many types of activation functions that are commonly used. Similar


to their biological inspiration, they tend to be close to zero when their input is


low and large when their input is high. Some common activation functions are


shown in figure D.5.


Sometimes special layers are incorporated to achieve certain effects. For ex-


ample, in figure D.4, we used a


softmax


layer at the end to force the output to


represent a two-element categorical distribution. The softmax function applies


Â© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


584


appendix d. neural representations


Figure D.4. A simple, two-layer,


fully connected network trained


to classify whether a given coor-


0.8


0.5


dinate lies within a circle (shown


in


white).


The


nonlinearities


0.6


fully connected + sigmoid


allow neural networks to form


complicated, nonlinear decision


0.4


boundaries.


fully connected + softmax


0.5


0.2


pred


0.5


0.5


Figure D.5. Several common acti-


sigmoid


tanh


softplus


vation functions.


1/


exp