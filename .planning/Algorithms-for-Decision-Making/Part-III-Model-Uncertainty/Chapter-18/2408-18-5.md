---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (18.5)
---

# (18.5)

_Source page: 386_



where


corresponds to trajectories generated by


to depth


. Here, we introduce


the


feature expectations


vector


, which is the expected discounted accumulated


feature values. These feature expectations can be estimated from


rollouts, as


implemented in algorithm 18.4.


Algorithm 18.4. A structure for in-


struct


InverseReinforcementLearning


verse reinforcement learning and


# problem


a method for estimating a feature


# initial state distribution


expectations vector from rollouts.


# depth


# number of samples


# parameterized policy


# binary feature mapping


μE


# expert feature expectations


RL


# reinforcement learning method


# tolerance


end


function


feature_expectations


::


InverseReinforcementLearning


sum


-1


for


,(


))


in


enumerate


))


τs


simulate


rand


),


for


in 1


return


mean


for


in


τs


end


We can use the expert demonstrations to estimate the expert feature expecta-


tions


, and we want to find a policy that matches these feature expectations


as closely as possible. At the first iteration, we begin with a randomized policy


and estimate its feature expectations, denoted as


. At iteration


, we find


a new


corresponding to a reward function