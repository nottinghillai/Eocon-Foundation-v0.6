---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (17.11)
---

# (17.11)

_Source page: 360_



166, 1994.


With a suitable exploration strategy, the


will converge to


arg max


which is what is used in the


-learning update.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


17.3. sarsa


339


Figure 17.1.


-learning used to it-


50


rollouts


100


rollouts


eratively learn an action value func-


tion for the hex world problem.


Each state is colored according to


the expected value of the best ac-


tion in that state according to


Actions are similarly the best ex-


150


rollouts


200


rollouts


pected actions.


-learning was run


with


0.1


and


10


steps per roll-


out.


10


10


Example 17.2. How to use an ex-


Suppose we want to apply


-learning to an MDP problem


. We can con-


ploration strategy with


-learning


struct an exploration policy, such as the


-greedy policy implemented in


in simulation. The parameter set-


tings are notional.


algorithm 16.6 from the previous chapter. The


-learning model comes from


algorithm 17.2, and the simulate function is implemented in algorithm 15.9.


zeros


length


),


length


))


0.2


# learning rate


model


QLearning


0.1


# probability of random action


EpsilonGreedyExploration


20


# number of steps to simulate


# initial state


simulate


model


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


340


chapter 17. model-free methods


Sarsa is referred to as a type of


on-policy


reinforcement learning method because


it attempts to directly estimate the value of the exploration policy as it follows it. In


contrast,


-learning is an


off-policy


method because it attempts to find the value of


the optimal policy while following the exploration strategy. Although


-learning


and Sarsa both converge to an optimal strategy, the speed of convergence depends


on the application. Sarsa is run on the hex world problem in figure 17.2.


Algorithm 17.3. The Sarsa update


mutable struct


Sarsa


for model-free reinforcement learn-


# state space (assumes 1:nstates)


ing. We update the matrix


con-


# action space (assumes 1:nactions)


taining the state-action values,


# discount


is a constant learning rate, and


# action value function


is the most recent experience tu-


# learning rate


# most recent experience tuple (s,a,r)


ple. As with the


-learning imple-


end


mentation, the update function can


be used in the simulator in algo-


lookahead


model


::


Sarsa


model


rithm 15.9.


function


update!


model


::


Sarsa


s′


if


model


!=


nothing


model


model


model


model


model


+=


])


end


model


return


model


end


Figure 17.2.


Sarsa used to itera-


50


rollouts


100


rollouts


tively learn an action value func-


tion for the hex world problem in


a manner otherwise identical to


figure 17.1. We find that Sarsa is


slower to converge to the true ac-


tion value function.


150


rollouts


200


rollouts


10


10


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


17.4. eligibility traces


341