---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (17.21)
---

# (17.21)

_Source page: 366_



© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


17.7. experience replay


345


This update is implemented in algorithm 17.5 with the addition of a scaled gra-


dient step (algorithm 12.2), which is often needed to ensure that the gradient


steps do not become too large. Example 17.3 shows how to use this update with a


linear action value approximation. Figure 17.3 demonstrates this algorithm with


the mountain car problem.


Algorithm 17.5.


The


-learning


struct


GradientQLearning


update with action value function


# action space (assumes 1:nactions)


approximation. With each new ex-


# discount


perience tuple


s′


, we up-


# parameterized action value function Q(θ,s,a)


date our vector


with constant


∇Q


# gradient of action value function


learning rate


. Our parameter-


# action value function parameter


# learning rate


ized action value function is given


end


by


and its gradient is


∇Q


function


lookahead


model


::


GradientQLearning


return


model


model


end


function


update!


model


::


GradientQLearning


s′


model


model


model


model


model


maximum


s′


a′


for


a′


in


))


model


∇Q


+=


scale_gradient


return


model


end


14


Experience replay played an im-


portant role in the work of V.


Mnih, K. Kavukcuoglu, D. Silver, A.


Graves, I. Antonoglou, D. Wierstra,