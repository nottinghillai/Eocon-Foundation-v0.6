---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (17.14)
---

# (17.14)

_Source page: 363_



Although the impact of eligibility traces is especially pronounced in environ-


ments with sparse reward, the algorithm can speed learning in general environ-


ments where reward is more distributed.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


342


chapter 17. model-free methods


Algorithm 17.4. The Sarsa


up-


mutable struct


SarsaLambda


date, which uses eligibility traces to


# state space (assumes 1:nstates)


propagate reward back in time to


# action space (assumes 1:nactions)


speed learning of sparse rewards.


# discount


The matrix


contains the state-


# action value function


action values, the matrix


con-


# trace


# learning rate


tains exponentially decaying state-


# trace decay rate


action visit counts,


is a constant


# most recent experience tuple (s,a,r)


learning rate,


is an exponential


end


decay parameter, and


is the most


recent experience tuple.


lookahead


model


::


SarsaLambda


model


function


update!


model


::


SarsaLambda


s′


if


model


!=


nothing


model


model


model


model


model


model


+=


for


in


model


for


in


model


model


+=


model


model


*=


end


end


else


model


] .


0.0


end


model


return


model


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


17.5. reward shaping


343


Special care must be taken when applying eligibility traces to an off-policy


For an overview of this problem


algorithm like


-learning that attempts to learn the value of the optimal policy.


and a potential solution, see A.


Eligibility traces propagate back values obtained from an exploration policy. This


Harutyunyan, M. G. Bellemare, T.


mismatch can result in learning instabilities.


Stepleton, and R. Munos, “Q(


with Off-Policy Corrections,” in


In-


ternational Conference on Algorithmic


Learning Theory (ALT)


, 2016.