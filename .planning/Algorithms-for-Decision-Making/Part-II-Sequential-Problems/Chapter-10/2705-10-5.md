---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: '10.5'
---

# 10.5

_Source page: 445_



hungry


0.5


hungry


))


10


hungry


0.5


Ignoring the baby yields an expected reward:


hungry


ignore


hungry


) +


sated


ignore


sated


10


hungry


The expected reward for each action is plotted as follows over the belief space:


ignore


sing


feed


10


15


0.2


0.6


0.8


0.4


hungry


We find that under a one-step horizon, it is never optimal to feed or sing to the baby. The


ignore action is dominant.


Exercise 20.3.


Why does the implementation of value iteration in algorithm 20.8 call


expand


in algorithm 20.9 rather than evaluating the plan in algorithm 20.2 to obtain alpha


vectors for each new conditional plan?


Solution:


The plan evaluation method applies equation (20.8) recursively to evaluate the


expected utility for a conditional plan. Conditional plans grow very large as the horizon


increases. POMDP value iteration can save computation by using the alpha vectors for the


subplans from the previous iteration:


) =