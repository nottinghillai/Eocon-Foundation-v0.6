---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: 1 2 3 4 5 6 7 8 9 10
---

# 1 2 3 4 5 6 7 8 9 10

_Source page: 396_



floor


floor


floor


Solution:


These distributions over relative duration are analogous to distributions over


trajectories for this elevator problem. In applying the principle of maximum entropy, we


prefer the distribution with most entropy. Hence, we would choose policy B, which, in


being most uniform, has the greatest entropy.


Exercise 18.6.


Consider the policy optimization step in generative adversarial imitation


learning. Rewrite the objective in the form of a reward function so that traditional rein-


forcement learning techniques can be applied.


Solution:


We rewrite equation (18.19), dropping the terms dependent on the expert data


set, and flip the sign to change from minimization over


to a maximization over


of the


reward, producing the surrogate reward function:


) =


log


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


18.8. exercises


375


Although


may be quite different from the unknown true reward function, it can


be used to drive the learned policy into regions of the state-action space similar to those


covered by the expert.


Exercise 18.7.


Explain how generative adversarial imitation learning could be changed


such that the discriminator takes in trajectories rather than state-action pairs. Why might


this be useful?


Solution:


Changing generative adversarial imitation learning such that the discriminator


takes trajectories is straightforward, especially if the trajectories are of fixed length. The


expert data set is split into trajectories, and the learned policy is used to produce trajectories,


just as it was before. Rather than operating on state-action pairs, the discriminator takes in


trajectories using a representation such as a recurrent neural network (appendix D.5) and


produces a classification probability. The objective function remains largely unchanged:


max


min


log


))


log


))


∼D


15


This approach was used in A.


The advantage of running the discriminator over entire trajectories is that it can help


Kuefler, J. Morton, T. A. Wheeler,


the discriminator capture features that are not apparent from individual state-action pairs,


and M. J. Kochenderfer, “Imitating


which can result in better policies. For example, when looking at individual accelerations


Driver Behavior with Generative


and turn rates for an autonomous driving policy, there is very little for a discriminator to


Adversarial Networks,” in


IEEE In-


learn. A discriminator trained to look at longer trajectories can see more of the vehicle’s


telligent Vehicles Symposium (IV)


behavior, such as lane change aggressiveness and smoothness, to better match expert


2017.


15


driving demonstrations.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com