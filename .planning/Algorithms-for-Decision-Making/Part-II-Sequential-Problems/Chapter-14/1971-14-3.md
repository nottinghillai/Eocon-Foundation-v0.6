---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (14.3)
---

# (14.3)

_Source page: 304_



been discussed in the literature.


traj


traj


step


An overview of some of these


that


have


been


used


in


the


where


step


is a function that depends on the current state and action, much


context of MDPs is provided by


A.


Ruszczyński,


“Risk-Averse


like the reward function in MDPs. If


is defined as the expectation of


traj


Dynamic


Programming


for


the objective is the same as when solving an MDP, where


step


is simply the


Markov


Decision


Processes,”


reward function. We can thus use the policy evaluation algorithms introduced in


Mathematical Programming


, vol. 125,


no. 2, pp. 235–261, 2010.


section 7.2 to evaluate our policy with respect to any performance metric of the


form in equation (14.3).


We used


to represent the


Policy evaluation will output a value function that is a function of the state,


value function associated with pol-


corresponding to the expected value of the performance metric when starting


icy


in previous chapters.


from that state. Example 14.2 shows slices of this value function for the collision


avoidance problem. The overall performance is given by