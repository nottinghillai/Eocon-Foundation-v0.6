---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (12.24)
---

# (12.24)

_Source page: 283_



KL


·|


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


262


chapter 12. policy gradient optimization


Algorithm 12.6. An implementa-


struct


ClampedSurrogateUpdate


tion of clamped surrogate policy


# problem


optimization, which returns a new


# initial state distribution


policy parameterization for policy


# depth


of an MDP


with initial state


# number of trajectories


distribution


. This implementa-


# policy


# policy likelihood


tion samples


trajectories to depth


∇π


# policy likelihood gradient


, and then uses them to estimate


# divergence bound


the policy gradient in


k_max


subse-


# step size


quent updates. The policy gradient


k_max


# number of iterations per update


using the clamped objective is con-


end


structed using the policy gradients


∇p


with clamping parameter


function


clamped_gradient


::


ClampedSurrogateUpdate


θ′


τs


∇π


∇π


sum


-1


for


,(


))


in


zip


end


]))


∇f


r_togo


begin


θ′


if


r_togo


&&


||


r_togo


&&


return


zeros


length


))


end


return


∇π


θ′


r_togo


end


∇f


mean


∇f


))


for


,(


))


in


enumerate


))


return


mean


∇f


for


in


τs


end


function


update


::


ClampedSurrogateUpdate


k_max


k_max


πθ


τs


simulate


rand


),


πθ


for


in 1


θ′


copy


for


in 1


k_max


θ′


+=


clamped_gradient


θ′


τs


end


return


θ′


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


12.6. summary


263


which is the trust region policy objective where the constraint is implemented


as a penalty for some coefficient


. TRPO typically uses a hard constraint rather


than a penalty because it is difficult to choose a value of


that performs well


within a single problem, let alone across multiple problems.


surrogate objective


surrogate constraint


TRPO effective objective


clamped surrogate objective


Figure 12.7. A comparison of surro-


gate objectives related to clamped


surrogate policy optimization us-


ing the linear quadratic regulator


problem. The


-axis shows surro-


gate objectives as we travel from


at


toward


, given a natural


policy update at


. The surrogate


0.2


0.2


0.6


0.8


0.4


0.4


1.2


1.4


1.6


1.8


objectives were centered at


by


linear interpolation factor


subtracting the surrogate objective


function value for


. We see that


the clamped surrogate objective be-


haves very similarly to the effective


TRPO objective without needing a


constraint. Note that


and


can