---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (12.23)
---

# (12.23)

_Source page: 283_



Â·|


where


can be estimated from reward-to-go. The gradient of the lower-


bound objective equation (12.22) (with clamping), is the same, except there is no


contribution from experience tuples for which the objective is actively clamped.


That is, if either the reward-to-go is positive and the probability ratio is greater


than


, or if the reward-to-go is negative and the probability ratio is less than


, the gradient contribution is zero.


Like TRPO, the gradient can be computed for a parameterization


from


experience generated from


. Hence, several gradient updates can be run in


a row using the same set of sampled trajectories. Algorithm 12.6 provides an


implementation of this.


The clamped surrogate objective is compared to several other surrogate ob-


jectives in figure 12.7, which includes a line plot for the effective objective for