---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (12.6)
---

# (12.6)

_Source page: 274_



where the unnormalized search direction


is simply


. Of course, we do


not know


exactly, but we can use any of the methods described in the


previous chapter to estimate it. Algorithm 12.3 provides an implementation.


Algorithm 12.3. The update func-


struct


RestrictedPolicyUpdate


tion for the restricted policy gra-


# problem


dient method at


for a problem


# initial state distribution


with initial state distribution


# depth


The gradient is estimated from an


# number of samples


initial state distribution


to depth


∇logπ


# gradient of log likelihood


# policy


with


simulations of parameter-


# divergence bound


ized policy


with log policy


end


gradient


∇logπ


function


update


::


RestrictedPolicyUpdate


∇logπ


∇logπ


πθ


sum


-1


for


, (


))


in


enumerate


))


τs


simulate


rand


),


πθ


for


in 1


∇log


sum


∇logπ


for


in


∇U


∇log


mean


∇U


for


in


τs


return


sqrt


dot


))


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


12.3. natural gradient update


253