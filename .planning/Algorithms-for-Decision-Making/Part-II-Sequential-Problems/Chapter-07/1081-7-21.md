---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (7.21)
---

# (7.21)

_Source page: 170_



where


and


in equation (7.16) are replaced with their vector equivalents, the


summation is replaced with an integral, and


provides a probability density


rather than a probability mass. Computing equation (7.21) is not straightforward


for an arbitrary continuous transition distribution and reward function.


In some cases, exact solution methods do exist for MDPs with continuous


19


19


For a detailed overview, see chap-


state and action spaces.


In particular, if a problem has


linear dynamics


and has


ter 4 of volume I of D. P. Bertsekas,


quadratic reward


, then the optimal policy can be efficiently found in closed form.


Dynamic Programming and Optimal


Such a system is known in control theory as a


linear quadratic regulator


LQR


) and


Control


. Athena Scientific, 2007.


20


20


For a compact summary of LQR


has been well studied.


and other related control problems,


A problem has linear dynamics if the next state


after taking action


from


see A. Shaiju and I. R. Petersen,


state


is determined by an equation of the form:


“Formulas for Discrete Time LQR,


LQG, LEQG and Minimax LQG


Optimal Control Problems,”