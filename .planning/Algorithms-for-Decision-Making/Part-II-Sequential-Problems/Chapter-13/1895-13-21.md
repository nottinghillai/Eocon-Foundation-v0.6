---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (13.21)
---

# (13.21)

_Source page: 293_



We can tune parameter


to balance between bias and variance. If


then we have the high-bias, low-variance estimate for the temporal difference


residual from the previous section. If


, we have the unbiased full rollout


with increased variance. Figure 13.1 demonstrates the algorithm with different


values for


Algorithm 13.2. Generalized ad-


struct


GeneralizedAdvantageEstimation


vantage estimation for computing


# problem


both a policy gradient and a value


# initial state distribution


function gradient for an MDP


# depth


with initial state distribution


. The


# number of samples


policy is parameterized by


and


∇logπ


# gradient of log likelihood ∇logπ(θ,a,s)


# parameterized value function U(ϕ, s)


has a log-gradient


∇logπ


. The value


∇U


# gradient of value function ∇U(ϕ,s)


function


is parameterized by


# weight ∈[0,1]


and has gradient


∇U


. This method


end


runs


rollouts to depth


. The gen-


eralized advantage is computed


function


gradient


::


GeneralizedAdvantageEstimation


with exponential weighting


using


∇logπ


∇logπ


equation (13.21) with a finite hori-


∇U


∇U


zon. The implementation here is a


πθ


simplified version of what was pre-


sum


-1


for


,(


))


in


enumerate


end


]))


sented in the original paper, which


][


][


])


][


])


included aspects of trust regions


sum


((


-1


-1


for


in 1


when taking steps.


∇Uθ


sum


∇logπ


-1


for


, (


))


in


enumerate


end-1


]))


∇ℓϕ


sum


((


))


∇U


for


, (


))


in


enumerate


))


trajs


simulate


rand


),


πθ


for


in 1


return


mean


∇Uθ


for


in


trajs


),


mean


∇ℓϕ


for


in


trajs


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


272


chapter 13. actor-critic methods


Figure 13.1. A comparison of basic


actor-critic to generalized advan-


value function parameterization


policy parameterization


tage estimation on the simple regu-


lator problem with


0.9


, a Gaus-


sian policy


) =


and an approximate value func-


tion


) =


. We find


that generalized advantage estima-


tion is more efficiently able to ap-


0.5


proach well-performing policy and


0.5


value function parameterizations.


(Recall that the optimal policy pa-


rameterization is