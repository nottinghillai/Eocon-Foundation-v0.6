---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (11.5)
---

# (11.5)

_Source page: 254_



where


is the


th


standard basis


vector, consisting of zeros except for the


th


component, which is set to


As discussed in section 10.1, we need to simulate policy rollouts to estimate


. We can use algorithm 11.1 to generate trajectories. From these trajectories,


we can compute their return and estimate the utility associated with the policy.


Algorithm 11.2 implements the gradient estimate in equation (11.5) by simulating


rollouts for each component and averaging the returns.


Algorithm 11.1. A method for gen-


function


simulate


::


MDP


erating a trajectory associated with


[]


problem


starting in state


and


for


executing policy


to depth


. It


creates a vector


containing state-


s′


TR


action-reward tuples.


push!


, (


))


This implementation does not


s′


end


store the final state, as it is not


return


typically needed in the ensuing al-


end


gorithms. The final state after tak-


ing


actions should generally be


thought of as being part of a trajec-


tory rollout.


A major challenge in arriving at accurate estimates of the policy gradient is the


fact that the variance of the trajectory rewards can be quite high. One approach to


reduce the resulting variance in the gradient estimate is to have each rollout share


This random seed sharing is used


the same random generator seeds.


This approach can be helpful, for example,


in the PEGASUS algorithm. A. Y.


in cases where one rollout happens to hit a low-probability transition early on.


Ng and M. Jordan, “A Policy


Other rollouts will have the same tendency due to the shared random generator,


Search Method for Large MDPs


and POMDPs,” in


Conference on


and their rewards will tend to be biased in the same way.


Uncertainty in Artificial Intelligence


Policy representations have a significant effect on the policy gradient. Exam-