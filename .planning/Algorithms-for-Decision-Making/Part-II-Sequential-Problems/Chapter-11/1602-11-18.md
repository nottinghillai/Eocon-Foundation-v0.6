---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (11.18)
---

# (11.18)

_Source page: 259_



log


replaced with an action gradient.


Equations (11.17) and (11.18) require knowing the transition likelihood, which is


in contrast with equation (11.15) for stochastic policies.


Algorithm 11.4. A method for esti-


struct


LikelihoodRatioGradient


mating a policy gradient of a pol-


# problem


icy


for an MDP


with initial


# initial state distribution


state distribution


using the likeli-


# depth


hood ratio trick. The gradient with


# number of samples


respect to the parameterization vec-


∇logπ


# gradient of log likelihood


end


tor


is estimated from


rollouts to


depth


using the log policy gradi-


function


gradient


::


LikelihoodRatioGradient


ents


∇logπ


∇logπ


∇logπ


πθ


sum


-1


for


, (


))


in


enumerate


))


∇U


sum


∇logπ


for


in


return


mean


∇U


simulate


rand


),


πθ


))


for


in 1


end