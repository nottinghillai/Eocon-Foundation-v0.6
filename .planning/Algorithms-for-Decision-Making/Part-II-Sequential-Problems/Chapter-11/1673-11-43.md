---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (11.43)
---

# (11.43)

_Source page: 265_



base


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


244


chapter 11. policy gradient estimation


Algorithm 11.6. Likelihood ratio


struct


BaselineSubtractionGradient


gradient estimation with reward-


# problem


to-go and baseline subtraction for


# initial state distribution


an MDP


, policy


, and initial state


# depth


distribution


. The gradient with


# number of samples


respect to the parameterization vec-


∇logπ


# gradient of log likelihood


end


tor


is estimated from


rollouts to


depth


using the log policy gradi-


function


gradient


::


BaselineSubtractionGradient


ents


∇logπ


∇logπ


∇logπ


πθ


∇logπ


-1


sum


-1


for


,(


))


in


enumerate


end


]))


numer


sum


).


for


,(


))


in


enumerate


))


denom


sum


).


2 for


,(


))


in


enumerate


))


base


numer


) .


denom


trajs


simulate


rand


),


πθ


for


in 1


rbase


mean


base


for


in


trajs


∇U


sum


).


).


rbase


for


,(


))


in


enumerate


))


return


mean


∇U


for


in


trajs


end


likelihood ratio


reward-to-go


baseline subtraction


0.5


20


Figure 11.3. Several policy gradient


methods used to optimize policies


for the simple regulator problem


expected reward


from the same initial parameteriza-


40


tion. Each gradient evaluation ran


six rollouts to depth


10


. The magni-


20


0.5


10


tude of the gradient was limited to


iteration


, and step updates were applied


with step size


0.2


. The optimal pol-


icy parameterization is shown in


black.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


11.6. summary


245


It is common to use likelihood ratio policy gradient estimation with this base-


15


15


This combination is used in


line subtraction (algorithm 11.6).


. Figure 11.3 compares the methods discussed


the class of algorithms called


here.