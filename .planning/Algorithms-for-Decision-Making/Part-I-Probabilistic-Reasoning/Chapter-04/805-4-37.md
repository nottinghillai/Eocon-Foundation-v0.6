---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (4.37)
---

# (4.37)

_Source page: 106_



mis


mis


ing of four instances with six miss-


obs


obs


mis


ing entries.


where


mis


consist of all the observed and missing data, respectively.


obs


and


If the data is continuous, then the sum would be replaced by an integral. The


marginalization over the missing data can be computationally expensive. The


same marginalization also affects the computational tractability of a Bayesian


approach.


This section discusses two general approaches for learning with missing data


without having to enumerate over all the possible combinations of missing values.


The first involves learning the distribution parameters using predicted values of


the missing entries. The second involves an iterative approach for improving our


parameter estimates.


We will focus on the context where data is


missing at random


, meaning that


the probability that an entry is missing is conditionally independent of its value,


given the values of the observed variables. An example of a situation that does


not adhere to this assumption might include radar data containing measurements


of the distance to a target, but the measurement may be missing either due to


noise or because the target is beyond the sensing range. The fact that an entry is


missing is an indication that the value is more likely to be high. Accounting for


this form of missingness requires different models and algorithms from what we


Different


discuss here.


missingness mechanisms


and


associated


inference


tech-


niques are reviewed by R. J. A.


Little and D. B. Rubin,


Statistical