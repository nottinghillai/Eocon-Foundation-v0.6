---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: 4, 2
---

# 4, 2

_Source page: 326_



1.5


0.5


0.2


0.6


0.8


0.2


0.6


0.8


0.2


0.6


0.8


0.4


0.4


0.4


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


15.4. directed exploration strategies


305


Algorithm 15.4. The explore-then-


mutable struct


ExploreThenCommitExploration


commit exploration strategy. If


is


# pulls remaining until commitment


strictly positive, it will return a ran-


end


dom action after decrementing


Otherwise, it will return a greedy


function


::


ExploreThenCommitExploration


)(


model


::


BanditModel


action.


if


-=


return


rand


eachindex


model


))


end


return


argmax


mean.


model


))


end


pulls arm


with probability proportional to


exp


λρ


, where the


precision parame-


ter


controls the amount of exploration. We have uniform random selection


as


and greedy selection as


. As more data is accumulated, we may


want to increase


by a multiplicative factor to reduce exploration.


Algorithm 15.5. The softmax ex-


mutable struct


SoftmaxExploration


ploration strategy. It selects action


# precision parameter


with probability proportional to


# precision factor


exp


λρ


. The precision parameter


end


is scaled by a factor


at each step.


function


::


SoftmaxExploration


)(


model


::


BanditModel


weights


exp.


mean.


model


))


*=


return


rand


Categorical


normalize


weights


)))


end


This general strategy is related to


upper confidence bound exploration


A variety of exploration strategies are grounded in the idea of


optimism under


interval exploration


, and


interval es-


uncertainty


. If we are optimistic about the outcomes of our actions to the extent that


timation


, referring to the upper


bound of a confidence interval.


our data statistically allows, we will be implicitly driven to balance exploration


L. P. Kaelbling,


Learning in Embed-


and exploitation. One such approach is


quantile exploration


(algorithm 15.6),


ded Systems


. MIT Press, 1993. See


also E. Kaufmann, “On Bayesian


where we choose the arm with the highest


quantile


(section 2.2.2) for the payoff


Index Policies for Sequential Re-


probability. Values for


0.5


result in optimism under uncertainty, incentivizing


source Allocation,”


Annals of Statis-


the exploration of actions that have not been tried as often. Larger values of


tics


, vol. 46, no. 2, pp. 842–865,


2018.


result in more exploration. Example 15.3 shows quantile estimation and compares


it with the other exploration strategies.


An alternative to computing the upper confidence bound for our posterior


distribution exactly is to use


UCB1 exploration


(algorithm 15.7), originally intro-


duced in section 9.6 for exploration in Monte Carlo tree search. In this strategy,


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


306


chapter 15. exploration and exploitation


Algorithm 15.6.


Quantile explo-


mutable struct


QuantileExploration


ration, which returns the action


# quantile (e.g., 0.95)


with the highest


quantile.


end


function


::


QuantileExploration


)(


model


::


BanditModel


return


argmax


([


quantile


for


in


model


])


end


we select the action


that maximizes


log