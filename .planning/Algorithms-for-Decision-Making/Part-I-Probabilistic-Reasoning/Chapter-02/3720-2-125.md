---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: '2.125'
---

# 2.125

_Source page: 604_



10


. We would take a small step in the opposite


direction to improve our function approximation.


Data sets for modern problems tend to be very large, making the gradient of


equation (D.1) expensive to evaluate. It is common to sample random subsets of


the training data in each iteration, using these


batches


to compute the loss gradient.


A sufficiently large, single-layer


In addition to reducing computation, computing gradients with smaller batch


neural network can, in theory, ap-


sizes introduces some stochasticity to the gradient, which helps training to avoid


proximate any function. See A.


getting stuck in local minima.


Pinkus, “Approximation Theory


of the MLP Model in Neural


Networks,”


Acta Numerica


, vol. 8,


pp. 143–195, 1999.