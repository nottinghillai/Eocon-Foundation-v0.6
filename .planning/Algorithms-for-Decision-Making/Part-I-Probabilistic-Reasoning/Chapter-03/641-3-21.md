---
converted: '2025-10-25'
source: AlgorithmforDecisionMaking.pdf
title: (3.21)
---

# (3.21)

_Source page: 83_



Figure 3.6 compares the convergence of the estimate of


in the chem-


ical detection network using direct, likelihood weighted, and Gibbs sampling.


Direct sampling takes the longest to converge. The direct sampling curve has long


periods during which the estimate does not change because samples are incon-


sistent with the observations. Likelihood weighted sampling converges faster in


this example. Spikes occur when a sample is generated with


, and then


gradually decrease. Gibbs sampling, in this example, quickly converges to the


true value of


0.5


As mentioned earlier, Gibbs sampling, like other Markov chain Monte Carlo


methods, produces samples from the desired distribution in the limit. In practice,


we have to run Gibbs for some amount of time, called the


burn-in period


, before


converging to a steady-state distribution. The samples produced during burn-in


are normally discarded. If many samples are to be used from a single Gibbs


sampling series, it is common to


thin


the samples by keeping only every


th


sample because of potential correlation between samples.


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


62


chapter 3. inference


Algorithm 3.10. Gibbs sampling


function


update_gibbs_sample!


bn


evidence


ordering


implemented for a Bayesian net-


for


in


ordering


work


bn


with evidence


evidence


name


bn


vars


].


name


and an ordering


ordering


. The


if


haskey


evidence


name


method iteratively updates the as-


blanket


bn


signment


for


iterations.


name


rand


)[


name


end


end


end


function


gibbs_sample!


bn


evidence


ordering


for


in 1


update_gibbs_sample!


bn


evidence


ordering


end


end


struct


GibbsSampling


m_samples


# number of samples to use


m_burnin


# number of samples to discard during burn-in


m_skip


# number of samples to skip for thinning


ordering


# array of variable indices


end


function


infer


::


GibbsSampling


bn


query


evidence


table


FactorTable


()


merge


rand


bn


),


evidence


gibbs_sample!


bn


evidence


ordering


m_burnin


for


in 1


m_samples


gibbs_sample!


bn


evidence


ordering


m_skip


select


query


table


get


table


end


vars


filter


->


name


query


bn


vars


return


normalize!


Factor


vars


table


))


end


© 2022 Massachusetts Institute of Technology, shared under a Creative Commons CC-BY-NC-ND license.


2025-09-21 10:49:56-07:00, comments to bugs@algorithmsbook.com


3.9. inference in gaussian models


63


Figure 3.6.


A comparison of


direct sampling


sampling-based inference methods


likelihood weighted


on the chemical detection network.


Gibbs sampling


Both likelihood weighted and di-


0.8


rect sampling have poor conver-


gence due to the rarity of events,


whereas Gibbs sampling is able


0.6


to converge to the true value effi-


ciently, even with no burn-in pe-


riod or thinning.


0.4


estimate of


0.2