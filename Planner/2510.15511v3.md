5
2
0
2

t
c
O
1
2

]

G
L
.
s
c
[

3
v
1
1
5
5
1
.
0
1
5
2
:
v
i
X
r
a

Preprint.

LANGUAGE MODELS ARE INJECTIVE
AND HENCE INVERTIBLE

Giorgos Nikolaou‡∗ Tommaso Mencattini†,‡,∗

Donato Crisostomi† Andrea Santilli† Yannis Panagakis§,¶ Emanuele Rodol`a†

†Sapienza University of Rome

‡EPFL

§University of Athens

¶Archimedes RC

ABSTRACT

Transformer components such as non-linear activations and normalization are in-
herently non-injective, suggesting that different inputs could map to the same out-
put and prevent exact recovery of the input from a model's representations. In
this paper, we challenge this view. First, we prove mathematically that trans-
former language models mapping discrete input sequences to their corresponding
sequence of continuous representations are injective and therefore lossless, a prop-
erty established at initialization and preserved during training. Second, we confirm
this result empirically through billions of collision tests on six state-of-the-art lan-
guage models, and observe no collisions. Third, we operationalize injectivity: we
introduce SIPIT, the first algorithm that provably and efficiently reconstructs the
exact input text from hidden activations, establishing linear-time guarantees and
demonstrating exact invertibility in practice. Overall, our work establishes injec-
tivity as a fundamental and exploitable property of language models, with direct
implications for transparency, interpretability, and safe deployment.

1

INTRODUCTION

A core question in understanding large language
models is whether their internal representations
faithfully preserve the information in their inputs.
Since Transformer architectures rely heavily on non-
linearities, normalization, and many-to-one atten-
tions mechanisms, it is often assumed that they dis-
card information: different inputs could collapse to
the same hidden state, making exact recovery of
the input impossible. This view motivates concerns
around transparency, robustness, and safe deploy-
ment, as it suggests that the link between text and
representation is inherently lossy.

LLM

SIPIT

Figure 1: The map from prompts to latent
space is injective. SIPIT inverts it.

In this paper, we show that this intuition is misleading. Despite their apparent complexity, standard
decoder-only Transformer language models (seen as maps from prompts to hidden states) are in
fact almost-surely injective; for essentially all parameter settings and during the course of training,
different prompts yield different last-token representations (e.g., see Figure 1).

Building upon this property, we further provide a practical algorithm, SIPIT, that reconstructs the
exact input from hidden activations. To our knowledge, it is the first to guarantee exact recovery in
provable linear time (worst case bound), often faster in practice, turning injectivity from a theoretical
property into an operational tool.

Our approach. To establish our result, we take a rigorous mathematical view of Transformers
as functions. The key idea is that their components (embeddings, LayerNorm, causal attention,
MLPs, and residual wiring) are smooth and structured enough that the model, as a whole, behaves
predictably with respect to its parameters. Using tools from real analysis, we show that collisions

∗Equal contribution; author order settled via Mario Kart.

1

LATENT SPACEεδxx(cid:31)z(cid:31)zRdδ>0=⇒ε>0PROMPT SPACE

Preprint.

(two different prompts producing the exact same representation) can only occur on a set of parameter
values that has measure zero; that is, they are mathematical exceptions rather than possibilities one
should expect in practice. Moreover, we prove that common training procedures (gradient descent
with standard step sizes) never move parameters into this exceptional set. In layman's terms, almost
all models at initialization are injective, and training preserves this property.

Technically, our proofs rely on two ingredients. First, we establish that Transformers are real-
analytic functions of their parameters, which allows us to reason precisely about when and where
collisions could occur. Second, we construct parameter settings where no two prompts collide, and
show that gradient descent (GD) does not collapse such separation, i.e., collisions remain a measure-
zero event. The end result is a finite-horizon guarantee: after any fixed number of training steps, and
under mild assumptions, injectivity holds with probability one. We provide complete formal proofs
of these statements.

Main result. Our central finding is that causal decoder-only Transformer language models are
injective almost surely. Formally, consider one such model with embedding width d, at least one
attention head per block, real-analytic components, finite vocabulary
, and finite context length K.
Initialize its parameters θ at random, using any distribution that has a density1 (such as Gaussian,
uniform, or Xavier/Glorot), and train for any finite number T of GD steps with step sizes in (0, 1).
Then, with probability one over the random initialization,

V

= s′ =

s

r(s ; θT )

= r(s′ ; θT ) ,

⇒

i.e., the map from prompts s to last-token representations r(s ; θT ) is injective across all prompts in
≤K. In short, collisions in practical settings form a measure-zero set, and neither initialization nor

V
training will ever place a model inside that set.

Significance. Our result shows that in standard decoder-only Transformers, different prompts al-
most surely yield different last-token representations across all practically relevant parameter set-
tings and training procedures. The guarantee is both generic (it fails only on a measure-zero set
of pathological parameters) and practical (it holds at finite width, depth, and training time under
common initializations).

Conceptually, we replace a long-assumed property with a rigorous theorem, showing that injectivity
is not an asymptotic idealization but a structural consequence of the architecture itself. Techni-
cally, our analytic framework pinpoints when collisions can arise (through deliberate non-analytic
choices such as quantization or tying), and clarifies that otherwise the model is inherently lossless.
Importantly, it establishes that last-token states almost everywhere identify the input.

Finally, we turn this theoretical guarantee into an operational tool: our algorithm SIPIT uses
gradient-based reconstruction to recover prompts exactly from internal activations, efficiently and
with provable linear-time guarantees. This confirms empirically that collisions do not occur in
practice. Beyond transparency and safety, this elevates invertibility to a first-class property of Trans-
former language models, enabling stronger interpretability, probing, and causal analyses.

2 TRANSFORMERS ARE INJECTIVE

Summary.
In this section we show that decoder-only Transformers almost surely map different
prompts to different hidden states. Collisions can only occur under measure-zero parameter choices,
and gradient-based training never creates them. In simple terms, Transformer representations are
structurally lossless.

Approach. We consider causal decoder-only Transformer language models with vocabulary
finite context window K, and embedding dimension d. For an input sequence s
∈ V
denote the final hidden representation at the last token position2, given parameters θ.

,
V
≤K, let r(s ; θ)

Our analysis relies on three facts:

1Put simply, parameters are not drawn from a degenerate or hand-crafted set.
2We focus on the last-token state, since it alone drives next-token prediction; earlier rows matter only insofar

as they shape this final state. Injectivity at the last token is the property of real operational interest.

2

̸
̸
Preprint.

(i) Real-analyticity. Each component of the architecture (embeddings, positional encodings,
LayerNorm with ε > 0, causal attention, MLPs with analytic activations, residuals) is real-
analytic in its parameters (see Appendix A.2 for the mathematical background). This
smoothness implies that the set of parameter values causing two distinct prompts to col-
lide is extremely thin (measure zero).

(ii) Initialization. Standard initialization schemes (Gaussian, uniform, Xavier/Glorot, etc.)
draw parameters from continuous distributions with densities, so they avoid measure-zero
sets with probability one.

(iii) Training. Gradient-based updates (including SGD and mini-batch/full-batch GD) preserve
absolute continuity of the parameter distribution after any finite number of steps; thus,
training cannot generate collisions.

These facts allow us to state and prove injectivity results without relying on asymptotics.

We begin by establishing the analytic structure of the architecture.
Theorem 2.1 (Transformers are real-analytic). Fix embedding dimension d and context length K.
Assume the MLP activation is real-analytic (e.g. tanh, GELU). Then for every input sequence s

∈

≤K, the map

V

(s, θ)

(cid:55)→

r(s ; θ)

d
R

∈

(1)

is real-analytic jointly in the parameters θ and the input embeddings.

Sketch of proof (full proof in Appendix B, Proposition B.3). Each building block is real-analytic:
polynomials (embeddings, projections), exponential and softmax (attention), reciprocal square root
(LayerNorm with ε > 0), analytic activations in the MLP, and affine maps. Real-analytic functions
are closed under addition, multiplication, quotient, and composition. Since the Transformer is a
finite composition of such blocks, the entire map is real-analytic.

This smoothness result drives everything that follows:
it ensures that collisions, if they exist, are confined to
measure-zero parameter sets. We now ask: what happens
at initialization?
Theorem 2.2 (Almost-sure injectivity at initialization).
Let θ be drawn from any distribution with a density (e.g.
Gaussian or uniform). Then for any two distinct prompts
s, s′

≤K,

∈ V

f1

f2

Pr[r(s ; θ) = r(s′ ; θ)] = 0 .

(2)

f1

f2

−

Sketch of proof (full proof in Appendix C, Theorem C.2).
Fix s

= s′ and consider

h(θ) =

r(s ; θ)
∥

−

r(s′ ; θ)

2
2 .

∥

(3)

Figure 2: Two real-analytic functions
f1 and f2 and their difference f1
−
f2. Black contours show the zero sets,
which form thin curves (measure zero)
rather than regions of positive measure.

= r(s′ ; θ).

By Theorem 2.1, h is real-analytic. A fundamental di-
chotomy of real-analytic functions states that either h is
identically zero, or its zero set has Lebesgue measure zero (see Figure 2 for an illustration). There-
fore, to rule out the pathological case h
0 it suffices to exhibit a single parameter setting where
r(s ; θ)
This can always be done: if s and s′ differ at the last position (symbol or length), freeze the network
so that the last state reduces to embedding plus position, and choose distinct rows; this already
separates r(s) and r(s′). If instead they differ earlier, let i⋆ be the first mismatch and set one attention
head so the last position attends almost entirely to i⋆, encoding its token in the value; this forces
different outputs for s and s′.

≡

Hence h is not identically zero, and so the collision set
has Lebesgue measure
zero. Since standard initializations have densities, the probability of sampling such θ is zero, and
r(s ; θ)

= r(s′ ; θ) (injectivity) holds almost surely at initialization.

θ : h(θ) = 0

{

}

3

̸
̸
̸
Preprint.

According to Theorem 2.2, at initialization, collisions are mathematically impossible except on a
vanishingly small set of parameter values. Finally, with the following Theorem we ensure training
does not break injectivity.
Theorem 2.3 (Injectivity preserved under training). Let θ0 be initialized from a distribution with a
density, and let θT be the parameters after T steps of gradient descent with step sizes in (0, 1). Then
with probability one,

s

= s′ =

⇒

r(s ; θT )

= r(s′ ; θT ) ,

(4)

Sketch of proof (full proof in Theorems C.1 and C.5). At initialization, θ0 is drawn from a distribu-
tion with a density, hence absolutely continuous. To break injectivity during training, GD would
need to map this continuous law onto the measure-zero collision set identified in Theorem 2.2. We
show this cannot happen.

η

−

∇L

(θ), where

A single GD step is the map ϕ(θ) = θ
is the training loss. Because the network
and the softmax cross-entropy loss are real-analytic, ϕ is also real-analytic. Its Jacobian determinant
det Dϕ(θ) is itself real-analytic and not identically zero (one can check this by evaluating at a
simple parameter setting). Hence the set where det Dϕ = 0 has measure zero. Away from that set,
the Inverse Function Theorem applies: ϕ is a smooth, locally invertible change of coordinates that
can stretch or bend space but cannot collapse regions of positive volume onto lower-dimensional
sets. Therefore, pushing forward an absolutely continuous distribution through ϕ yields another
absolutely continuous distribution.

L

Since this argument holds for each step, any finite sequence of GD updates preserves absolute con-
tinuity of the parameter law. Combining with Theorem 2.2, which shows that collision sets are
measure-zero, we conclude that r(s ; θT )

= r(s′ ; θT ) almost surely for all s

= s′.

Thus injectivity is not just an initialization property but remains true throughout training. A simple
but important corollary follows.
Corollary 2.3.1 (SGD and mini-batch GD). Under the assumptions of Theorem 2.3, the same con-
clusion holds when the updates are θt+1 = θt
Bt(θt) with arbitrary (possibly random or
ηt
adversarial) batch selections

t, thus including the singleton case of SGD and the full dataset.

∇

−

L

θ

B

η

−

∇L

Proof. The proof argument of Theorem 2.3 is unchanged: for each fixed batch
B(θ) is real-analytic with a Jacobian that is not identically zero. Indeed, the
, the update map
ϕB(θ) = θ
i, so at the point θ⋆ from the single-sample proof (where
batch loss is the average
the Jacobian determinant is sample-independent and nonzero) the batch Jacobian coincides with the
single-sample one by linearity of differentiation, and its determinant is therefore also nonzero. Thus,
the finite composition of such maps preserves absolute continuity of the parameter law.

B = 1
|B|

i=1 L

(cid:80)|B|

L

B

Together with this robustness to different training regimes, we can also strengthen the guarantee
itself: injectivity holds not just pairwise, but globally across finite sets of prompts.
Corollary 2.3.2 (Distinctness for finite sets). For any finite set of prompts
tations

are almost surely all distinct.

≤K, the represen-

S ⊆ V

r(s ; θT ) : s
{

∈ S}

Proof. See Appendix C, Corollary C.2.1.

These results show that decoder-only Transformer language models are structurally injective: dif-
ferent prompts almost surely yield different last-token states. Collisions can be manufactured,
e.g., through deliberate non-analytic choices (quantization, non-smooth activations), but in practical
training pipelines, injectivity is guaranteed; extensive experiments in §4.1 confirm this empirically.

Failure cases. We showed that non-injective transformers are overwhelmingly unlikely, though it
is still possible for an adversary to construct collisions by hand. For instance, if two vocabulary
= vj are assigned exactly the same embedding vector, then any prompts differing only by
items vi
swapping vi and vj yield identical representations. Likewise, if two absolute positional embeddings
are made exactly equal and the remaining weights are tuned to suppress other positional signals,

4

̸
̸
̸
̸
̸
Preprint.

Figure 3: Seeking collisions in a large-scale prompt set (§4.1). The minimum distances between
last-token states are far above the collision threshold 10−6: (left) across layers for GPT-2 and
Gemma-3 families (one dot per layer), (right) across depth for GPT-2 Small, where distances
grow with depth.

one can force collisions between sequences that differ only at those positions. These scenarios,
however, require deliberately engineered parameter choices: under continuous random initialization
and standard training, the probability of such coincidences is zero.

3 EXACT PROMPT RECOVERY VIA SIPIT

In the previous section, we have proven that decoder-only Transformers are almost surely injective,
i.e., different prompts map to different hidden states. We now show how this property can be used
in practice to reconstruct the exact input prompt given hidden states at some layer. We call this
algorithm SIPIT (Sequential Inverse Prompt via ITerative updates).

Formally, recall from §2 that the mapping from a prompt s to its last-token state is almost surely
injective. Since the last state is itself a deterministic function of the hidden matrix at any layer ℓ,
injectivity extends to the full representation

s

(cid:55)→

H(ℓ)(s)

T ×d .

R

∈

(5)

We denote by ht(s) the row of H(ℓ)(s) at position t. In the following, the parameters θ and target
layer ℓ are considered fixed and omitted for simplicity.

The algorithm exploits the causal structure of Transformers: the hidden state at position t depends
only on the prefix
and the current token st. This means that if we already know the
⟩
prefix, then the hidden state at position t uniquely identifies st.

s1, . . . , st−1

⟨

Example. Suppose the vocabulary is a, b, c and the true prompt is
. At t = 1, the hidden state
depends only on s1. By comparing the observed state with the three candidate states produced by
trying a, b, and c, we can tell exactly which one matches, thus recovering s1 = a. Then at t = 2, we
, so we try appending each candidate token and again match the resulting hidden
know the prefix
a
⟩
⟨
state to recover s2 = b. Iterating this procedure reconstructs the full sequence.

a, b

⟨

⟩

More generally, we can look at the "one-step" map

vj

(cid:55)→

ht(π

⊕

vj) ,

vj

,

∈ V

(6)

which gives the hidden state at step t for each possible next token, given the fixed prefix π =
s1, . . . , st−1

⟨
Remark. By the analytic arguments of §2, the one-step map is almost surely injective: with a fixed
prefix, any two distinct tokens almost surely yield distinct hidden states.

denotes concatenation).

(here

⊕

⟩

This property makes sequence recovery straightforward. At each step t, given the hidden state (cid:98)ht and
the already recovered prefix, we simply check which candidate token produces a matching hidden
state. That token must be the true st. Repeating this process recovers the entire sequence.

5

Gemma-3/1BGemma-3/4BGemma-3/12BGPT-2/SGPT-2/MGPT-2/L10−510−310−1101103L2Distance(min)Collisionthreshold123456789101112Layer10−510−310−1101103L2DistanceCollisionthresholdPreprint.

This leads to the SIPIT algorithm, shown in Algorithm 1. At every position, the algorithm cycles
through vocabulary candidates (according to some policy such as random order or gradient-guided
search) until it finds the unique match3, then appends it to the reconstructed prefix and moves on.

Algorithm 1 SIP-IT: Sequential Inverse Prompt via Iterative Updates

RT ×d; vocabulary

; tolerance ε

0.

≥

V

∈
ˆs1, . . . , ˆsT
⟨

.

⟩

▷ tested candidates

▷ new candidate token vj (see Alg. 2 and 3)
▷ verify vj (see Def. D.2)
▷ hit!

Require: Observed layer-ℓ states (cid:98)H(ℓ)
Ensure: Recovered sequence (cid:98)s =
1: (cid:98)s
2: for t = 1 to T do
3:
4:
5:

POLICY (

← ⟨ ⟩

|V|

do

,(cid:98)s, ℓ)
,
V
π,t(vj ; ε) then
vj

C

C ← ∅
for j = 1 to
vj
←
if (cid:98)ht
∈ A
(cid:98)s
← (cid:98)s
⊕
break

else

6:
7:
8:
9:
10:
11:
12:
13: end for
14: return (cid:98)s

end if
end for

C ← C ∪ {

}

vj

To rule out edge cases and analyze the computational cost of SIPIT, we now state a formal guarantee.

Theorem 3.1 (Correctness of SIPIT). Under the assumptions of Theorem 2.3, given observed hidden
states (cid:98)H(ℓ), SIPIT recovers the true input sequence s with probability one in at most T

steps.

|V|

Sketch of proof (full proof in Appendix D, Thm. D.2, Prop. D.4). At each step, local injectivity en-
sures a unique token matches the observed state. As the policy spans the vocabulary, this token will
be found in at most

trials. Induction over t = 1, . . . , T completes the argument.

|V|

In short, SIPIT turns the almost-sure injectivity of Transformer representations into a constructive
procedure: not only are hidden states unique identifiers of prompts, but the exact input sequence
can be efficiently recovered in linear time, and often faster in practice. It is a structural property of
Transformer representations, not a quirk of initialization or training.

4 EXPERIMENTS

We previously proved that decoder-only Transformers are injective (§2) and introduced an algorithm,
SIPIT, that leverages this property to recover the exact input prompt from hidden states at a given
layer (§3). We now provide extensive empirical evidence supporting our theory by showing that
distinct prompts yield distinct embeddings, i.e., no collisions occur by a large margin (§4.1). We
then demonstrate that SIPIT successfully reconstructs the original input prompt (§4.2).

Environment. All experiments were run on a single NVIDIA A100-SXM (64 GB) GPU.
Python 3.11, CUDA 12.2, PyTorch 2.8.0, and transformers 4.50.0 were used for all experi-
ments. Reported runtimes refer to this setup.

4.1 SEARCHING FOR COLLISIONS

We collected 100k prompts by uniformly sampling from a mixture of
wikipedia-en4, C4 (Raffel

al., 2020), The Pile (Gao et

four datasets:
and

al., 2020),

et

3In practice, we accept matches if the observed hidden state is within an ε-ball around the predicted one.
4https://huggingface.co/datasets/wikimedia/wikipedia

6

Preprint.

GPT-2 Small

Gemma3-1B

Figure 4: Exhaustive collision search on the 10 closest prefix prompts. The boxplots look flat and
uneventful, and that is the point: even under stress-test conditions with billions of candidate pairs,
all minima stay well above the collision threshold, showing that nothing collapses.

python-github-code5. For each prompt, we extracted the last-token representation and
systematically checked whether any two distinct prompts produced identical embeddings. This
process required around 5 billion pairwise comparisons.

We observed no collisions across all models and layers:
distinct prompts always yielded distinct last-token states.
Figure 3 (left) shows the per-layer minimum distances for
the Gemma3 pretrained (Team et al., 2025) and GPT-2
(Radford et al., 2019) families, with strictly positive val-
ues throughout. Table 1 complements this by report-
ing the same statistic for Llama-3.1-8B (Grattafiori
et al., 2024), Mistral-7B-v0.1 (Jiang et al., 2023),
Phi-4-mini-instruct (Microsoft et al., 2025) and
TinyStories-33M (Eldan & Li, 2023), again show-
ing clear separation at the first, middle, and last layers.

Model

Llama-3.1-8B
Mistral-7B-v0.1
Phi-4-mini-ins
TinyStories-33M

layer 1

layer L

L2 Distance (min)
layer L
2
0.129
0.187
1.336
1.434

0.620
1.274
9.020
2.793

0.001
0.002
0.014
0.029

Table 1: Minimum pairwise distance
between last-token states in the first,
middle, and final layers of four models.
All values are well above the collision
threshold 10−6 (no collisions).

Finally, Figure 3 (right) zooms in on GPT-2 Small, re-
vealing that these distances typically increase with depth. Additional results for GPT-2 Medium,
GPT-2 Large and Gemma3 (1B, 4B, 12B) appear in Appendix E, confirming the same trend.

Figure 5 shows how pairwise distances between last-
length in GPT-2
token states vary with prompt
Small. Three patterns emerge: (i) the minimum dis-
tance is never close to zero at all lengths, and (ii) it
grows rapidly at short lengths but then levels off, sug-
gesting that beyond a moderate context size, adding to-
kens does not affect separability; (iii) the overall spread
(min-max) stays bounded, with no sign of pathologi-
cal collapses. Similar behavior is seen in Gemma3 (see
Appendix E, Figure 9). Overall, clear margins emerge
quickly and then stabilize, making collisions unlikely at
any sequence length.

Figure 5: Sequence length vs. pairwise
distance for GPT-2. Min, mean, and max
distances rise at short lengths and then sta-
bilize, indicating consistent separability.

Exhaustive collision test. Different from previous ex-
periments, in this setting (Figure 4), we restrict our
analysis to the 10 prompts from the dataset mixture
whose embeddings have the smallest last-token distances. For each of these prompts, we appended
every vocabulary token and computed all pairwise distances between the resulting last-token states,
effectively performing an exhaustive search over continuations and yielding more than 343 billion
prompt pairs per model.

This exhaustive experiment helps rule out the possibility that earlier observations were simply due
to chance in random sampling rather than a true absence of collisions. While a complete search over
all possible prompts would be ideal, it is computationally infeasible. The number of unique prompts
grows exponentially with sequence length, and the number of pairwise comparisons grows even
faster. For context, even with single-token prompts and the vocabulary size of Gemma3-1B, there

5https://huggingface.co/datasets/angie-chen55/python-github-code

7

0123456789SampleIndex10−710−1102L2DistanceCollisionthreshold0123456789SampleIndex10−710−1104L2DistanceCollisionthreshold0100200300400500Sequencelength10−510−310−1101103L2DistanceCollisionthresholdmeanminmaxPreprint.

are already over 34 trillion possible prompt pairs, making exhaustive evaluation entirely impractical.
Our compromise still revealed structure: we identified 5 prompt pairs with highly similar last-token
embeddings, suggesting overlapping semantic content and motivating us to ask whether distinct next
tokens could preserve meaning, i.e., yield essentially identical last-token hidden states.

Figure 4 reports the resulting distributions (min/median/mean/max) as boxplots for both GPT-2
Small and Gemma3-1B, with distances far from zero (no collision), confirming local injectivity
as predicted by our theory.

4.2

INVERTIBILITY RESULTS

We now test whether the theoretical injectivity trans-
lates into exact recovery on pre-trained models. Using
SIPIT with only the hidden states at a fixed layer, we at-
tempt to reconstruct the full prompt token-by-token for
GPT-2 Small. We sample 100 prompts, with a 90%-
10% split between meaningful sentences and random to-
ken sequences (to test robustness in unstructured cases),
and attempt to reconstruct them from hidden states.

Method

Mean Time (s)

Accuracy

HARDPROMPTS
BRUTEFORCE (ours)
SIPIT (ours)

6132.59
3889.61
28.01

104.61
691.17
35.87

±
±
±

0.00
1.00
1.00

Table 2: Prompt inversion: SIPIT en-
sures exact recovery efficiently, unlike
HARDPROMPTS (no recovery) or brute
force (infeasible runtimes).

We compare against HARDPROMPTS (Wen et al.,
2023), which leverages gradient signals for approximate
prompt discovery, and against a SIPIT ablation without the gradient-guided candidate policy
(BRUTEFORCE).

Other inversion approaches (Morris et al., 2023a;b; Nazir et al., 2025) tackle a different setting
altogether: they operate in black box access, using sequences of next-token logprobs or encoder
logits rather than hidden states, and train auxiliary inverters to reconstruct text, at high computational
cost. Their outputs are typically approximate and not guaranteed exact. These differences make
them complementary but not directly comparable to our setting of training-free, exact inversion
from hidden states in decoder-only LMs.

Results are reported in Table 2. Across all prompts (20 tokens each),
SIPIT recovers the exact sequence with 100% token-level accuracy (no
errors, no collisions), matching the theoretical guarantee of linear-time
convergence.

In contrast, HARDPROMPTS fails to recover the true input in most
cases, while BRUTEFORCE eventually succeeds but at a prohibitive
computational cost, requiring several orders of magnitude longer.

Finally, Figure 6 shows inversion times by layer for longer prompts
(ranging from 20 to 200 tokens). Although deeper layers are costlier in
principle (since verifying a candidate and computing gradients require
traversing more blocks), the effect is minor: runtimes rise only slightly
from first to last layer, and the scaling remains graceful overall. Likely,
earlier layers need more iterations to converge, while deep layers store
richer information that reduces the search effort. As a result, the net
cost remains stable, confirming SIPIT is efficient across depth.

Figure 6:
Inversion time
as a function of depth.
Runtimes rise only mildly
across layers.

5 RELATED WORK

Our results connect to two active lines of research: theoretical analyses of Transformer architectures,
and inverse problems in language modeling. We briefly review both to position our contributions.

Analytical properties of Transformers. Viewed as functions on Rd, individual Transformer
components are clearly non-injective: LayerNorm collapses along per-example statistics (Ba
et al., 2016), residual connections can cancel, and in attention-only stacks, rank decays doubly-
exponentially with depth (Dong et al., 2021). Likewise, on the output side, the softmax bottleneck
constrains the distributions reachable by language models (Yang et al., 2018). From this algebraic

8

050100150Inversiontime(s)123456789101112Preprint.

perspective, Transformers seem inherently many-to-one, while in a generative sense, they can also
behave one-to-many when different prompts lead to the same continuation.

≤K to hidden
Our focus is different: we study the discrete-to-continuous map from prompts s
states in Rd. In this setting, analytic viewpoints on Transformer computation become powerful:
treating each layer as a real-analytic map yields almost-sure guarantees that hold at finite width,
depth, and training horizon. Recent work has adopted this angle for related properties: Jiang &
Haghtalab (2025) show that building blocks of modern architectures are almost always surjective,
while Sutter et al. (2025) prove that Transformers at random initialization are almost surely injective
with respect to the entire hidden-state matrix (and only at initialization).

∈ V

Differently, we prove injectivity with respect to the parameters and at the task-relevant last-token
state; crucially, we show that injectivity is not an initialization artifact but persists under training.

Inverse problems in language modeling.
Inverse problems seek to recover an unknown input
x from observations y produced by a forward process y = f (x) (Sun et al., 2021). Within this
landscape, language model inversion asks whether one can reconstruct a model's input prompt from
outputs or internal signals.

Several approaches have explored this idea. Output-to-prompt methods infer prompts from gener-
ated continuations, yielding approximate reconstructions that are often semantically similar rather
than exact (Zhang et al., 2024). Recent work by Morris and coauthors shows that model outputs are
information-rich even in black-box settings: Morris et al. (2023b) train a separate inverter to map
next-token probability vectors to text, and Nazir et al. (2025) extend this by taking sequences of
logprobs, applying a linear compression to embedding dimension, and training an encoder-decoder
inverter; this achieves higher exact-match rates but still without guarantees. Complementarily, Mor-
ris et al. (2023a) reconstruct text from encoder logits via a trained iterative inverter. These contri-
butions highlight privacy risks when probabilities or embeddings are exposed, but they differ from
our setting: they rely on trained inverters, remain approximate, and do not invert hidden states of
decoder-only LMs.

A related line of work frames the task as automated prompt optimization, casting prompt design as
discrete sequence optimization aligned with downstream performance (Guo et al., 2025; Sun et al.,
2022; Deng et al., 2022); methods such as AutoPrompt (Shin et al., 2020) and Hard Prompts Made
Easy (Wen et al., 2023) use gradient signals to discover effective, but approximate, prompts.

Unlike prior work, which yields approximate reconstructions from outputs, logits, or logprobs, our
approach is training-free, efficient, and comes with provable linear-time guarantees for exact recov-
ery from internal states.

6 DISCUSSION AND CONCLUSIONS

This work establishes that decoder-only Transformers are almost surely injective: distinct prompts
produce distinct hidden states under standard initialization and training. Building on this structural
result, we introduced SIPIT, the first algorithm that can recover the exact input sequence from hidden
activations, with provable linear-time guarantees. Together, these contributions move injectivity
from an informal belief to a rigorously grounded and operational property of language models.

The scientific impact is clear. Our findings reconcile two competing views in the community: Trans-
formers as "lossy" due to nonlinearities, normalization, and many-to-one attention, versus language
models as injective in their hidden representations. We advocate viewing language models as maps
on the sequence space rather than the embedding space; under this perspective, we prove that all
information about the input sequence is almost surely preserved end-to-end. The constructive in-
version offered by SIPIT strengthens this point in practice, establishing a clean baseline for inter-
pretability and auditing: if probes or inversion methods fail, it is not because the information is
missing. For mechanistic interpretability in particular, injectivity guarantees that last-token states
faithfully encode the full input, giving a sound foundation for causal and probing analyses.

Beyond theory, the findings carry practical and legal implications. Hidden states are not abstractions
but the prompt in disguise. Any system that stores or transmits them is effectively handling user
text itself. This affects privacy, deletion, and compliance: even after prompt deletion, embeddings

9

Preprint.

retain the content. Regulators have sometimes argued otherwise; for example, the Hamburg Data
Protection Commissioner claimed that weights do not qualify as personal data since training exam-
ples cannot be trivially reconstructed (HmbBfDI, 2024). Our results show that at inference time user
inputs remain fully recoverable. There is no "free privacy" once data enters a Transformer.

Finally, this work opens several directions. Extending the analysis to multimodal architectures such
as music and vision Transformers is an open problem. Studying approximate inversion under noise
or quantization will clarify how robust invertibility remains in practice. Bridging these technical
insights with evolving regulatory frameworks will be crucial for safe and responsible deployment.

REPRODUCIBILITY STATEMENT

We provide complete resources to ensure reproducibility of our results. The assumptions, defini-
tions, and full proofs can be found in section 2 and sections A to C (analytic tools and model spec-
ification in sections A and B; almost-sure injectivity and preservation under training in section C;
SIP-IT correctness, verifier, and margin analysis in section D). Implementation details for SIP-IT,
including pseudocode, are provided in section 3 and algorithm 1 and further elaborated in section E.
Our experimental setup (hardware and software versions) is described in section 4, while dataset
details and the prompt-sampling procedure for the 100k-prompt benchmark are given in section 4.1.
Finally, the supplementary materials include an anonymized code repository with end-to-end scripts,
fixed seeds, configuration files, and a comprehensive README with step-by-step reproduction in-
structions.

ACKNOWLEDGMENTS

Figure 1 is adapted from Autoencoder Diagrams by Keenan Crane (2025), used under CC0 1.0
Universal. We further acknowledge Adam Barla for the initial discussions on LLMs invertibility.

[Rest of content including references, appendices, etc. continues as in original...]




